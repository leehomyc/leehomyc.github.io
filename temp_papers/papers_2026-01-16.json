[{"paper":{"id":"2601.10477","authors":[{"_id":"69699e5e32f0333869ff9378","name":"Yu Wang","hidden":false},{"_id":"69699e5e32f0333869ff9379","name":"Yi Wang","hidden":false},{"_id":"69699e5e32f0333869ff937a","user":{"_id":"661de9defdbc9c247f159d15","avatarUrl":"/avatars/38e21e78327cc908201122405c48f41b.svg","isPro":false,"fullname":"Rui Dai","user":"DerryD","type":"user"},"name":"Rui Dai","status":"claimed_verified","statusLastChangedAt":"2026-01-16T14:43:46.050Z","hidden":false},{"_id":"69699e5e32f0333869ff937b","name":"Yujie Wang","hidden":false},{"_id":"69699e5e32f0333869ff937c","name":"Kaikui Liu","hidden":false},{"_id":"69699e5e32f0333869ff937d","name":"Xiangxiang Chu","hidden":false},{"_id":"69699e5e32f0333869ff937e","user":{"_id":"63ec91dec8827dd0f0f3b489","avatarUrl":"/avatars/3d0d9479a26673f859c226efaf1e4a43.svg","isPro":false,"fullname":"shengli","user":"yanshengli","type":"user"},"name":"Yansheng Li","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:32:19.008Z","hidden":false}],"publishedAt":"2026-01-15T15:00:36.000Z","submittedOnDailyAt":"2026-01-16T03:49:39.109Z","title":"Urban Socio-Semantic Segmentation with Vision-Language Reasoning","submittedOnDailyBy":{"_id":"66d255e3947594430c723ff6","avatarUrl":"/avatars/c56e4792332a01bf34085a75ee64916e.svg","isPro":false,"fullname":"xiaochonglinghu","user":"xiaochonglinghu","type":"user"},"summary":"As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.","upvotes":138,"discussionId":"69699e5f32f0333869ff937f","githubRepo":"https://github.com/AMAP-ML/SocioReasoner","githubRepoAddedBy":"user","ai_summary":"Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.","ai_keywords":["vision-language model","cross-modal recognition","multi-stage reasoning","reinforcement learning","socio-semantic segmentation","Urban Socio-Semantic Segmentation dataset","SocioReasoner"],"githubStars":125,"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}},"publishedAt":"2026-01-15T10:00:36.000Z","title":"Urban Socio-Semantic Segmentation with Vision-Language Reasoning","summary":"As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png","numComments":2,"submittedBy":{"_id":"66d255e3947594430c723ff6","avatarUrl":"/avatars/c56e4792332a01bf34085a75ee64916e.svg","fullname":"xiaochonglinghu","name":"xiaochonglinghu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.09668","authors":[{"_id":"6968bc424dcc6d53da2701df","name":"Ailin Huang","hidden":false},{"_id":"6968bc424dcc6d53da2701e0","name":"Chengyuan Yao","hidden":false},{"_id":"6968bc424dcc6d53da2701e1","name":"Chunrui Han","hidden":false},{"_id":"6968bc424dcc6d53da2701e2","user":{"_id":"62ecbffd99112e99c5f7fded","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png","isPro":false,"fullname":"Fanqi Wan","user":"Wanfq","type":"user"},"name":"Fanqi Wan","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:29:02.442Z","hidden":false},{"_id":"6968bc424dcc6d53da2701e3","name":"Hangyu Guo","hidden":false},{"_id":"6968bc424dcc6d53da2701e4","user":{"_id":"68c0dd3b8998cbe8217171a5","avatarUrl":"/avatars/554301bdaa61f190693482f28500f7ae.svg","isPro":false,"fullname":"吕浩然","user":"HaoRanLv","type":"user"},"name":"Haoran Lv","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:29:19.559Z","hidden":false},{"_id":"6968bc424dcc6d53da2701e5","name":"Hongyu Zhou","hidden":false},{"_id":"6968bc424dcc6d53da2701e6","name":"Jia Wang","hidden":false},{"_id":"6968bc424dcc6d53da2701e7","name":"Jian Zhou","hidden":false},{"_id":"6968bc424dcc6d53da2701e8","name":"Jianjian Sun","hidden":false},{"_id":"6968bc424dcc6d53da2701e9","user":{"_id":"625026b7d2d191ac43320c5e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg","isPro":false,"fullname":"Jingcheng Hu","user":"reign12","type":"user"},"name":"Jingcheng Hu","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:32:19.060Z","hidden":false},{"_id":"6968bc424dcc6d53da2701ea","user":{"_id":"658a810665df457a55ffcd04","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg","isPro":false,"fullname":"Linkangheng","user":"Kangheng","type":"user"},"name":"Kangheng Lin","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:29:41.402Z","hidden":false},{"_id":"6968bc424dcc6d53da2701eb","name":"Liang Zhao","hidden":false},{"_id":"6968bc424dcc6d53da2701ec","name":"Mitt Huang","hidden":false},{"_id":"6968bc424dcc6d53da2701ed","name":"Song Yuan","hidden":false},{"_id":"6968bc424dcc6d53da2701ee","name":"Wenwen Qu","hidden":false},{"_id":"6968bc424dcc6d53da2701ef","name":"Xiangfeng Wang","hidden":false},{"_id":"6968bc424dcc6d53da2701f0","user":{"_id":"6845364527e777c8bc42e444","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png","isPro":false,"fullname":"yanlin lai","user":"lyn22333","type":"user"},"name":"Yanlin Lai","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:29:26.009Z","hidden":false},{"_id":"6968bc424dcc6d53da2701f1","user":{"_id":"639c0eb734967bcf4565cf29","avatarUrl":"/avatars/f4788bb89b788b40ead4e1f3314044f7.svg","isPro":false,"fullname":"Yingxiu Zhao","user":"Yingxiu","type":"user"},"name":"Yingxiu Zhao","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:29:54.082Z","hidden":false},{"_id":"6968bc424dcc6d53da2701f2","user":{"_id":"664ae39ab5e5f95dc6209365","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg","isPro":false,"fullname":"Yinmin Zhang","user":"YinminZhang","type":"user"},"name":"Yinmin Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:29:48.054Z","hidden":false},{"_id":"6968bc424dcc6d53da2701f3","name":"Yukang Shi","hidden":false},{"_id":"6968bc424dcc6d53da2701f4","name":"Yuyang Chen","hidden":false},{"_id":"6968bc424dcc6d53da2701f5","name":"Zejia Weng","hidden":false},{"_id":"6968bc424dcc6d53da2701f6","name":"Ziyang Meng","hidden":false},{"_id":"6968bc424dcc6d53da2701f7","name":"Ang Li","hidden":false},{"_id":"6968bc424dcc6d53da2701f8","name":"Aobo Kong","hidden":false},{"_id":"6968bc424dcc6d53da2701f9","name":"Bo Dong","hidden":false},{"_id":"6968bc424dcc6d53da2701fa","name":"Changyi Wan","hidden":false},{"_id":"6968bc424dcc6d53da2701fb","name":"David Wang","hidden":false},{"_id":"6968bc424dcc6d53da2701fc","name":"Di Qi","hidden":false},{"_id":"6968bc424dcc6d53da2701fd","name":"Dingming Li","hidden":false},{"_id":"6968bc424dcc6d53da2701fe","name":"En Yu","hidden":false},{"_id":"6968bc424dcc6d53da2701ff","name":"Guopeng Li","hidden":false},{"_id":"6968bc424dcc6d53da270200","name":"Haiquan Yin","hidden":false},{"_id":"6968bc424dcc6d53da270201","name":"Han Zhou","hidden":false},{"_id":"6968bc424dcc6d53da270202","name":"Hanshan Zhang","hidden":false},{"_id":"6968bc424dcc6d53da270203","name":"Haolong Yan","hidden":false},{"_id":"6968bc424dcc6d53da270204","name":"Hebin Zhou","hidden":false},{"_id":"6968bc424dcc6d53da270205","user":{"_id":"68106c88b924dd6c328889c2","avatarUrl":"/avatars/8accf835b711bffa2ea307158950ab33.svg","isPro":false,"fullname":"Hongbo Peng","user":"M1chaelPeng","type":"user"},"name":"Hongbo Peng","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:32:21.188Z","hidden":false},{"_id":"6968bc424dcc6d53da270206","name":"Jiaran Zhang","hidden":false},{"_id":"6968bc424dcc6d53da270207","user":{"_id":"673e9988fc3c3c898a57949b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg","isPro":false,"fullname":"Jiashu Lv","user":"Jserw","type":"user"},"name":"Jiashu Lv","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:30:23.399Z","hidden":false},{"_id":"6968bc424dcc6d53da270208","name":"Jiayi Fu","hidden":false},{"_id":"6968bc424dcc6d53da270209","name":"Jie Cheng","hidden":false},{"_id":"6968bc424dcc6d53da27020a","name":"Jie Zhou","hidden":false},{"_id":"6968bc424dcc6d53da27020b","name":"Jisheng Yin","hidden":false},{"_id":"6968bc424dcc6d53da27020c","user":{"_id":"6502f241b1792803da7e8def","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png","isPro":false,"fullname":"JingJing Xie","user":"ownerEli","type":"user"},"name":"Jingjing Xie","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:30:31.565Z","hidden":false},{"_id":"6968bc424dcc6d53da27020d","name":"Jingwei Wu","hidden":false},{"_id":"6968bc424dcc6d53da27020e","name":"Jun Zhang","hidden":false},{"_id":"6968bc424dcc6d53da27020f","name":"Junfeng Liu","hidden":false},{"_id":"6968bc424dcc6d53da270210","name":"Kaijun Tan","hidden":false},{"_id":"6968bc424dcc6d53da270211","name":"Kaiwen Yan","hidden":false},{"_id":"6968bc424dcc6d53da270212","name":"Liangyu Chen","hidden":false},{"_id":"6968bc424dcc6d53da270213","name":"Lina Chen","hidden":false},{"_id":"6968bc424dcc6d53da270214","name":"Mingliang Li","hidden":false},{"_id":"6968bc424dcc6d53da270215","name":"Qian Zhao","hidden":false},{"_id":"6968bc424dcc6d53da270216","name":"Quan Sun","hidden":false},{"_id":"6968bc424dcc6d53da270217","name":"Shaoliang Pang","hidden":false},{"_id":"6968bc424dcc6d53da270218","name":"Shengjie Fan","hidden":false},{"_id":"6968bc424dcc6d53da270219","name":"Shijie Shang","hidden":false},{"_id":"6968bc424dcc6d53da27021a","user":{"_id":"682703cde798014f05e8d224","avatarUrl":"/avatars/167ba232ad427e995aa9629202c670d0.svg","isPro":false,"fullname":"SiyuanZhang","user":"SiyuanZhang","type":"user"},"name":"Siyuan Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:31:04.562Z","hidden":false},{"_id":"6968bc424dcc6d53da27021b","name":"Tianhao You","hidden":false},{"_id":"6968bc424dcc6d53da27021c","name":"Wei Ji","hidden":false},{"_id":"6968bc424dcc6d53da27021d","name":"Wuxun Xie","hidden":false},{"_id":"6968bc424dcc6d53da27021e","name":"Xiaobo Yang","hidden":false},{"_id":"6968bc424dcc6d53da27021f","name":"Xiaojie Hou","hidden":false},{"_id":"6968bc424dcc6d53da270220","name":"Xiaoran Jiao","hidden":false},{"_id":"6968bc424dcc6d53da270221","name":"Xiaoxiao Ren","hidden":false},{"_id":"6968bc424dcc6d53da270222","name":"Xiangwen Kong","hidden":false},{"_id":"6968bc424dcc6d53da270223","name":"Xin Huang","hidden":false},{"_id":"6968bc424dcc6d53da270224","name":"Xin Wu","hidden":false},{"_id":"6968bc424dcc6d53da270225","name":"Xing Chen","hidden":false},{"_id":"6968bc424dcc6d53da270226","name":"Xinran Wang","hidden":false},{"_id":"6968bc424dcc6d53da270227","name":"Xuelin Zhang","hidden":false},{"_id":"6968bc424dcc6d53da270228","user":{"_id":"64ae4d62179421d320b67c26","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg","isPro":false,"fullname":"Yana Wei","user":"llwswyn","type":"user"},"name":"Yana Wei","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:31:44.883Z","hidden":false},{"_id":"6968bc424dcc6d53da270229","name":"Yang Li","hidden":false},{"_id":"6968bc424dcc6d53da27022a","name":"Yanming Xu","hidden":false},{"_id":"6968bc424dcc6d53da27022b","name":"Yeqing Shen","hidden":false},{"_id":"6968bc424dcc6d53da27022c","name":"Yuang Peng","hidden":false},{"_id":"6968bc424dcc6d53da27022d","name":"Yue Peng","hidden":false},{"_id":"6968bc424dcc6d53da27022e","name":"Yu Zhou","hidden":false},{"_id":"6968bc424dcc6d53da27022f","name":"Yusheng Li","hidden":false},{"_id":"6968bc424dcc6d53da270230","name":"Yuxiang Yang","hidden":false},{"_id":"6968bc424dcc6d53da270231","name":"Yuyang Zhang","hidden":false},{"_id":"6968bc424dcc6d53da270232","name":"Zhe Xie","hidden":false},{"_id":"6968bc424dcc6d53da270233","name":"Zhewei Huang","hidden":false},{"_id":"6968bc424dcc6d53da270234","name":"Zhenyi Lu","hidden":false},{"_id":"6968bc424dcc6d53da270235","name":"Zhimin Fan","hidden":false},{"_id":"6968bc424dcc6d53da270236","name":"Zihui Cheng","hidden":false},{"_id":"6968bc424dcc6d53da270237","name":"Daxin Jiang","hidden":false},{"_id":"6968bc424dcc6d53da270238","name":"Qi Han","hidden":false},{"_id":"6968bc424dcc6d53da270239","name":"Xiangyu Zhang","hidden":false},{"_id":"6968bc424dcc6d53da27023a","name":"Yibo Zhu","hidden":false},{"_id":"6968bc424dcc6d53da27023b","name":"Zheng Ge","hidden":false}],"publishedAt":"2026-01-14T17:58:24.000Z","submittedOnDailyAt":"2026-01-16T01:39:25.029Z","title":"STEP3-VL-10B Technical Report","submittedOnDailyBy":{"_id":"625026b7d2d191ac43320c5e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg","isPro":false,"fullname":"Jingcheng Hu","user":"reign12","type":"user"},"summary":"We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.","upvotes":129,"discussionId":"6968bc434dcc6d53da27023c","projectPage":"https://stepfun-ai.github.io/Step3-VL-10B","githubRepo":"https://github.com/stepfun-ai/Step3-VL-10B","githubRepoAddedBy":"auto","ai_summary":"STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.","ai_keywords":["multimodal tokens","Perception Encoder","Qwen3-8B decoder","vision-language synergy","reinforcement learning","Parallel Coordinated Reasoning","test-time compute","visual hypotheses","MMBench","MMMU","AIME2025","MathVision"],"githubStars":152,"organization":{"_id":"66e43eae9d477f566f937935","name":"stepfun-ai","fullname":"StepFun","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}},"publishedAt":"2026-01-14T12:58:24.000Z","title":"STEP3-VL-10B Technical Report","summary":"We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png","numComments":4,"submittedBy":{"_id":"625026b7d2d191ac43320c5e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg","fullname":"Jingcheng Hu","name":"reign12","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":20,"isUserFollowing":false},"organization":{"_id":"66e43eae9d477f566f937935","name":"stepfun-ai","fullname":"StepFun","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.08763","authors":[{"_id":"6969b0a232f0333869ff946a","user":{"_id":"64351475901c5734bcb64248","avatarUrl":"/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg","isPro":false,"fullname":"Zhiyuan Hu","user":"zhiyuanhucs","type":"user"},"name":"Zhiyuan Hu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:32:38.232Z","hidden":false},{"_id":"6969b0a232f0333869ff946b","user":{"_id":"6891c906f3c31445cc040ab1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg","isPro":false,"fullname":"Yucheng Wang","user":"DevilEnfant","type":"user"},"name":"Yucheng Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:32:48.080Z","hidden":false},{"_id":"6969b0a232f0333869ff946c","name":"Yufei He","hidden":false},{"_id":"6969b0a232f0333869ff946d","user":{"_id":"682deb444988bd82847e2b03","avatarUrl":"/avatars/15da087e84386ea72c6fa2db63571420.svg","isPro":false,"fullname":"Jia-Ying Wu","user":"EricaWu","type":"user"},"name":"Jiaying Wu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:32:59.692Z","hidden":false},{"_id":"6969b0a232f0333869ff946e","name":"Yilun Zhao","hidden":false},{"_id":"6969b0a232f0333869ff946f","name":"See-Kiong Ng","hidden":false},{"_id":"6969b0a232f0333869ff9470","user":{"_id":"672793ffa5255a517fd02045","avatarUrl":"/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg","isPro":false,"fullname":"Cynthia Breazeal","user":"cynthiabreazeal","type":"user"},"name":"Cynthia Breazeal","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:33:06.327Z","hidden":false},{"_id":"6969b0a232f0333869ff9471","user":{"_id":"655722e80438e0854fae7554","avatarUrl":"/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg","isPro":false,"fullname":"Luu Anh Tuan","user":"anhtuanluu36","type":"user"},"name":"Anh Tuan Luu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:33:12.181Z","hidden":false},{"_id":"6969b0a232f0333869ff9472","user":{"_id":"682352cdb1c5350f850dd952","avatarUrl":"/avatars/5426efe0195ac8f914839e6585b1a112.svg","isPro":false,"fullname":"Hae Won Park","user":"robohaewon","type":"user"},"name":"Hae Won Park","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:33:17.979Z","hidden":false},{"_id":"6969b0a232f0333869ff9473","user":{"_id":"651d8032c50012d33e914f2f","avatarUrl":"/avatars/0a44c9f51fc50ce86582e328c361ea00.svg","isPro":false,"fullname":"Bryan Hooi","user":"bhooi","type":"user"},"name":"Bryan Hooi","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:33:23.007Z","hidden":false}],"publishedAt":"2026-01-13T17:48:43.000Z","submittedOnDailyAt":"2026-01-16T01:00:36.686Z","title":"Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs","submittedOnDailyBy":{"_id":"64351475901c5734bcb64248","avatarUrl":"/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg","isPro":false,"fullname":"Zhiyuan Hu","user":"zhiyuanhucs","type":"user"},"summary":"Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.","upvotes":111,"discussionId":"6969b0a232f0333869ff9474","ai_summary":"Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.","ai_keywords":["reinforcement learning","large language models","exploration collapse","pass@k","pass@1","rollout-level objective","high-level solution strategies","clustering","policy advantages","AUC@K"],"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}},"publishedAt":"2026-01-13T12:48:43.000Z","title":"Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs","summary":"Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png","numComments":3,"submittedBy":{"_id":"64351475901c5734bcb64248","avatarUrl":"/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg","fullname":"Zhiyuan Hu","name":"zhiyuanhucs","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.09667","authors":[{"_id":"6969b0f732f0333869ff9476","user":{"_id":"64351475901c5734bcb64248","avatarUrl":"/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg","isPro":false,"fullname":"Zhiyuan Hu","user":"zhiyuanhucs","type":"user"},"name":"Zhiyuan Hu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:33:48.445Z","hidden":false},{"_id":"6969b0f732f0333869ff9477","user":{"_id":"662b4e3bc709a61df840fda1","avatarUrl":"/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg","isPro":false,"fullname":"Hu Yunhai","user":"AlexCCtop","type":"user"},"name":"Yunhai Hu","status":"claimed_verified","statusLastChangedAt":"2026-01-16T14:37:06.706Z","hidden":false},{"_id":"6969b0f732f0333869ff9478","user":{"_id":"650026d30339dae3dba2cec5","avatarUrl":"/avatars/fcc9ea4336f8d4bb177e5c9eacdd05c9.svg","isPro":false,"fullname":"Juncheng Liu","user":"juncliu","type":"user"},"name":"Juncheng Liu","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:33.401Z","hidden":false},{"_id":"6969b0f732f0333869ff9479","name":"Shuyue Stella Li","hidden":false},{"_id":"6969b0f732f0333869ff947a","name":"Yucheng Wang","hidden":false},{"_id":"6969b0f732f0333869ff947b","user":{"_id":"638e40d450a4e4beef98196b","avatarUrl":"/avatars/fe27e019baf48caeb44e19b7289db9fb.svg","isPro":false,"fullname":"Zhen Xu","user":"zhenxu","type":"user"},"name":"Zhen Xu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:34:04.868Z","hidden":false},{"_id":"6969b0f732f0333869ff947c","name":"See-Kiong Ng","hidden":false},{"_id":"6969b0f732f0333869ff947d","user":{"_id":"655722e80438e0854fae7554","avatarUrl":"/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg","isPro":false,"fullname":"Luu Anh Tuan","user":"anhtuanluu36","type":"user"},"name":"Anh Tuan Luu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:34:15.855Z","hidden":false},{"_id":"6969b0f732f0333869ff947e","name":"Xinxing Xu","hidden":false},{"_id":"6969b0f732f0333869ff947f","user":{"_id":"651d8032c50012d33e914f2f","avatarUrl":"/avatars/0a44c9f51fc50ce86582e328c361ea00.svg","isPro":false,"fullname":"Bryan Hooi","user":"bhooi","type":"user"},"name":"Bryan Hooi","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:34:25.577Z","hidden":false},{"_id":"6969b0f732f0333869ff9480","user":{"_id":"672793ffa5255a517fd02045","avatarUrl":"/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg","isPro":false,"fullname":"Cynthia Breazeal","user":"cynthiabreazeal","type":"user"},"name":"Cynthia Breazeal","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:34:31.289Z","hidden":false},{"_id":"6969b0f732f0333869ff9481","user":{"_id":"682352cdb1c5350f850dd952","avatarUrl":"/avatars/5426efe0195ac8f914839e6585b1a112.svg","isPro":false,"fullname":"Hae Won Park","user":"robohaewon","type":"user"},"name":"Hae Won Park","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:34:36.481Z","hidden":false}],"publishedAt":"2026-01-14T17:57:43.000Z","submittedOnDailyAt":"2026-01-16T01:01:32.343Z","title":"Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning","submittedOnDailyBy":{"_id":"64351475901c5734bcb64248","avatarUrl":"/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg","isPro":false,"fullname":"Zhiyuan Hu","user":"zhiyuanhucs","type":"user"},"summary":"Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.","upvotes":63,"discussionId":"6969b0f832f0333869ff9482","ai_summary":"Multi-Agent Test-Time Reinforcement Learning (MATTRL) enhances multi-agent reasoning through structured textual experience injection and consensus-based decision making at inference time.","ai_keywords":["multi-agent systems","reinforcement learning","test-time reinforcement learning","multi-agent reinforcement learning","credit assignment","multi-expert teams","dialogue systems","distribution-shift-robust reasoning"],"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}},"publishedAt":"2026-01-14T12:57:43.000Z","title":"Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning","summary":"Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09667.png","numComments":3,"submittedBy":{"_id":"64351475901c5734bcb64248","avatarUrl":"/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg","fullname":"Zhiyuan Hu","name":"zhiyuanhucs","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.02242","authors":[{"_id":"695cafde6aa73bc11f091566","user":{"_id":"65e7151ef7c2e46887e225b1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65e7151ef7c2e46887e225b1/NXj_CrUkzwdUT8T3a-H8N.jpeg","isPro":false,"fullname":"Grigorii Alekseenko","user":"Riko0","type":"user"},"name":"Grigorii Alekseenko","status":"claimed_verified","statusLastChangedAt":"2026-01-12T10:36:29.469Z","hidden":false},{"_id":"695cafde6aa73bc11f091567","user":{"_id":"65ae526111a0a3ff61d7d726","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65ae526111a0a3ff61d7d726/x6OVUbowh-eXH9bxWCERB.jpeg","isPro":false,"fullname":"Aleksandr","user":"grac20101","type":"user"},"name":"Aleksandr Gordeev","status":"claimed_verified","statusLastChangedAt":"2026-01-12T14:16:49.474Z","hidden":false},{"_id":"695cafde6aa73bc11f091568","user":{"_id":"6498095fce9190ebb8699113","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png","isPro":true,"fullname":"Irina Tolstykh","user":"iitolstykh","type":"user"},"name":"Irina Tolstykh","status":"claimed_verified","statusLastChangedAt":"2026-01-14T12:42:37.939Z","hidden":false},{"_id":"695cafde6aa73bc11f091569","name":"Bulat Suleimanov","hidden":false},{"_id":"695cafde6aa73bc11f09156a","name":"Vladimir Dokholyan","hidden":false},{"_id":"695cafde6aa73bc11f09156b","name":"Georgii Fedorov","hidden":false},{"_id":"695cafde6aa73bc11f09156c","name":"Sergey Yakubson","hidden":false},{"_id":"695cafde6aa73bc11f09156d","name":"Aleksandra Tsybina","hidden":false},{"_id":"695cafde6aa73bc11f09156e","name":"Mikhail Chernyshov","hidden":false},{"_id":"695cafde6aa73bc11f09156f","user":{"_id":"6416d8ef8f689506e70dd2e5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1679218871593-noauth.jpeg","isPro":false,"fullname":"Maksim Kuprashevich","user":"WildChlamydia","type":"user"},"name":"Maksim Kuprashevich","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:34:25.729Z","hidden":false}],"publishedAt":"2026-01-05T16:17:20.000Z","submittedOnDailyAt":"2026-01-16T08:00:29.947Z","title":"VIBE: Visual Instruction Based Editor","submittedOnDailyBy":{"_id":"6498095fce9190ebb8699113","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png","isPro":true,"fullname":"Irina Tolstykh","user":"iitolstykh","type":"user"},"summary":"Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.","upvotes":46,"discussionId":"695cafde6aa73bc11f091570","projectPage":"https://riko0.github.io/VIBE/","githubRepo":"https://github.com/ai-forever/vibe","githubRepoAddedBy":"user","ai_summary":"A compact image editing system uses a 2B-parameter model for guidance and a 1.6B-parameter diffusion model to achieve high-quality edits with low computational requirements and strict source consistency.","ai_keywords":["diffusion models","Qwen3-VL","Sana1.5","instruction-based image editing","image generation","source consistency","inference efficiency","parameter-efficient models","ImgEdit benchmark","GEdit benchmark"],"githubStars":22},"publishedAt":"2026-01-05T11:17:20.000Z","title":"VIBE: Visual Instruction Based Editor","summary":"Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02242.png","numComments":2,"submittedBy":{"_id":"6498095fce9190ebb8699113","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png","fullname":"Irina Tolstykh","name":"iitolstykh","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":27,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.07641","authors":[{"_id":"696745d2c5e371f6b235d1f1","user":{"_id":"67e655f7d6b8333a8f78eadf","avatarUrl":"/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg","isPro":false,"fullname":"Jiaxuan Lu","user":"Blue-Giant","type":"user"},"name":"Jiaxuan Lu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:34:53.789Z","hidden":false},{"_id":"696745d2c5e371f6b235d1f2","user":{"_id":"65617bf9f5532ac1bde64d07","avatarUrl":"/avatars/6659fe26eecfce8ac699caa73b823fe1.svg","isPro":false,"fullname":"ZIYU KONG","user":"ziyukong","type":"user"},"name":"Ziyu Kong","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:34:59.511Z","hidden":false},{"_id":"696745d2c5e371f6b235d1f3","name":"Yemin Wang","hidden":false},{"_id":"696745d2c5e371f6b235d1f4","name":"Rong Fu","hidden":false},{"_id":"696745d2c5e371f6b235d1f5","user":{"_id":"691b0f528411a45dc9ee9de8","avatarUrl":"/avatars/261c28f7e616a8482970f50c1f8919fd.svg","isPro":false,"fullname":"Haiyuan Wan","user":"HY-Wan","type":"user"},"name":"Haiyuan Wan","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:33:47.426Z","hidden":false},{"_id":"696745d2c5e371f6b235d1f6","user":{"_id":"67c443afb753bd020f9c97d8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png","isPro":false,"fullname":"Cheng","user":"YangC777","type":"user"},"name":"Cheng Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:33:18.927Z","hidden":false},{"_id":"696745d2c5e371f6b235d1f7","name":"Wenjie Lou","hidden":false},{"_id":"696745d2c5e371f6b235d1f8","name":"Haoran Sun","hidden":false},{"_id":"696745d2c5e371f6b235d1f9","user":{"_id":"67fc7887864dfcbd93ce6322","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1ecRk5ZWDXALss7e4fjtQ.png","isPro":false,"fullname":"Lilong Wang","user":"Eason2025","type":"user"},"name":"Lilong Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:35:20.390Z","hidden":false},{"_id":"696745d2c5e371f6b235d1fa","user":{"_id":"671280b9a895018bbc281dea","avatarUrl":"/avatars/55f3c2d3698011bc718ec18295519caa.svg","isPro":false,"fullname":"Yankai Jiang","user":"yankaijiang","type":"user"},"name":"Yankai Jiang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:35:26.741Z","hidden":false},{"_id":"696745d2c5e371f6b235d1fb","name":"Xiaosong Wang","hidden":false},{"_id":"696745d2c5e371f6b235d1fc","name":"Xiao Sun","hidden":false},{"_id":"696745d2c5e371f6b235d1fd","user":{"_id":"6538b861613fe158bd581e35","avatarUrl":"/avatars/6817dbfe903675721fd227058b0a91ac.svg","isPro":false,"fullname":"Dongzhan Zhou","user":"schrodingers-tiger","type":"user"},"name":"Dongzhan Zhou","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:35:36.629Z","hidden":false}],"publishedAt":"2026-01-12T15:22:51.000Z","submittedOnDailyAt":"2026-01-16T05:19:21.135Z","title":"Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning","submittedOnDailyBy":{"_id":"67e655f7d6b8333a8f78eadf","avatarUrl":"/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg","isPro":false,"fullname":"Jiaxuan Lu","user":"Blue-Giant","type":"user"},"summary":"The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.","upvotes":35,"discussionId":"696745d2c5e371f6b235d1fe","githubRepo":"https://github.com/lujiaxuan0520/Test-Time-Tool-Evol","githubRepoAddedBy":"user","ai_summary":"Test-Time Tool Evolution enables AI agents to dynamically create and refine computational tools during inference, overcoming limitations of static tool libraries in scientific applications.","ai_keywords":["LLM-based agents","tool libraries","scientific reasoning","computational methods","test-time tool evolution","SciEvo benchmark","tool synthesis","tool verification","tool evolution","cross-domain adaptation"],"githubStars":34,"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "}},"publishedAt":"2026-01-12T10:22:51.000Z","title":"Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning","summary":"The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07641.png","numComments":1,"submittedBy":{"_id":"67e655f7d6b8333a8f78eadf","avatarUrl":"/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg","fullname":"Jiaxuan Lu","name":"Blue-Giant","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "},"isAuthorParticipating":true},{"paper":{"id":"2601.10305","authors":[{"_id":"6969a0b932f0333869ff9381","user":{"_id":"67e289aea1e569cd0a41db1d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/w7b_6u7nZDH9Lp6NJKdVJ.png","isPro":false,"fullname":"shen hengyu","user":"dewecho","type":"user"},"name":"Hengyu Shen","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:35:54.646Z","hidden":false},{"_id":"6969a0b932f0333869ff9382","user":{"_id":"641030c77a15af878ae5bd8f","avatarUrl":"/avatars/8a5037edf55c78ebc317c8b191343671.svg","isPro":false,"fullname":"TianchengGu","user":"TianchengGu","type":"user"},"name":"Tiancheng Gu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:36:00.059Z","hidden":false},{"_id":"6969a0b932f0333869ff9383","name":"Bin Qin","hidden":false},{"_id":"6969a0b932f0333869ff9384","name":"Lan Wu","hidden":false},{"_id":"6969a0b932f0333869ff9385","name":"Yuling Wu","hidden":false},{"_id":"6969a0b932f0333869ff9386","name":"Shuo Tan","hidden":false},{"_id":"6969a0b932f0333869ff9387","user":{"_id":"63dfc05342591dda0b945e58","avatarUrl":"/avatars/3fd796035c2243d6b03cc361bc06e64e.svg","isPro":false,"fullname":"Zelong Sun","user":"dfgdgh","type":"user"},"name":"Zelong Sun","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:36:29.711Z","hidden":false},{"_id":"6969a0b932f0333869ff9388","name":"Jun Wang","hidden":false},{"_id":"6969a0b932f0333869ff9389","name":"Nan Wu","hidden":false},{"_id":"6969a0b932f0333869ff938a","name":"Xiang An","hidden":false},{"_id":"6969a0b932f0333869ff938b","user":{"_id":"6760a8f5e4b55ba1b2b0a7b4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NddUMmwmZFbS25v1q8KyS.png","isPro":false,"fullname":"Weidong Cai","user":"SeriousBro","type":"user"},"name":"Weidong Cai","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:36:16.748Z","hidden":false},{"_id":"6969a0b932f0333869ff938c","user":{"_id":"694d00c3ece16a65e2b84774","avatarUrl":"/avatars/72bfeec4602ba4069faf0dba02c2be96.svg","isPro":false,"fullname":"Ziyong Feng","user":"fengziyong","type":"user"},"name":"Ziyong Feng","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:36:10.787Z","hidden":false},{"_id":"6969a0b932f0333869ff938d","user":{"_id":"63e202f352b7578dba448ab5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg","isPro":false,"fullname":"Kaicheng Yang","user":"Kaichengalex","type":"user"},"name":"Kaicheng Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:30:51.089Z","hidden":false}],"publishedAt":"2026-01-15T11:28:58.000Z","submittedOnDailyAt":"2026-01-16T00:37:46.383Z","title":"DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset","submittedOnDailyBy":{"_id":"63e202f352b7578dba448ab5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg","isPro":false,"fullname":"Kaicheng Yang","user":"Kaichengalex","type":"user"},"summary":"Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.","upvotes":29,"discussionId":"6969a0b932f0333869ff938e","projectPage":"https://deepglint.github.io/DanQing/","githubRepo":"https://github.com/deepglint/DanQing","githubRepoAddedBy":"user","ai_summary":"A large-scale Chinese image-text dataset called DanQing is introduced to advance vision-language pretraining, demonstrating superior performance in various downstream tasks through continual pretraining of the SigLIP2 model.","ai_keywords":["Vision-Language Pre-training","contrastive pretraining","cross-modal retrieval","image captioning","SigLIP2","continual pre-training"],"githubStars":12},"publishedAt":"2026-01-15T06:28:58.000Z","title":"DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset","summary":"Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10305.png","numComments":2,"submittedBy":{"_id":"63e202f352b7578dba448ab5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg","fullname":"Kaicheng Yang","name":"Kaichengalex","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.10402","authors":[{"_id":"6969f54c844a787c4fdea404","user":{"_id":"65352517fca2c10e43035e32","avatarUrl":"/avatars/f404cef8a79f27435865ac4d85ef0927.svg","isPro":false,"fullname":"xinyuzhu","user":"xinyuzhu","type":"user"},"name":"Xinyu Zhu","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:28:31.884Z","hidden":false},{"_id":"6969f54c844a787c4fdea405","user":{"_id":"6614ed4809c63bfbddc53ddf","avatarUrl":"/avatars/018bfc168bb4010cf6018e42148e0f51.svg","isPro":false,"fullname":"Yuzhu Cai","user":"Ethical-Lens","type":"user"},"name":"Yuzhu Cai","status":"claimed_verified","statusLastChangedAt":"2026-01-16T14:36:59.231Z","hidden":false},{"_id":"6969f54c844a787c4fdea406","user":{"_id":"651e1f5522890326303fb1f4","avatarUrl":"/avatars/4e47b8d7669c3aafcab4e1438ed386e7.svg","isPro":false,"fullname":"Zexi Liu","user":"ZeroXLeo","type":"user"},"name":"Zexi Liu","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:28:33.861Z","hidden":false},{"_id":"6969f54c844a787c4fdea407","user":{"_id":"690320c2eba76a9b99854b4d","avatarUrl":"/avatars/a612962c3627bdb7986a5ab5f14af2eb.svg","isPro":false,"fullname":"Bingyang Zheng","user":"bulibuliyang","type":"user"},"name":"Bingyang Zheng","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:38:51.672Z","hidden":false},{"_id":"6969f54c844a787c4fdea408","name":"Cheng Wang","hidden":false},{"_id":"6969f54c844a787c4fdea409","name":"Rui Ye","hidden":false},{"_id":"6969f54c844a787c4fdea40a","user":{"_id":"64f793d4dcd7b028c15bbe50","avatarUrl":"/avatars/fb5d7392736309dd5c80ac32750d164b.svg","isPro":false,"fullname":"Jiaao Chen","user":"Jiaaoc","type":"user"},"name":"Jiaao Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:39:08.324Z","hidden":false},{"_id":"6969f54c844a787c4fdea40b","user":{"_id":"677e237c70bd7b5431f88450","avatarUrl":"/avatars/cc64b0af06588e43c94c185900362751.svg","isPro":false,"fullname":"Hanrui Wang","user":"azrealwang","type":"user"},"name":"Hanrui Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:39:14.281Z","hidden":false},{"_id":"6969f54c844a787c4fdea40c","user":{"_id":"68c08c9da619a2f12eae4913","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FYHEbHaFSOcWScBmeHQcF.png","isPro":false,"fullname":"Wei Chen Wang","user":"bumbigby","type":"user"},"name":"Wei-Chen Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:39:21.035Z","hidden":false},{"_id":"6969f54c844a787c4fdea40d","name":"Yuzhi Zhang","hidden":false},{"_id":"6969f54c844a787c4fdea40e","name":"Linfeng Zhang","hidden":false},{"_id":"6969f54c844a787c4fdea40f","name":"Weinan E","hidden":false},{"_id":"6969f54c844a787c4fdea410","name":"Di Jin","hidden":false},{"_id":"6969f54c844a787c4fdea411","user":{"_id":"65257545b017be1fc1915364","avatarUrl":"/avatars/9bffd3fb567d2fa1e5c3546d77560b43.svg","isPro":false,"fullname":"Siheng Chen","user":"sihengchen","type":"user"},"name":"Siheng Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:39:39.159Z","hidden":false}],"publishedAt":"2026-01-15T13:52:04.000Z","submittedOnDailyAt":"2026-01-16T06:18:42.647Z","title":"Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering","submittedOnDailyBy":{"_id":"6614ed4809c63bfbddc53ddf","avatarUrl":"/avatars/018bfc168bb4010cf6018e42148e0f51.svg","isPro":false,"fullname":"Yuzhu Cai","user":"Ethical-Lens","type":"user"},"summary":"The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.","upvotes":26,"discussionId":"6969f54c844a787c4fdea412","projectPage":"https://sjtu-sai-agents.github.io/ML-Master/","githubRepo":"https://github.com/sjtu-sai-agents/ML-Master","githubRepoAddedBy":"user","ai_summary":"ML-Master 2.0 enables long-term autonomous machine learning engineering through hierarchical cognitive caching that manages extended context and learns from execution traces.","ai_keywords":["Large Language Models","ultra-long-horizon autonomy","machine learning engineering","Hierarchical Cognitive Caching","cognitive accumulation","context management","experimental strategy","execution traces","cross-task wisdom"],"githubStars":332,"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}},"publishedAt":"2026-01-15T08:52:04.000Z","title":"Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering","summary":"The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10402.png","numComments":1,"submittedBy":{"_id":"6614ed4809c63bfbddc53ddf","avatarUrl":"/avatars/018bfc168bb4010cf6018e42148e0f51.svg","fullname":"Yuzhu Cai","name":"Ethical-Lens","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.10061","authors":[{"_id":"6969aeaf32f0333869ff9459","name":"Chengzhuo Tong","hidden":false},{"_id":"6969aeaf32f0333869ff945a","user":{"_id":"662db438137b72821671db2f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hLMBYuvyF6sfkS0sSrDt-.jpeg","isPro":false,"fullname":"Mingkun Chang","user":"D4isyC","type":"user"},"name":"Mingkun Chang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:37:17.228Z","hidden":false},{"_id":"6969aeaf32f0333869ff945b","user":{"_id":"67c14a89f85f9a6c361226ba","avatarUrl":"/avatars/538eede44205f49fe5a562dcce992d7c.svg","isPro":false,"fullname":"shenglong","user":"zhangshenglong","type":"user"},"name":"Shenglong Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:37:31.154Z","hidden":false},{"_id":"6969aeaf32f0333869ff945c","user":{"_id":"65e71ef39cf349af2940b317","avatarUrl":"/avatars/fc1cd8d3510946fc947d67b16b51834b.svg","isPro":false,"fullname":"Yuran Wang","user":"Ryann829","type":"user"},"name":"Yuran Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:37.205Z","hidden":false},{"_id":"6969aeaf32f0333869ff945d","name":"Cheng Liang","hidden":false},{"_id":"6969aeaf32f0333869ff945e","user":{"_id":"6713a71e7dfe714b425cccfb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/95YYcbv_f6J8yWTunwn4z.png","isPro":false,"fullname":"zhizhengzhao","user":"zhizhengzhao","type":"user"},"name":"Zhizheng Zhao","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:37:42.205Z","hidden":false},{"_id":"6969aeaf32f0333869ff945f","name":"Ruichuan An","hidden":false},{"_id":"6969aeaf32f0333869ff9460","user":{"_id":"6671214c92412fd4640714eb","avatarUrl":"/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg","isPro":false,"fullname":"bohan zeng","user":"zbhpku","type":"user"},"name":"Bohan Zeng","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:37:03.210Z","hidden":false},{"_id":"6969aeaf32f0333869ff9461","user":{"_id":"673c7319d11b1c2e246ead9c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg","isPro":false,"fullname":"Yang Shi","user":"DogNeverSleep","type":"user"},"name":"Yang Shi","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:39.706Z","hidden":false},{"_id":"6969aeaf32f0333869ff9462","name":"Yifan Dai","hidden":false},{"_id":"6969aeaf32f0333869ff9463","user":{"_id":"68418019d777f13c594ffe5f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a2yvPF9IoCcXKFz7TkADe.png","isPro":false,"fullname":"Ziming Zhao","user":"ZimingZhao","type":"user"},"name":"Ziming Zhao","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:38:15.698Z","hidden":false},{"_id":"6969aeaf32f0333869ff9464","name":"Guanbin Li","hidden":false},{"_id":"6969aeaf32f0333869ff9465","name":"Pengfei Wan","hidden":false},{"_id":"6969aeaf32f0333869ff9466","name":"Yuanxing Zhang","hidden":false},{"_id":"6969aeaf32f0333869ff9467","name":"Wentao Zhang","hidden":false}],"publishedAt":"2026-01-15T04:33:06.000Z","submittedOnDailyAt":"2026-01-16T00:52:07.049Z","title":"CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation","submittedOnDailyBy":{"_id":"6671214c92412fd4640714eb","avatarUrl":"/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg","isPro":false,"fullname":"bohan zeng","user":"zbhpku","type":"user"},"summary":"Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.","upvotes":25,"discussionId":"6969aeaf32f0333869ff9468","projectPage":"https://cof-t2i.github.io/","githubRepo":"https://github.com/VisionChengzhuo/CoF-T2I","githubRepoAddedBy":"user","ai_summary":"Chain-of-Frame reasoning is integrated into text-to-image generation through progressive visual refinement with explicit intermediate steps, achieving superior performance on benchmark datasets.","ai_keywords":["Chain-of-Frame reasoning","text-to-image generation","progressive visual refinement","CoF trajectories","GenEval","Imagine-Bench"],"githubStars":18,"organization":{"_id":"662c559b322afcbae51b3c8b","name":"KlingTeam","fullname":"Kling Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}},"publishedAt":"2026-01-14T23:33:06.000Z","title":"CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation","summary":"Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10061.png","numComments":1,"submittedBy":{"_id":"6671214c92412fd4640714eb","avatarUrl":"/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg","fullname":"bohan zeng","name":"zbhpku","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"662c559b322afcbae51b3c8b","name":"KlingTeam","fullname":"Kling Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.10332","authors":[{"_id":"6969a7d132f0333869ff93bc","name":"Siqi Kou","hidden":false},{"_id":"6969a7d132f0333869ff93bd","name":"Jiachun Jin","hidden":false},{"_id":"6969a7d132f0333869ff93be","name":"Zetong Zhou","hidden":false},{"_id":"6969a7d132f0333869ff93bf","name":"Ye Ma","hidden":false},{"_id":"6969a7d132f0333869ff93c0","name":"Yugang Wang","hidden":false},{"_id":"6969a7d132f0333869ff93c1","name":"Quan Chen","hidden":false},{"_id":"6969a7d132f0333869ff93c2","name":"Peng Jiang","hidden":false},{"_id":"6969a7d132f0333869ff93c3","name":"Xiao Yang","hidden":false},{"_id":"6969a7d132f0333869ff93c4","name":"Jun Zhu","hidden":false},{"_id":"6969a7d132f0333869ff93c5","name":"Kai Yu","hidden":false},{"_id":"6969a7d132f0333869ff93c6","name":"Zhijie Deng","hidden":false}],"publishedAt":"2026-01-15T12:19:05.000Z","submittedOnDailyAt":"2026-01-16T00:29:52.118Z","title":"Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders","submittedOnDailyBy":{"_id":"654e330f350abceb30a1390b","avatarUrl":"/avatars/e54a8be788fa1bdc7acefecc208215bb.svg","isPro":false,"fullname":"KouSiqi","user":"karrykkk","type":"user"},"summary":"Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.","upvotes":20,"discussionId":"6969a7d132f0333869ff93c7","githubRepo":"https://github.com/zhijie-group/Think-Then-Generate","githubRepoAddedBy":"user","ai_summary":"Text-to-image diffusion models enhanced with language model reasoning capabilities achieve improved factual consistency and semantic alignment through a think-then-generate paradigm with dual-gradient reinforcement optimization.","ai_keywords":["text-to-image diffusion models","language model-based text encoders","think-then-generate paradigm","supervised fine-tuning","dual-gradient reinforcement optimization","image-grounded rewards","semantic alignment","factual consistency","visual realism"],"githubStars":37,"organization":{"_id":"673d5fe8d031224e947dc235","name":"SJTU-DENG-Lab","fullname":"DENG Lab @ SJTU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"}},"publishedAt":"2026-01-15T07:19:05.000Z","title":"Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders","summary":"Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10332.png","numComments":2,"submittedBy":{"_id":"654e330f350abceb30a1390b","avatarUrl":"/avatars/e54a8be788fa1bdc7acefecc208215bb.svg","fullname":"KouSiqi","name":"karrykkk","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"673d5fe8d031224e947dc235","name":"SJTU-DENG-Lab","fullname":"DENG Lab @ SJTU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.10714","authors":[{"_id":"6969e7be844a787c4fdea3d4","user":{"_id":"66cd9ce36b6031069d23e746","avatarUrl":"/avatars/9cc2a764273fbb8b1b261f31f0532d06.svg","isPro":false,"fullname":"Tal Reiss","user":"talreiss","type":"user"},"name":"Tal Reiss","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:28:36.976Z","hidden":false},{"_id":"6969e7be844a787c4fdea3d5","name":"Daniel Winter","hidden":false},{"_id":"6969e7be844a787c4fdea3d6","name":"Matan Cohen","hidden":false},{"_id":"6969e7be844a787c4fdea3d7","name":"Alex Rav-Acha","hidden":false},{"_id":"6969e7be844a787c4fdea3d8","name":"Yael Pritch","hidden":false},{"_id":"6969e7be844a787c4fdea3d9","name":"Ariel Shamir","hidden":false},{"_id":"6969e7be844a787c4fdea3da","name":"Yedid Hoshen","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66cd9ce36b6031069d23e746/rLOaYqevgHteThNLSPCuq.png"],"publishedAt":"2026-01-15T18:59:53.000Z","submittedOnDailyAt":"2026-01-16T08:13:37.271Z","title":"Alterbute: Editing Intrinsic Attributes of Objects in Images","submittedOnDailyBy":{"_id":"66cd9ce36b6031069d23e746","avatarUrl":"/avatars/9cc2a764273fbb8b1b261f31f0532d06.svg","isPro":false,"fullname":"Tal Reiss","user":"talreiss","type":"user"},"summary":"We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.","upvotes":18,"discussionId":"6969e7be844a787c4fdea3db","projectPage":"https://talreiss.github.io/alterbute/","ai_summary":"Alterbute presents a diffusion-based approach for editing object intrinsic attributes while preserving identity and context through relaxed training objectives and visual named entities for scalable supervision.","ai_keywords":["diffusion-based method","intrinsic attributes","object editing","identity preservation","visual named entities","vision-language model","training objective","extrinsic attributes","intrinsic attribute editing","identity-defining features"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-01-15T13:59:53.000Z","title":"Alterbute: Editing Intrinsic Attributes of Objects in Images","summary":"We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66cd9ce36b6031069d23e746/rLOaYqevgHteThNLSPCuq.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10714.png","numComments":1,"submittedBy":{"_id":"66cd9ce36b6031069d23e746","avatarUrl":"/avatars/9cc2a764273fbb8b1b261f31f0532d06.svg","fullname":"Tal Reiss","name":"talreiss","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.10712","authors":[{"_id":"6969b31632f0333869ff948f","user":{"_id":"6640c4b0b2f118e6197e12d7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6640c4b0b2f118e6197e12d7/I-4leoFMfs4fD0J-5Uuy6.jpeg","isPro":false,"fullname":"changle_qu","user":"ChangleQu","type":"user"},"name":"Changle Qu","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:29.887Z","hidden":false},{"_id":"6969b31632f0333869ff9490","name":"Sunhao Dai","hidden":false},{"_id":"6969b31632f0333869ff9491","name":"Hengyi Cai","hidden":false},{"_id":"6969b31632f0333869ff9492","name":"Jun Xu","hidden":false},{"_id":"6969b31632f0333869ff9493","name":"Shuaiqiang Wang","hidden":false},{"_id":"6969b31632f0333869ff9494","name":"Dawei Yin","hidden":false}],"publishedAt":"2026-01-15T18:59:23.000Z","submittedOnDailyAt":"2026-01-16T01:10:36.436Z","title":"MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching","submittedOnDailyBy":{"_id":"64db88993725f8d9a908c077","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg","isPro":false,"fullname":"Sunhao Dai","user":"KID-22","type":"user"},"summary":"Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.","upvotes":18,"discussionId":"6969b31732f0333869ff9495","projectPage":"https://huggingface.co/collections/ChangleQu/matchtir","githubRepo":"https://github.com/quchangle1/MatchTIR","githubRepoAddedBy":"user","ai_summary":"MatchTIR enhances LLM reasoning by introducing fine-grained credit assignment through bipartite matching and dual-level advantage estimation for tool-integrated tasks.","ai_keywords":["tool-integrated reasoning","reinforcement learning","credit assignment","bipartite matching","turn-level reward assignment","dual-level advantage estimation","trajectory-level signals","long-horizon tasks","multi-turn scenarios"],"githubStars":9,"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}},"publishedAt":"2026-01-15T13:59:23.000Z","title":"MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching","summary":"Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10712.png","numComments":1,"submittedBy":{"_id":"64db88993725f8d9a908c077","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg","fullname":"Sunhao Dai","name":"KID-22","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.10156","authors":[{"_id":"6969b57932f0333869ff9497","user":{"_id":"66632a3d2dc4dff9a98c38a5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66632a3d2dc4dff9a98c38a5/UYk99yT6M1mZviL8d2sm_.jpeg","isPro":false,"fullname":"Yutao Mou","user":"MurrayTom","type":"user"},"name":"Yutao Mou","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:27.949Z","hidden":false},{"_id":"6969b57932f0333869ff9498","name":"Zhangchi Xue","hidden":false},{"_id":"6969b57932f0333869ff9499","name":"Lijun Li","hidden":false},{"_id":"6969b57932f0333869ff949a","name":"Peiyang Liu","hidden":false},{"_id":"6969b57932f0333869ff949b","name":"Shikun Zhang","hidden":false},{"_id":"6969b57932f0333869ff949c","name":"Wei Ye","hidden":false},{"_id":"6969b57932f0333869ff949d","name":"Jing Shao","hidden":false}],"publishedAt":"2026-01-15T07:54:32.000Z","submittedOnDailyAt":"2026-01-16T01:20:36.772Z","title":"ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback","submittedOnDailyBy":{"_id":"641d3efac3983aa9491677b9","avatarUrl":"/avatars/53565486351c16a1ac8ea863963e2d9b.svg","isPro":false,"fullname":"adwardlee","user":"adwardlee","type":"user"},"summary":"While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.","upvotes":18,"discussionId":"6969b57932f0333869ff949e","ai_summary":"A guardrail model and reasoning framework are developed to detect and prevent unsafe tool invocations in LLM agents, improving both safety and task performance under adversarial conditions.","ai_keywords":["LLM-based agents","tool invocation","safety detection","multi-task reinforcement learning","guardrail model","ReAct-style agents","prompt injection attacks"]},"publishedAt":"2026-01-15T02:54:32.000Z","title":"ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback","summary":"While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10156.png","numComments":1,"submittedBy":{"_id":"641d3efac3983aa9491677b9","avatarUrl":"/avatars/53565486351c16a1ac8ea863963e2d9b.svg","fullname":"adwardlee","name":"adwardlee","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.10527","authors":[{"_id":"6969a8ce32f0333869ff93c9","name":"Xingjun Ma","hidden":false},{"_id":"6969a8ce32f0333869ff93ca","name":"Yixu Wang","hidden":false},{"_id":"6969a8ce32f0333869ff93cb","user":{"_id":"634bde123d11eaedd889e277","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1665916392312-noauth.png","isPro":false,"fullname":"Hengyuan Xu","user":"DobyXu","type":"user"},"name":"Hengyuan Xu","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:47.929Z","hidden":false},{"_id":"6969a8ce32f0333869ff93cc","name":"Yutao Wu","hidden":false},{"_id":"6969a8ce32f0333869ff93cd","name":"Yifan Ding","hidden":false},{"_id":"6969a8ce32f0333869ff93ce","name":"Yunhan Zhao","hidden":false},{"_id":"6969a8ce32f0333869ff93cf","name":"Zilong Wang","hidden":false},{"_id":"6969a8ce32f0333869ff93d0","name":"Jiabin Hua","hidden":false},{"_id":"6969a8ce32f0333869ff93d1","name":"Ming Wen","hidden":false},{"_id":"6969a8ce32f0333869ff93d2","name":"Jianan Liu","hidden":false},{"_id":"6969a8ce32f0333869ff93d3","name":"Ranjie Duan","hidden":false},{"_id":"6969a8ce32f0333869ff93d4","name":"Yifeng Gao","hidden":false},{"_id":"6969a8ce32f0333869ff93d5","name":"Yingshui Tan","hidden":false},{"_id":"6969a8ce32f0333869ff93d6","name":"Yunhao Chen","hidden":false},{"_id":"6969a8ce32f0333869ff93d7","name":"Hui Xue","hidden":false},{"_id":"6969a8ce32f0333869ff93d8","user":{"_id":"677d32cf28895e2124065a63","avatarUrl":"/avatars/897fa9fb3b3adc6e60094e3b505f94bc.svg","isPro":false,"fullname":"Xin Wang","user":"xinwang22","type":"user"},"name":"Xin Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:50.416Z","hidden":false},{"_id":"6969a8ce32f0333869ff93d9","name":"Wei Cheng","hidden":false},{"_id":"6969a8ce32f0333869ff93da","name":"Jingjing Chen","hidden":false},{"_id":"6969a8ce32f0333869ff93db","name":"Zuxuan Wu","hidden":false},{"_id":"6969a8ce32f0333869ff93dc","name":"Bo Li","hidden":false},{"_id":"6969a8ce32f0333869ff93dd","name":"Yu-Gang Jiang","hidden":false}],"publishedAt":"2026-01-15T15:52:52.000Z","submittedOnDailyAt":"2026-01-16T00:27:35.940Z","title":"A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5","submittedOnDailyBy":{"_id":"634bde123d11eaedd889e277","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1665916392312-noauth.png","isPro":false,"fullname":"Hengyuan Xu","user":"DobyXu","type":"user"},"summary":"The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.","upvotes":16,"discussionId":"6969a8ce32f0333869ff93de","projectPage":"https://xsafeai.github.io/AI-safety-report/","githubRepo":"https://github.com/XSafeAI/AI-safety-report","githubRepoAddedBy":"user","ai_summary":"Frontier language and vision models show varied safety performance across different evaluation criteria, highlighting the need for comprehensive, standardized safety assessments.","ai_keywords":["Large Language Models","Multimodal Large Language Models","safety evaluation","adversarial evaluation","multilingual evaluation","compliance evaluation","safety leaderboards","model safety profiles"],"githubStars":20},"publishedAt":"2026-01-15T10:52:52.000Z","title":"A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5","summary":"The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10527.png","numComments":1,"submittedBy":{"_id":"634bde123d11eaedd889e277","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1665916392312-noauth.png","fullname":"Hengyuan Xu","name":"DobyXu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.10611","authors":[{"_id":"6969a99532f0333869ff93f9","name":"Christopher Clark","hidden":false},{"_id":"6969a99532f0333869ff93fa","name":"Jieyu Zhang","hidden":false},{"_id":"6969a99532f0333869ff93fb","name":"Zixian Ma","hidden":false},{"_id":"6969a99532f0333869ff93fc","name":"Jae Sung Park","hidden":false},{"_id":"6969a99532f0333869ff93fd","name":"Mohammadreza Salehi","hidden":false},{"_id":"6969a99532f0333869ff93fe","name":"Rohun Tripathi","hidden":false},{"_id":"6969a99532f0333869ff93ff","name":"Sangho Lee","hidden":false},{"_id":"6969a99532f0333869ff9400","user":{"_id":"671fb13479275e038e39426b","avatarUrl":"/avatars/b2247844eab8a70085e4d30bcb3f08d1.svg","isPro":false,"fullname":"Zhongzheng R. Zhang","user":"zhongzhengrenzhang","type":"user"},"name":"Zhongzheng Ren","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:52:08.795Z","hidden":false},{"_id":"6969a99532f0333869ff9401","user":{"_id":"6634537af27c4c67908bcb05","avatarUrl":"/avatars/ec62e187cb45704e428bdd72531f2706.svg","isPro":false,"fullname":"Chris Dongjoo Kim","user":"kimdon20","type":"user"},"name":"Chris Dongjoo Kim","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:52:02.047Z","hidden":false},{"_id":"6969a99532f0333869ff9402","name":"Yinuo Yang","hidden":false},{"_id":"6969a99532f0333869ff9403","name":"Vincent Shao","hidden":false},{"_id":"6969a99532f0333869ff9404","name":"Yue Yang","hidden":false},{"_id":"6969a99532f0333869ff9405","name":"Weikai Huang","hidden":false},{"_id":"6969a99532f0333869ff9406","name":"Ziqi Gao","hidden":false},{"_id":"6969a99532f0333869ff9407","user":{"_id":"65de20ad4e73a7dea7fb4f08","avatarUrl":"/avatars/f3b0ad6cc9417e8ea3f0607fa62824d1.svg","isPro":false,"fullname":"Taira Anderson","user":"tairaa","type":"user"},"name":"Taira Anderson","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:51:47.133Z","hidden":false},{"_id":"6969a99532f0333869ff9408","name":"Jianrui Zhang","hidden":false},{"_id":"6969a99532f0333869ff9409","user":{"_id":"623dfe96dcda6a715304cbca","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/623dfe96dcda6a715304cbca/B7V_IbQNXAcotOKE_mJQ4.jpeg","isPro":false,"fullname":"Jitesh Jain","user":"praeclarumjj3","type":"user"},"name":"Jitesh Jain","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:51:32.979Z","hidden":false},{"_id":"6969a99532f0333869ff940a","user":{"_id":"63f54cc871a5d395c7217575","avatarUrl":"/avatars/b1b72c356d60e1df59449c2fec688596.svg","isPro":false,"fullname":"George Stoica","user":"gstoica3","type":"user"},"name":"George Stoica","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:51:26.838Z","hidden":false},{"_id":"6969a99532f0333869ff940b","name":"Winson Han","hidden":false},{"_id":"6969a99532f0333869ff940c","name":"Ali Farhadi","hidden":false},{"_id":"6969a99532f0333869ff940d","user":{"_id":"66429868ab89e3a3a85668b0","avatarUrl":"/avatars/170e0daa454838deee2bf946f7118651.svg","isPro":false,"fullname":"Ranjay Krishna","user":"ranjaykrishna","type":"user"},"name":"Ranjay Krishna","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:51:09.796Z","hidden":false}],"publishedAt":"2026-01-15T17:27:44.000Z","submittedOnDailyAt":"2026-01-16T00:29:35.966Z","title":"Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).","upvotes":15,"discussionId":"6969a99532f0333869ff940e","ai_summary":"Molmo2 is a new open-source video-language model family that achieves state-of-the-art performance through novel datasets and training methods, particularly excelling in video grounding tasks without relying on proprietary models.","ai_keywords":["video-language models","point-driven grounding","video grounding","object tracking","bi-directional attention","token-weight strategy","vision tokens","message-tree encoding","efficient packing"]},"publishedAt":"2026-01-15T12:27:44.000Z","title":"Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding","summary":"Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10611.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.10103","authors":[{"_id":"6969a9ec32f0333869ff941c","user":{"_id":"63451d2dfeba4bdba59cf9b1","avatarUrl":"/avatars/ff5dd2f3b502c54e7c3e2512c3e98b28.svg","isPro":false,"fullname":"Lizhen Wang","user":"wanglz14","type":"user"},"name":"Lizhen Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:53:38.544Z","hidden":false},{"_id":"6969a9ec32f0333869ff941d","user":{"_id":"668f66c56575435392165c25","avatarUrl":"/avatars/4ebe10abc29229e7862ada6891d1ccbe.svg","isPro":false,"fullname":"Zhu","user":"YongmingZhu","type":"user"},"name":"Yongming Zhu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:53:44.059Z","hidden":false},{"_id":"6969a9ec32f0333869ff941e","user":{"_id":"64645da34b34d49ac7553027","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WCSZ6h5NcPxZIalO7fTr-.jpeg","isPro":false,"fullname":"gezhipeng","user":"gezhipeng","type":"user"},"name":"Zhipeng Ge","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:53:54.283Z","hidden":false},{"_id":"6969a9ec32f0333869ff941f","user":{"_id":"64b50310a17e4a0519f0e42d","avatarUrl":"/avatars/3e272e679386893f1b2585f9431a390f.svg","isPro":false,"fullname":"Youwei Zheng","user":"figoyouwei","type":"user"},"name":"Youwei Zheng","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:54:01.102Z","hidden":false},{"_id":"6969a9ec32f0333869ff9420","user":{"_id":"602e78801f993496bc14d9c9","avatarUrl":"/avatars/eff73f60e6652d107cebeda3b2b87fe1.svg","isPro":false,"fullname":"Longhao Zhang","user":"zhanglonghao","type":"user"},"name":"Longhao Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:54:06.573Z","hidden":false},{"_id":"6969a9ec32f0333869ff9421","name":"Tianshu Hu","hidden":false},{"_id":"6969a9ec32f0333869ff9422","user":{"_id":"66547a30fdf911031c6c4c5d","avatarUrl":"/avatars/857da10b91053fc7f4f2c7839b73778f.svg","isPro":false,"fullname":"shiyangqin","user":"shiyangqin","type":"user"},"name":"Shiyang Qin","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:54:15.603Z","hidden":false},{"_id":"6969a9ec32f0333869ff9423","user":{"_id":"6178ea05267320abb99df778","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1636104707106-6178ea05267320abb99df778.jpeg","isPro":false,"fullname":"Mingshuang Luo","user":"luomingshuang","type":"user"},"name":"Mingshuang Luo","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:54:24.482Z","hidden":false},{"_id":"6969a9ec32f0333869ff9424","name":"Jiaxu Zhang","hidden":false},{"_id":"6969a9ec32f0333869ff9425","name":"Xin Chen","hidden":false},{"_id":"6969a9ec32f0333869ff9426","name":"Yulong Wang","hidden":false},{"_id":"6969a9ec32f0333869ff9427","user":{"_id":"68ae7924f4dfb86cc1f1bfcd","avatarUrl":"/avatars/4ceb01ab039d3a14f247ca21dc20c4ad.svg","isPro":false,"fullname":"Zerong Zheng","user":"zerongzheng","type":"user"},"name":"Zerong Zheng","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:54:42.104Z","hidden":false},{"_id":"6969a9ec32f0333869ff9428","name":"Jianwen Jiang","hidden":false},{"_id":"6969a9ec32f0333869ff9429","name":"Chao Liang","hidden":false},{"_id":"6969a9ec32f0333869ff942a","user":{"_id":"67f13c1dec600c92da40e8d0","avatarUrl":"/avatars/f72e616b6b3dca5d952968b07a940b1e.svg","isPro":false,"fullname":"weifeng chen","user":"dsafsdf234234","type":"user"},"name":"Weifeng Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:54:57.901Z","hidden":false},{"_id":"6969a9ec32f0333869ff942b","name":"Xing Wang","hidden":false},{"_id":"6969a9ec32f0333869ff942c","name":"Yuan Zhang","hidden":false},{"_id":"6969a9ec32f0333869ff942d","user":{"_id":"671aa30b496f0bc5ae04da4b","avatarUrl":"/avatars/902d7f9fd56f84953d67d9229bd9d6b7.svg","isPro":false,"fullname":"Mingyuan Gao","user":"GMY1999","type":"user"},"name":"Mingyuan Gao","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:55:05.827Z","hidden":false}],"publishedAt":"2026-01-15T06:16:22.000Z","submittedOnDailyAt":"2026-01-16T00:31:06.568Z","title":"FlowAct-R1: Towards Interactive Humanoid Video Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.","upvotes":14,"discussionId":"6969a9ed32f0333869ff942e","ai_summary":"FlowAct-R1 enables real-time interactive humanoid video generation with high-fidelity synthesis and low-latency responsiveness through MMDiT architecture and chunkwise diffusion forcing strategies.","ai_keywords":["MMDiT","diffusion models","video synthesis","real-time interaction","chunkwise diffusion forcing","self-forcing","temporal consistency","efficient distillation","time-to-first-frame","full-body control","behavioral states"],"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}},"publishedAt":"2026-01-15T01:16:22.000Z","title":"FlowAct-R1: Towards Interactive Humanoid Video Generation","summary":"Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10103.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.09881","authors":[{"_id":"6969a8e332f0333869ff93e0","name":"Weili Nie","hidden":false},{"_id":"6969a8e332f0333869ff93e1","user":{"_id":"6966f44db6dd7c2164e88c0f","avatarUrl":"/avatars/daa55039e17f6e1139a0b4bab8d8f9d9.svg","isPro":false,"fullname":"Julius Berner","user":"julberner","type":"user"},"name":"Julius Berner","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:45.611Z","hidden":false},{"_id":"6969a8e332f0333869ff93e2","user":{"_id":"66c398fc5f4422f886b71a00","avatarUrl":"/avatars/9cd690d7857de1b926ddcdc2bccbfdfa.svg","isPro":false,"fullname":"Nanye Ma","user":"willllis","type":"user"},"name":"Nanye Ma","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:53:19.433Z","hidden":false},{"_id":"6969a8e332f0333869ff93e3","name":"Chao Liu","hidden":false},{"_id":"6969a8e332f0333869ff93e4","user":{"_id":"6596422646624a86ff3b3bda","avatarUrl":"/avatars/216e12b77e45ac5f1fa20932f5745411.svg","isPro":false,"fullname":"Saining Xie","user":"sainx","type":"user"},"name":"Saining Xie","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:53:27.908Z","hidden":false},{"_id":"6969a8e332f0333869ff93e5","name":"Arash Vahdat","hidden":false}],"publishedAt":"2026-01-14T21:30:03.000Z","submittedOnDailyAt":"2026-01-16T01:01:57.721Z","title":"Transition Matching Distillation for Fast Video Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd","upvotes":14,"discussionId":"6969a8e432f0333869ff93e6","ai_summary":"Transition Matching Distillation enables efficient video generation by distilling diffusion models into few-step predictors using conditional flows and semantic representation decomposition.","ai_keywords":["video diffusion models","distillation","denoising trajectory","conditional flow","flow head","diffusion backbone","transition matching","video generation","semantic representations","distribution matching distillation"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-01-14T16:30:03.000Z","title":"Transition Matching Distillation for Fast Video Generation","summary":"Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09881.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.10657","authors":[{"_id":"6969aaf232f0333869ff9430","user":{"_id":"64de3c145e192985054c74c3","avatarUrl":"/avatars/bc07670392fa7726412d48e8c8c2c8c0.svg","isPro":false,"fullname":"Minghao Yan","user":"minghaoyan","type":"user"},"name":"Minghao Yan","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:43.551Z","hidden":false},{"_id":"6969aaf232f0333869ff9431","name":"Bo Peng","hidden":false},{"_id":"6969aaf232f0333869ff9432","user":{"_id":"665a131a08cba81fb74d745a","avatarUrl":"/avatars/fc7e216b5abb556762edf4fcdbd9aad0.svg","isPro":false,"fullname":"Benjamin Coleman","user":"BenjaminColeman305","type":"user"},"name":"Benjamin Coleman","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:49:04.553Z","hidden":false},{"_id":"6969aaf232f0333869ff9433","name":"Ziqi Chen","hidden":false},{"_id":"6969aaf232f0333869ff9434","user":{"_id":"6187199bb1085ab638324e82","avatarUrl":"/avatars/02281e4e5dc260fff62de34589f42b7d.svg","isPro":false,"fullname":"zhouhang xie","user":"zhohanx","type":"user"},"name":"Zhouhang Xie","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:49:15.754Z","hidden":false},{"_id":"6969aaf232f0333869ff9435","user":{"_id":"64daab70c38427829daf5958","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cjL27oSvuJf1x0Zq3SrEJ.jpeg","isPro":false,"fullname":"Zhankui He","user":"ZhankuiHe","type":"user"},"name":"Zhankui He","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:49:24.778Z","hidden":false},{"_id":"6969aaf232f0333869ff9436","name":"Noveen Sachdeva","hidden":false},{"_id":"6969aaf232f0333869ff9437","user":{"_id":"683aa7ffb13aecd29bddac74","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rFG39yrXagxE15Yqr2NKb.png","isPro":false,"fullname":"Isabella Ye","user":"isye4","type":"user"},"name":"Isabella Ye","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:49:30.522Z","hidden":false},{"_id":"6969aaf232f0333869ff9438","name":"Weili Wang","hidden":false},{"_id":"6969aaf232f0333869ff9439","name":"Chi Wang","hidden":false},{"_id":"6969aaf232f0333869ff943a","name":"Ed H. Chi","hidden":false},{"_id":"6969aaf232f0333869ff943b","name":"Wang-Cheng Kang","hidden":false},{"_id":"6969aaf232f0333869ff943c","name":"Derek Zhiyuan Cheng","hidden":false},{"_id":"6969aaf232f0333869ff943d","name":"Beidou Wang","hidden":false}],"publishedAt":"2026-01-15T18:25:23.000Z","submittedOnDailyAt":"2026-01-16T12:36:02.902Z","title":"PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution","submittedOnDailyBy":{"_id":"64de3c145e192985054c74c3","avatarUrl":"/avatars/bc07670392fa7726412d48e8c8c2c8c0.svg","isPro":false,"fullname":"Minghao Yan","user":"minghaoyan","type":"user"},"summary":"Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent's context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.","upvotes":11,"discussionId":"6969aaf232f0333869ff943e","ai_summary":"PACEvolve framework addresses key failure modes in LLM evolutionary search through hierarchical context management, momentum-based backtracking, and adaptive sampling policies for improved self-improvement and solution discovery.","ai_keywords":["evolutionary search","large language models","context pollution","mode collapse","weak collaboration","Progress-Aware Consistent Evolution","hierarchical context management","momentum-based backtracking","self-adaptive sampling policy","cross-trajectory collaboration"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-01-15T13:25:23.000Z","title":"PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution","summary":"Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent's context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10657.png","numComments":1,"submittedBy":{"_id":"64de3c145e192985054c74c3","avatarUrl":"/avatars/bc07670392fa7726412d48e8c8c2c8c0.svg","fullname":"Minghao Yan","name":"minghaoyan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.10592","authors":[{"_id":"6969b18332f0333869ff9484","user":{"_id":"630491107424d937fa3258be","avatarUrl":"/avatars/b8bd81bc8544674ee26b78702afdb87c.svg","isPro":true,"fullname":"Delong Chen","user":"chendelong","type":"user"},"name":"Delong Chen","status":"claimed_verified","statusLastChangedAt":"2026-01-16T14:37:03.915Z","hidden":false},{"_id":"6969b18332f0333869ff9485","user":{"_id":"67a4de92a41dcedc1632eb43","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rKpXABi8RqSeRh2RXBlZU.png","isPro":false,"fullname":"Tejaswi Kasarla","user":"tkasarla","type":"user"},"name":"Tejaswi Kasarla","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:52:21.274Z","hidden":false},{"_id":"6969b18332f0333869ff9486","name":"Yejin Bang","hidden":false},{"_id":"6969b18332f0333869ff9487","user":{"_id":"62bdeedd01dc22b4d22a371e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62bdeedd01dc22b4d22a371e/ahbK9Ehurx1TgQAVw1TcS.jpeg","isPro":false,"fullname":"Mustafa Shukor","user":"mshukor","type":"user"},"name":"Mustafa Shukor","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:52:30.924Z","hidden":false},{"_id":"6969b18332f0333869ff9488","user":{"_id":"681abde923dbf6b59c410214","avatarUrl":"/avatars/f863d0a3d05fc99b29535d797d7a6292.svg","isPro":false,"fullname":"willy Chung","user":"willy1027","type":"user"},"name":"Willy Chung","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:52:37.777Z","hidden":false},{"_id":"6969b18332f0333869ff9489","name":"Jade Yu","hidden":false},{"_id":"6969b18332f0333869ff948a","user":{"_id":"65f08f0bcd30f1ed6ea6cbd1","avatarUrl":"/avatars/5eb2c2032dc2054d4ca86665ba0d428c.svg","isPro":false,"fullname":"Allen Bolourchi","user":"allen-ml","type":"user"},"name":"Allen Bolourchi","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:52:48.723Z","hidden":false},{"_id":"6969b18332f0333869ff948b","user":{"_id":"639111acb73a0f8c02a92844","avatarUrl":"/avatars/58766701471be66fd784bedd319741a4.svg","isPro":false,"fullname":"Theo Moutakanni","user":"TheoM","type":"user"},"name":"Theo Moutakanni","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:52:55.020Z","hidden":false},{"_id":"6969b18332f0333869ff948c","name":"Pascale Fung","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/G3oU0qIaKddVQVgjhv5A2.gif"],"publishedAt":"2026-01-15T17:02:27.000Z","submittedOnDailyAt":"2026-01-16T01:03:48.816Z","title":"Action100M: A Large-scale Video Action Dataset","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.","upvotes":10,"discussionId":"6969b18332f0333869ff948d","githubRepo":"https://github.com/facebookresearch/Action100M","githubRepoAddedBy":"user","ai_summary":"Action100M is a large-scale video action dataset constructed from internet instructional videos using automated pipelines with V-JEPA embeddings and GPT-based reasoning for structured annotations.","ai_keywords":["Action100M","V-JEPA","Tree-of-Captions","GPT-OSS-120B","Self-Refine","video action recognition","zero-shot performance","data-scaling improvements"],"githubStars":75,"organization":{"_id":"66b54027408752ae16404b05","name":"metaresearch","fullname":"Meta Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"}},"publishedAt":"2026-01-15T12:02:27.000Z","title":"Action100M: A Large-scale Video Action Dataset","summary":"Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/G3oU0qIaKddVQVgjhv5A2.gif"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10592.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"organization":{"_id":"66b54027408752ae16404b05","name":"metaresearch","fullname":"Meta Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.10131","authors":[{"_id":"696abd9b3f1837bfb89705e4","name":"Yizhan Li","hidden":false},{"_id":"696abd9b3f1837bfb89705e5","name":"Florence Cloutier","hidden":false},{"_id":"696abd9b3f1837bfb89705e6","name":"Sifan Wu","hidden":false},{"_id":"696abd9b3f1837bfb89705e7","name":"Ali Parviz","hidden":false},{"_id":"696abd9b3f1837bfb89705e8","name":"Boris Knyazev","hidden":false},{"_id":"696abd9b3f1837bfb89705e9","name":"Yan Zhang","hidden":false},{"_id":"696abd9b3f1837bfb89705ea","name":"Glen Berseth","hidden":false},{"_id":"696abd9b3f1837bfb89705eb","name":"Bang Liu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/7qJmDYnOweo-35j1pTG4M.png"],"publishedAt":"2026-01-15T07:18:05.000Z","submittedOnDailyAt":"2026-01-16T20:09:00.165Z","title":"M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints","submittedOnDailyBy":{"_id":"654a97282d2fcd6bf2851173","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png","isPro":false,"fullname":"Bang Liu","user":"Bang-UdeM-Mila","type":"user"},"summary":"Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce M olGen, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.","upvotes":9,"discussionId":"696abd9b3f1837bfb89705ec","ai_summary":"A fragment-level, retrieval-augmented framework with multi-agent reasoning and GRPO-trained optimization enables precise molecular generation under multiple physicochemical constraints.","ai_keywords":["large language models","multi-agent reasoner","fragment-level edits","retrieval-augmented generation","Group Relative Policy Optimization","multi-property constraints","molecular generation","physicochemical properties","QED","LogP","Molecular Weight","HOMO","LUMO"],"organization":{"_id":"6227f7c251f12089212890c7","name":"UdeM","fullname":"University of Montreal"}},"publishedAt":"2026-01-15T02:18:05.000Z","title":"M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints","summary":"Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce M olGen, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/7qJmDYnOweo-35j1pTG4M.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10131.png","numComments":1,"submittedBy":{"_id":"654a97282d2fcd6bf2851173","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png","fullname":"Bang Liu","name":"Bang-UdeM-Mila","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":12,"isUserFollowing":false},"organization":{"_id":"6227f7c251f12089212890c7","name":"UdeM","fullname":"University of Montreal"},"isAuthorParticipating":false},{"paper":{"id":"2601.10547","authors":[{"_id":"6969e2dc32f0333869ff952e","user":{"_id":"63c7636b656e7822e23e6f6b","avatarUrl":"/avatars/41bfd5e1ce6daab6058eacfd33c7a268.svg","isPro":true,"fullname":"Dongchao Yang","user":"Dongchao","type":"user"},"name":"Dongchao Yang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:43:00.669Z","hidden":false},{"_id":"6969e2dc32f0333869ff952f","user":{"_id":"66a1983418ce1e6e432fc0d3","avatarUrl":"/avatars/5f2fd2427057eaf1de5c3c2ce6d19c22.svg","isPro":false,"fullname":"Yuxin XIE","user":"bverxie","type":"user"},"name":"Yuxin Xie","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:43:07.620Z","hidden":false},{"_id":"6969e2dc32f0333869ff9530","name":"Yuguo Yin","hidden":false},{"_id":"6969e2dc32f0333869ff9531","name":"Zheyu Wang","hidden":false},{"_id":"6969e2dc32f0333869ff9532","name":"Xiaoyu Yi","hidden":false},{"_id":"6969e2dc32f0333869ff9533","user":{"_id":"643de56c0efe9e9de09a720e","avatarUrl":"/avatars/1911ee9f28a328f169b7fcb380890739.svg","isPro":false,"fullname":"Gongxi Zhu","user":"Gongxizhu","type":"user"},"name":"Gongxi Zhu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:43:33.999Z","hidden":false},{"_id":"6969e2dc32f0333869ff9534","name":"Xiaolong Weng","hidden":false},{"_id":"6969e2dc32f0333869ff9535","name":"Zihan Xiong","hidden":false},{"_id":"6969e2dc32f0333869ff9536","name":"Yingzhe Ma","hidden":false},{"_id":"6969e2dc32f0333869ff9537","name":"Dading Cong","hidden":false},{"_id":"6969e2dc32f0333869ff9538","name":"Jingliang Liu","hidden":false},{"_id":"6969e2dc32f0333869ff9539","name":"Zihang Huang","hidden":false},{"_id":"6969e2dc32f0333869ff953a","name":"Jinghan Ru","hidden":false},{"_id":"6969e2dc32f0333869ff953b","name":"Rongjie Huang","hidden":false},{"_id":"6969e2dc32f0333869ff953c","name":"Haoran Wan","hidden":false},{"_id":"6969e2dc32f0333869ff953d","name":"Peixu Wang","hidden":false},{"_id":"6969e2dc32f0333869ff953e","name":"Kuoxi Yu","hidden":false},{"_id":"6969e2dc32f0333869ff953f","name":"Helin Wang","hidden":false},{"_id":"6969e2dc32f0333869ff9540","name":"Liming Liang","hidden":false},{"_id":"6969e2dc32f0333869ff9541","name":"Xianwei Zhuang","hidden":false},{"_id":"6969e2dc32f0333869ff9542","user":{"_id":"64e85f3572c2b9493fc09fcb","avatarUrl":"/avatars/673626e7e0c7fdf2e5e6e98422cdad3d.svg","isPro":false,"fullname":"Wang","user":"YuanyuanWang","type":"user"},"name":"Yuanyuan Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:44:41.696Z","hidden":false},{"_id":"6969e2dc32f0333869ff9543","user":{"_id":"63783d9d84318944acd305c4","avatarUrl":"/avatars/cafd4fd0287e77cc6ebf37c8c8509174.svg","isPro":false,"fullname":"Haohan Guo","user":"hhguo","type":"user"},"name":"Haohan Guo","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:44:26.552Z","hidden":false},{"_id":"6969e2dc32f0333869ff9544","name":"Junjie Cao","hidden":false},{"_id":"6969e2dc32f0333869ff9545","name":"Zeqian Ju","hidden":false},{"_id":"6969e2dc32f0333869ff9546","name":"Songxiang Liu","hidden":false},{"_id":"6969e2dc32f0333869ff9547","name":"Yuewen Cao","hidden":false},{"_id":"6969e2dc32f0333869ff9548","name":"Heming Weng","hidden":false},{"_id":"6969e2dc32f0333869ff9549","name":"Yuexian Zou","hidden":false}],"publishedAt":"2026-01-15T16:14:25.000Z","submittedOnDailyAt":"2026-01-16T07:25:07.606Z","title":"HeartMuLa: A Family of Open Sourced Music Foundation Models","submittedOnDailyBy":{"_id":"63c7636b656e7822e23e6f6b","avatarUrl":"/avatars/41bfd5e1ce6daab6058eacfd33c7a268.svg","isPro":true,"fullname":"Dongchao Yang","user":"Dongchao","type":"user"},"summary":"We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.","upvotes":8,"discussionId":"6969e2dc32f0333869ff954a","ai_summary":"A suite of open-source music foundation models is introduced, featuring components for audio-text alignment, lyric recognition, music coding, and large language model-based song generation with controllable attributes and scalable parameterization.","ai_keywords":["Music Foundation Models","audio-text alignment model","lyric recognition model","music codec tokenizer","LLM-based song generation model","autoregressive modeling","musical attribute control","short video background music generation","parameter scaling"]},"publishedAt":"2026-01-15T11:14:25.000Z","title":"HeartMuLa: A Family of Open Sourced Music Foundation Models","summary":"We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10547.png","numComments":1,"submittedBy":{"_id":"63c7636b656e7822e23e6f6b","avatarUrl":"/avatars/41bfd5e1ce6daab6058eacfd33c7a268.svg","fullname":"Dongchao Yang","name":"Dongchao","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.10553","authors":[{"_id":"6969a9ad32f0333869ff9410","name":"Jianhao Yuan","hidden":false},{"_id":"6969a9ad32f0333869ff9411","name":"Xiaofeng Zhang","hidden":false},{"_id":"6969a9ad32f0333869ff9412","name":"Felix Friedrich","hidden":false},{"_id":"6969a9ad32f0333869ff9413","name":"Nicolas Beltran-Velez","hidden":false},{"_id":"6969a9ad32f0333869ff9414","name":"Melissa Hall","hidden":false},{"_id":"6969a9ad32f0333869ff9415","name":"Reyhane Askari-Hemmat","hidden":false},{"_id":"6969a9ad32f0333869ff9416","name":"Xiaochuang Han","hidden":false},{"_id":"6969a9ad32f0333869ff9417","name":"Nicolas Ballas","hidden":false},{"_id":"6969a9ad32f0333869ff9418","name":"Michal Drozdzal","hidden":false},{"_id":"6969a9ad32f0333869ff9419","name":"Adriana Romero-Soriano","hidden":false}],"publishedAt":"2026-01-15T16:18:00.000Z","submittedOnDailyAt":"2026-01-16T00:30:01.607Z","title":"Inference-time Physics Alignment of Video Generative Models with Latent World Models","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.","upvotes":7,"discussionId":"6969a9ad32f0333869ff941a","ai_summary":"Latent world models enhance video generation physics plausibility through inference-time alignment and trajectory steering, achieving superior performance in challenging benchmarks.","ai_keywords":["video generative models","physics plausibility","latent world model","VJEPA-2","test-time compute","denoising trajectories","inference-time alignment","Perception Test PhysicsIQ Challenge"],"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}},"publishedAt":"2026-01-15T11:18:00.000Z","title":"Inference-time Physics Alignment of Video Generative Models with Latent World Models","summary":"State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10553.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.09142","authors":[{"_id":"696861ca0ac10a06522f6a37","user":{"_id":"6457709270b1337109951a7f","avatarUrl":"/avatars/ac2fd0806e027b95c5bb1ae98dd3d220.svg","isPro":false,"fullname":"MaShijian","user":"FutureMa","type":"user"},"name":"Shijian Ma","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:00.930Z","hidden":false},{"_id":"696861ca0ac10a06522f6a38","name":"Yan Lin","hidden":false},{"_id":"696861ca0ac10a06522f6a39","name":"Yi Yang","hidden":false}],"publishedAt":"2026-01-14T04:26:43.000Z","submittedOnDailyAt":"2026-01-16T00:37:19.403Z","title":"EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge","submittedOnDailyBy":{"_id":"6457709270b1337109951a7f","avatarUrl":"/avatars/ac2fd0806e027b95c5bb1ae98dd3d220.svg","isPro":false,"fullname":"MaShijian","user":"FutureMa","type":"user"},"summary":"Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.","upvotes":7,"discussionId":"696861cb0ac10a06522f6a3a","ai_summary":"EvasionBench introduces a large-scale benchmark for detecting evasive responses in earnings calls using a multi-model annotation framework that leverages disagreement between advanced language models to identify challenging examples, resulting in a highly accurate model with significantly reduced inference costs.","ai_keywords":["multi-model annotation framework","frontier LLMs","disagreement mining","implicit regularization","judge-resolved samples","evasive answers","earnings calls","financial transparency","Cohen's Kappa","LLM performance","parameter-efficient fine-tuning"]},"publishedAt":"2026-01-13T23:26:43.000Z","title":"EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge","summary":"Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09142.png","numComments":2,"submittedBy":{"_id":"6457709270b1337109951a7f","avatarUrl":"/avatars/ac2fd0806e027b95c5bb1ae98dd3d220.svg","fullname":"MaShijian","name":"FutureMa","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.08881","authors":[{"_id":"6968839e0ac10a06522f6ad4","name":"Yu Xu","hidden":false},{"_id":"6968839e0ac10a06522f6ad5","user":{"_id":"641c35366d71bace63abd20e","avatarUrl":"/avatars/a919a439d7eb975febb183f41a0e6fc9.svg","isPro":false,"fullname":"Yana-Hangabina","user":"Yana-Hangabina","type":"user"},"name":"Hongbin Yan","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:33.652Z","hidden":false},{"_id":"6968839e0ac10a06522f6ad6","name":"Juan Cao","hidden":false},{"_id":"6968839e0ac10a06522f6ad7","name":"Yiji Cheng","hidden":false},{"_id":"6968839e0ac10a06522f6ad8","name":"Tiankai Hang","hidden":false},{"_id":"6968839e0ac10a06522f6ad9","name":"Runze He","hidden":false},{"_id":"6968839e0ac10a06522f6ada","name":"Zijin Yin","hidden":false},{"_id":"6968839e0ac10a06522f6adb","name":"Shiyi Zhang","hidden":false},{"_id":"6968839e0ac10a06522f6adc","name":"Yuxin Zhang","hidden":false},{"_id":"6968839e0ac10a06522f6add","name":"Jintao Li","hidden":false},{"_id":"6968839e0ac10a06522f6ade","name":"Chunyu Wang","hidden":false},{"_id":"6968839e0ac10a06522f6adf","name":"Qinglin Lu","hidden":false},{"_id":"6968839e0ac10a06522f6ae0","name":"Tong-Yee Lee","hidden":false},{"_id":"6968839e0ac10a06522f6ae1","name":"Fan Tang","hidden":false}],"publishedAt":"2026-01-12T14:52:45.000Z","submittedOnDailyAt":"2026-01-16T03:40:32.897Z","title":"TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts","submittedOnDailyBy":{"_id":"641c35366d71bace63abd20e","avatarUrl":"/avatars/a919a439d7eb975febb183f41a0e6fc9.svg","isPro":false,"fullname":"Yana-Hangabina","user":"Yana-Hangabina","type":"user"},"summary":"Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.","upvotes":6,"discussionId":"6968839e0ac10a06522f6ae2","projectPage":"https://yuci-gpt.github.io/TAG-MoE/","ai_summary":"A novel framework injects semantic intent into Mixture-of-Experts routing for image generation and editing, resolving task interference through hierarchical task annotation and predictive alignment regularization.","ai_keywords":["Mixture-of-Experts","diffusion transformers","task interference","gating networks","semantic intent","hierarchical task semantic annotation","predictive alignment regularization","task-specific specialization"]},"publishedAt":"2026-01-12T09:52:45.000Z","title":"TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts","summary":"Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08881.png","numComments":1,"submittedBy":{"_id":"641c35366d71bace63abd20e","avatarUrl":"/avatars/a919a439d7eb975febb183f41a0e6fc9.svg","fullname":"Yana-Hangabina","name":"Yana-Hangabina","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.10201","authors":[{"_id":"6969bbc432f0333869ff94b8","user":{"_id":"66f8689725464a7989b75845","avatarUrl":"/avatars/43a61a528c5779103eaf5687ba44ee14.svg","isPro":false,"fullname":"Jiarui Yao","user":"FlippyDora","type":"user"},"name":"Jiarui Yao","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:48:39.837Z","hidden":false},{"_id":"6969bbc432f0333869ff94b9","user":{"_id":"6618d1c3c1167c8d8702b19d","avatarUrl":"/avatars/24611ca6c4158f4978ac8476a87d8d9c.svg","isPro":false,"fullname":"Ruida WANG","user":"RickyDeSkywalker","type":"user"},"name":"Ruida Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:48:45.697Z","hidden":false},{"_id":"6969bbc432f0333869ff94ba","user":{"_id":"64afcb35a80174e17e924422","avatarUrl":"/avatars/e8ed10280b80cc7fd20048e6c2a192ee.svg","isPro":false,"fullname":"Tong Zhang","user":"TongZhang","type":"user"},"name":"Tong Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:48:53.695Z","hidden":false}],"publishedAt":"2026-01-15T09:01:53.000Z","submittedOnDailyAt":"2026-01-16T01:48:59.489Z","title":"PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary","submittedOnDailyBy":{"_id":"66f8689725464a7989b75845","avatarUrl":"/avatars/43a61a528c5779103eaf5687ba44ee14.svg","isPro":false,"fullname":"Jiarui Yao","user":"FlippyDora","type":"user"},"summary":"Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.","upvotes":5,"discussionId":"6969bbc432f0333869ff94bb","githubRepo":"https://github.com/MaxwellJryao/Process-Reward-Learning","githubRepoAddedBy":"user","ai_summary":"Process Reward Learning decomposes reinforcement learning objectives into intermediate steps to provide fine-grained supervision for improving large language model reasoning abilities.","ai_keywords":["Large Language Models","reinforcement learning","entropy regularization","reward maximization","KL-divergence penalty","process rewards","reasoning ability","pass@n","average@n"],"githubStars":4},"publishedAt":"2026-01-15T04:01:53.000Z","title":"PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary","summary":"Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10201.png","numComments":1,"submittedBy":{"_id":"66f8689725464a7989b75845","avatarUrl":"/avatars/43a61a528c5779103eaf5687ba44ee14.svg","fullname":"Jiarui Yao","name":"FlippyDora","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.06431","authors":[{"_id":"6969d8c032f0333869ff950e","name":"Qingyu Ren","hidden":false},{"_id":"6969d8c032f0333869ff950f","name":"Qianyu He","hidden":false},{"_id":"6969d8c032f0333869ff9510","user":{"_id":"67ff4ecb58ed263257befc9e","avatarUrl":"/avatars/3b903411404e7c75d228afaaa88285ff.svg","isPro":false,"fullname":"JingwenChang","user":"Chang992","type":"user"},"name":"Jingwen Chang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:50:07.361Z","hidden":false},{"_id":"6969d8c032f0333869ff9511","name":"Jie Zeng","hidden":false},{"_id":"6969d8c032f0333869ff9512","user":{"_id":"634014022c72fd4ca251fed7","avatarUrl":"/avatars/32a85d41f7435f7facfd580d9d124c33.svg","isPro":false,"fullname":"liangjiaqing","user":"liangjiaqing","type":"user"},"name":"Jiaqing Liang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:50:27.203Z","hidden":false},{"_id":"6969d8c032f0333869ff9513","name":"Yanghua Xiao","hidden":false},{"_id":"6969d8c032f0333869ff9514","name":"Han Xia","hidden":false},{"_id":"6969d8c032f0333869ff9515","user":{"_id":"635f79e9514c057dd4db5b27","avatarUrl":"/avatars/ba758201e9ff70a020899598c3b9d6f4.svg","isPro":false,"fullname":"zeye sun","user":"sunzeyeah","type":"user"},"name":"Zeye Sun","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:50:42.120Z","hidden":false},{"_id":"6969d8c032f0333869ff9516","name":"Fei Yu","hidden":false}],"publishedAt":"2026-01-10T05:11:38.000Z","submittedOnDailyAt":"2026-01-16T03:52:53.415Z","title":"LSRIF: Logic-Structured Reinforcement Learning for Instruction Following","submittedOnDailyBy":{"_id":"65702537933a5eae4c5b0193","avatarUrl":"/avatars/1c0618eae6828bd841f03d7b6223b086.svg","isPro":false,"fullname":"rain","user":"dd12345789","type":"user"},"summary":"Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.","upvotes":5,"discussionId":"6969d8c032f0333869ff9517","ai_summary":"A logic-structured training framework explicitly models instruction logic through constraint-aware reward mechanisms, improving instruction-following and reasoning capabilities in large language models.","ai_keywords":["instruction-following","logical structures","sequential dependencies","conditional branching","LSRInstruct","LSRIF","structure-aware rewarding","parallel structures","sequential structures","conditional branches","attention layers","token-level attention"],"organization":{"_id":"67c1d682826160b28f778510","name":"antgroup","fullname":"Ant Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}},"publishedAt":"2026-01-10T00:11:38.000Z","title":"LSRIF: Logic-Structured Reinforcement Learning for Instruction Following","summary":"Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06431.png","numComments":1,"submittedBy":{"_id":"65702537933a5eae4c5b0193","avatarUrl":"/avatars/1c0618eae6828bd841f03d7b6223b086.svg","fullname":"rain","name":"dd12345789","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67c1d682826160b28f778510","name":"antgroup","fullname":"Ant Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.10129","authors":[{"_id":"6969b6df32f0333869ff94a0","name":"Linquan Wu","hidden":false},{"_id":"6969b6df32f0333869ff94a1","user":{"_id":"6502f42b2f87793060db18b4","avatarUrl":"/avatars/de12b6cd34a6149783fba39a947f4295.svg","isPro":false,"fullname":"Tianxiang Jiang","user":"txjiang","type":"user"},"name":"Tianxiang Jiang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:47:56.320Z","hidden":false},{"_id":"6969b6df32f0333869ff94a2","user":{"_id":"669e136f5bd3f749a330b5b8","avatarUrl":"/avatars/cf76b1c6d5cdc1eb189e19c4c62d904c.svg","isPro":false,"fullname":"Yifei Dong","user":"fly1113","type":"user"},"name":"Yifei Dong","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:47:45.882Z","hidden":false},{"_id":"6969b6df32f0333869ff94a3","user":{"_id":"679770cc0972df3a95b9d6ba","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/679770cc0972df3a95b9d6ba/bAC6fGb1pXeGUSu4tG3YJ.jpeg","isPro":false,"fullname":"Haoyu Yang","user":"afunnyhy","type":"user"},"name":"Haoyu Yang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:48:11.448Z","hidden":false},{"_id":"6969b6df32f0333869ff94a4","name":"Fengji Zhang","hidden":false},{"_id":"6969b6df32f0333869ff94a5","name":"Shichaang Meng","hidden":false},{"_id":"6969b6df32f0333869ff94a6","name":"Ai Xuan","hidden":false},{"_id":"6969b6df32f0333869ff94a7","name":"Linqi Song","hidden":false},{"_id":"6969b6df32f0333869ff94a8","user":{"_id":"67c6ec64c60daca4757e5821","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hXpsfp_QNxm4LQRkudPq-.png","isPro":false,"fullname":"Jacky Keung","user":"jackykeung","type":"user"},"name":"Jacky Keung","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:46:46.552Z","hidden":false}],"publishedAt":"2026-01-15T07:14:24.000Z","submittedOnDailyAt":"2026-01-16T01:35:21.398Z","title":"LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning","submittedOnDailyBy":{"_id":"6744754ff9940208b97a6a9a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6744754ff9940208b97a6a9a/PRG6_0jAfsj0uoUJvKyWf.png","isPro":false,"fullname":"Arsenever","user":"Eurayka","type":"user"},"summary":"Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.","upvotes":4,"discussionId":"6969b6df32f0333869ff94a9","ai_summary":"LaViT addresses the perception gap in multimodal reasoning by aligning latent visual thoughts through autoregressive reconstruction of visual semantics and attention trajectories, improving visual grounding and model performance.","ai_keywords":["multimodal latent reasoning","external supervision","intrinsic visual attention dynamics","Perception Gap","distillation","student models","teacher models","textual output","visual regions","language priors","grounded perception","LaViT","latent visual thoughts","autoregressive reconstruction","visual semantics","attention trajectories","curriculum sensory gating mechanism","shortcut learning","complex reasoning tasks","visual grounding"]},"publishedAt":"2026-01-15T02:14:24.000Z","title":"LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning","summary":"Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10129.png","numComments":1,"submittedBy":{"_id":"6744754ff9940208b97a6a9a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6744754ff9940208b97a6a9a/PRG6_0jAfsj0uoUJvKyWf.png","fullname":"Arsenever","name":"Eurayka","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.09876","authors":[{"_id":"6969a43732f0333869ff93b5","user":{"_id":"673ba2b141d69ace67cf289f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hiC2TM2y2oz5X-slJdMy9.png","isPro":false,"fullname":"YIFEI SHEN","user":"yifeis02","type":"user"},"name":"Yifei Shen","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:29:53.419Z","hidden":false},{"_id":"6969a43732f0333869ff93b6","user":{"_id":"62f662bcc58915315c4eccea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg","isPro":true,"fullname":"Yilun Zhao","user":"yilunzhao","type":"user"},"name":"Yilun Zhao","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:40:43.185Z","hidden":false},{"_id":"6969a43732f0333869ff93b7","user":{"_id":"66cb9e6e8347e21e8a62bdae","avatarUrl":"/avatars/8bb855a7e2728bcd441dbf3ee03e41db.svg","isPro":false,"fullname":"Justice Ou","user":"JusticeOu","type":"user"},"name":"Justice Ou","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:40:49.836Z","hidden":false},{"_id":"6969a43732f0333869ff93b8","name":"Tinglin Huang","hidden":false},{"_id":"6969a43732f0333869ff93b9","user":{"_id":"5f5ba21188f57f65f951f255","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png","isPro":false,"fullname":"Arman Cohan","user":"armanc","type":"user"},"name":"Arman Cohan","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:41:04.821Z","hidden":false}],"publishedAt":"2026-01-14T21:12:06.000Z","submittedOnDailyAt":"2026-01-16T12:34:46.196Z","title":"Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL","submittedOnDailyBy":{"_id":"673ba2b141d69ace67cf289f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hiC2TM2y2oz5X-slJdMy9.png","isPro":false,"fullname":"YIFEI SHEN","user":"yifeis02","type":"user"},"summary":"Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.","upvotes":4,"discussionId":"6969a43732f0333869ff93ba","githubRepo":"https://github.com/Barryshen1/ClinSQL","githubRepoAddedBy":"user","ai_summary":"CLINSQL benchmark evaluates text-to-SQL models on complex clinical tasks requiring multi-table joins, temporal reasoning, and patient similarity analysis from real-world EHR data.","ai_keywords":["text-to-SQL","EHR tables","clinical reasoning","multi-table joins","temporal windows","patient-similarity cohorts","schema metadata","clinical coding systems","long contexts","multi-step queries","Chain-of-Thought self-refinement","rubric-based SQL analysis","execution checks","clinical reliability"],"githubStars":1,"organization":{"_id":"6315a1bb86b3db2ac420100e","name":"UW","fullname":"University of Washington","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/gr5B_WVvbMr4kTox5UkwZ.jpeg"}},"publishedAt":"2026-01-14T16:12:06.000Z","title":"Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL","summary":"Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09876.png","numComments":1,"submittedBy":{"_id":"673ba2b141d69ace67cf289f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hiC2TM2y2oz5X-slJdMy9.png","fullname":"YIFEI SHEN","name":"yifeis02","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6315a1bb86b3db2ac420100e","name":"UW","fullname":"University of Washington","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/gr5B_WVvbMr4kTox5UkwZ.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.10338","authors":[{"_id":"696a2fa2b23c50a8fcb39c2d","user":{"_id":"63c6b13950cc81901da4da6c","avatarUrl":"/avatars/23f2016e7c7ad902c5750e2ddc0b6358.svg","isPro":false,"fullname":"YI LIU","user":"sumleo","type":"user"},"name":"Yi Liu","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:41:52.558Z","hidden":false},{"_id":"696a2fa2b23c50a8fcb39c2e","name":"Weizhe Wang","hidden":false},{"_id":"696a2fa2b23c50a8fcb39c2f","name":"Ruitao Feng","hidden":false},{"_id":"696a2fa2b23c50a8fcb39c30","user":{"_id":"62069538ecd4915a6b4da26b","avatarUrl":"/avatars/f0ea79446b8f3e128789836f9d3ac0c7.svg","isPro":false,"fullname":"Zhang Yao","user":"yaozhang","type":"user"},"name":"Yao Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:42:12.937Z","hidden":false},{"_id":"696a2fa2b23c50a8fcb39c31","name":"Guangquan Xu","hidden":false},{"_id":"696a2fa2b23c50a8fcb39c32","user":{"_id":"64ae49347a9df1bd416f7e21","avatarUrl":"/avatars/f8226d99e8f272906e8934c3487ca6e2.svg","isPro":false,"fullname":"Deng","user":"Gelei","type":"user"},"name":"Gelei Deng","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:42:29.738Z","hidden":false},{"_id":"696a2fa2b23c50a8fcb39c33","name":"Yuekang Li","hidden":false},{"_id":"696a2fa2b23c50a8fcb39c34","name":"Leo Zhang","hidden":false}],"publishedAt":"2026-01-15T12:31:52.000Z","submittedOnDailyAt":"2026-01-16T10:03:43.716Z","title":"Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale","submittedOnDailyBy":{"_id":"63c6b13950cc81901da4da6c","avatarUrl":"/avatars/23f2016e7c7ad902c5750e2ddc0b6358.svg","isPro":false,"fullname":"YI LIU","user":"sumleo","type":"user"},"summary":"The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.","upvotes":3,"discussionId":"696a2fa2b23c50a8fcb39c35","ai_summary":"Large-scale security analysis of AI agent skills reveals widespread vulnerabilities including prompt injection, data exfiltration, and privilege escalation risks.","ai_keywords":["AI agent frameworks","agent skills","security analysis","vulnerability detection","prompt injection","data exfiltration","privilege escalation","supply chain risks","LLM-based semantic classification","static analysis","SkillScan","vulnerability taxonomy","permission systems"]},"publishedAt":"2026-01-15T07:31:52.000Z","title":"Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale","summary":"The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10338.png","numComments":1,"submittedBy":{"_id":"63c6b13950cc81901da4da6c","avatarUrl":"/avatars/23f2016e7c7ad902c5750e2ddc0b6358.svg","fullname":"YI LIU","name":"sumleo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.10080","authors":[{"_id":"6969bb6332f0333869ff94b1","user":{"_id":"64323dd503d81fa4d26deaf9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png","isPro":false,"fullname":"Letian Peng","user":"KomeijiForce","type":"user"},"name":"Letian Peng","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:46:27.655Z","hidden":false},{"_id":"6969bb6332f0333869ff94b2","name":"Kun Zhou","hidden":false},{"_id":"6969bb6332f0333869ff94b3","user":{"_id":"64c8f7b155abbb02c8c3e72c","avatarUrl":"/avatars/229477bdf487846e15e52c1451aaed15.svg","isPro":true,"fullname":"Longfei Yun","user":"ylf1017","type":"user"},"name":"Longfei Yun","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:46:19.813Z","hidden":false},{"_id":"6969bb6332f0333869ff94b4","user":{"_id":"64a62c2f500beb50968e5c9c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wfL3ojJmXqyzGUmCblPf4.jpeg","isPro":false,"fullname":"Yupeng Hou","user":"hyp1231","type":"user"},"name":"Yupeng Hou","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:28:57.131Z","hidden":false},{"_id":"6969bb6332f0333869ff94b5","user":{"_id":"660655119e3555d648f6c6b5","avatarUrl":"/avatars/ae1e2c97a08be39b77a9f1a5c2a718ef.svg","isPro":false,"fullname":"Jingbo Shang","user":"shangjingbo","type":"user"},"name":"Jingbo Shang","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:46:33.540Z","hidden":false}],"publishedAt":"2026-01-15T05:12:43.000Z","submittedOnDailyAt":"2026-01-16T01:46:22.167Z","title":"Deriving Character Logic from Storyline as Codified Decision Trees","submittedOnDailyBy":{"_id":"64323dd503d81fa4d26deaf9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png","isPro":false,"fullname":"Letian Peng","user":"KomeijiForce","type":"user"},"summary":"Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on 85 characters across 16 artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.","upvotes":3,"discussionId":"6969bb6332f0333869ff94b6","ai_summary":"Executable and interpretable decision trees are induced from narrative data to create robust behavioral profiles for role-playing agents, outperforming traditional methods in consistency and reliability.","ai_keywords":["decision trees","behavioral profiles","role-playing agents","narrative data","scene conditions","behavioral statements","deterministic retrieval","hierarchical specialization","agent grounding"]},"publishedAt":"2026-01-15T00:12:43.000Z","title":"Deriving Character Logic from Storyline as Codified Decision Trees","summary":"Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on 85 characters across 16 artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10080.png","numComments":1,"submittedBy":{"_id":"64323dd503d81fa4d26deaf9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png","fullname":"Letian Peng","name":"KomeijiForce","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.09499","authors":[{"_id":"6968a3720ac10a06522f6b6e","user":{"_id":"6943e8596f46bd27c14dda56","avatarUrl":"/avatars/13b28d18550bbeea24a8d0a39282ecb9.svg","isPro":true,"fullname":"Edgar Sucar","user":"edgarsucar","type":"user"},"name":"Edgar Sucar","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:40:19.827Z","hidden":false},{"_id":"6968a3720ac10a06522f6b6f","user":{"_id":"6387261720244d72a73f39f4","avatarUrl":"/avatars/46c097e65f2b1efd1630de004f61582d.svg","isPro":false,"fullname":"Eldar Insafutdinov","user":"einsafutdinov","type":"user"},"name":"Eldar Insafutdinov","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:08.932Z","hidden":false},{"_id":"6968a3720ac10a06522f6b70","user":{"_id":"64b6fc043240387159385543","avatarUrl":"/avatars/3a6c9072eb812c43e304030fdb942bc9.svg","isPro":false,"fullname":"Zihang Lai","user":"zlai","type":"user"},"name":"Zihang Lai","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:40:25.207Z","hidden":false},{"_id":"6968a3720ac10a06522f6b71","user":{"_id":"68055d1082c2bae6af503848","avatarUrl":"/avatars/e32c2f0a8efbf54707ead520ae9374bf.svg","isPro":false,"fullname":"Andrea Vedaldi","user":"vedaldi","type":"user"},"name":"Andrea Vedaldi","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:40:30.543Z","hidden":false}],"publishedAt":"2026-01-14T14:03:42.000Z","submittedOnDailyAt":"2026-01-16T12:24:01.107Z","title":"V-DPM: 4D Video Reconstruction with Dynamic Point Maps","submittedOnDailyBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","isPro":true,"fullname":"AK","user":"akhaliq","type":"user"},"summary":"Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.","upvotes":3,"discussionId":"6968a3720ac10a06522f6b72","projectPage":"https://www.robots.ox.ac.uk/~vgg/research/vdpm/","githubRepo":"https://github.com/eldar/vdpm","githubRepoAddedBy":"user","ai_summary":"Dynamic Point Maps extended to video input through V-DPM framework achieve state-of-the-art 3D and 4D reconstruction by recovering both dynamic depth and full 3D motion of scene points.","ai_keywords":["Dynamic Point Maps","3D reconstruction","video input","VGGT","synthetic data","4D reconstruction","3D motion recovery"],"githubStars":43},"publishedAt":"2026-01-14T09:03:42.000Z","title":"V-DPM: 4D Video Reconstruction with Dynamic Point Maps","summary":"Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09499.png","numComments":1,"submittedBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","fullname":"AK","name":"akhaliq","type":"user","isPro":true,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":9112,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.09923","authors":[{"_id":"6969f752844a787c4fdea435","user":{"_id":"666c1544de375f3114d590c0","avatarUrl":"/avatars/de6c4d3395160603b0bf68c9bca398df.svg","isPro":false,"fullname":"Hanna Foerster","user":"aprilflower","type":"user"},"name":"Hanna Foerster","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:45:26.738Z","hidden":false},{"_id":"6969f752844a787c4fdea436","user":{"_id":"66349b8dc53115681c534d07","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Suu9bZJecfCcJccjRsrqH.png","isPro":false,"fullname":"Robert Mullins","user":"rcmullins","type":"user"},"name":"Robert Mullins","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:45:33.009Z","hidden":false},{"_id":"6969f752844a787c4fdea437","user":{"_id":"664a1d42640bc89c935f191e","avatarUrl":"/avatars/a7a40b51564c43ed595840945e0a58f8.svg","isPro":false,"fullname":"Tom Blanchard","user":"tom-bl","type":"user"},"name":"Tom Blanchard","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:45:40.522Z","hidden":false},{"_id":"6969f752844a787c4fdea438","name":"Nicolas Papernot","hidden":false},{"_id":"6969f752844a787c4fdea439","user":{"_id":"6695455330bd2a19adca14c0","avatarUrl":"/avatars/a183124c6f13f1179f202c7462ef5bf7.svg","isPro":false,"fullname":"Kristina Nikolic","user":"nkristina","type":"user"},"name":"Kristina Nikolić","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:45:50.298Z","hidden":false},{"_id":"6969f752844a787c4fdea43a","user":{"_id":"63568f18ba90b4ea9fe91cb5","avatarUrl":"/avatars/3e8b3c573e20cf80d329a312bfc34728.svg","isPro":false,"fullname":"Florian Tramer","user":"ftramer","type":"user"},"name":"Florian Tramèr","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:45:56.278Z","hidden":false},{"_id":"6969f752844a787c4fdea43b","name":"Ilia Shumailov","hidden":false},{"_id":"6969f752844a787c4fdea43c","name":"Cheng Zhang","hidden":false},{"_id":"6969f752844a787c4fdea43d","user":{"_id":"6654464df0c8c891ca528618","avatarUrl":"/avatars/26aa1f673da92e3632576954bcee1b28.svg","isPro":false,"fullname":"Yiren Zhao","user":"YirenZH","type":"user"},"name":"Yiren Zhao","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:46:05.784Z","hidden":false}],"publishedAt":"2026-01-14T23:06:35.000Z","submittedOnDailyAt":"2026-01-16T06:06:19.702Z","title":"CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents","submittedOnDailyBy":{"_id":"6475c2794766357252e69e9f","avatarUrl":"/avatars/757ed789423113369868e972f21ce559.svg","isPro":false,"fullname":"i","user":"iliashum","type":"user"},"summary":"AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.","upvotes":2,"discussionId":"6969f752844a787c4fdea43e","ai_summary":"Computer Use Agents face security challenges from prompt injection attacks, but a single-shot planning approach with architectural isolation enables secure autonomous task execution while maintaining performance.","ai_keywords":["prompt injection attacks","architectural isolation","Computer Use Agents","UI workflows","execution graph","conditional branches","control flow integrity","instruction injections","Branch Steering attacks","OSWorld"]},"publishedAt":"2026-01-14T18:06:35.000Z","title":"CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents","summary":"AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09923.png","numComments":1,"submittedBy":{"_id":"6475c2794766357252e69e9f","avatarUrl":"/avatars/757ed789423113369868e972f21ce559.svg","fullname":"i","name":"iliashum","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.06378","authors":[{"_id":"6969e919844a787c4fdea3dd","name":"Hao Zhang","hidden":false},{"_id":"6969e919844a787c4fdea3de","name":"Jiahao Luo","hidden":false},{"_id":"6969e919844a787c4fdea3df","name":"Bohui Wan","hidden":false},{"_id":"6969e919844a787c4fdea3e0","name":"Yizhou Zhao","hidden":false},{"_id":"6969e919844a787c4fdea3e1","name":"Zongrui Li","hidden":false},{"_id":"6969e919844a787c4fdea3e2","name":"Michael Vasilkovsky","hidden":false},{"_id":"6969e919844a787c4fdea3e3","name":"Chaoyang Wang","hidden":false},{"_id":"6969e919844a787c4fdea3e4","name":"Jian Wang","hidden":false},{"_id":"6969e919844a787c4fdea3e5","name":"Narendra Ahuja","hidden":false},{"_id":"6969e919844a787c4fdea3e6","name":"Bing Zhou","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65712f530ea91e592a1e8126/DxspfQrp3ncqz0T_8Z5rz.mp4"],"publishedAt":"2026-01-10T01:26:28.000Z","submittedOnDailyAt":"2026-01-16T15:29:59.900Z","title":"RigMo: Unifying Rig and Motion Learning for Generative Animation","submittedOnDailyBy":{"_id":"65712f530ea91e592a1e8126","avatarUrl":"/avatars/2ed00ca40b7ff2b88e4a75ef04807447.svg","isPro":true,"fullname":"Hao Zhang","user":"haoz19","type":"user"},"summary":"Despite significant progress in 4D generation, rig and motion, the core structural and dynamic components of animation are typically modeled as separate problems. Existing pipelines rely on ground-truth skeletons and skinning weights for motion generation and treat auto-rigging as an independent process, undermining scalability and interpretability. We present RigMo, a unified generative framework that jointly learns rig and motion directly from raw mesh sequences, without any human-provided rig annotations. RigMo encodes per-vertex deformations into two compact latent spaces: a rig latent that decodes into explicit Gaussian bones and skinning weights, and a motion latent that produces time-varying SE(3) transformations. Together, these outputs define an animatable mesh with explicit structure and coherent motion, enabling feed-forward rig and motion inference for deformable objects. Beyond unified rig-motion discovery, we introduce a Motion-DiT model operating in RigMo's latent space and demonstrate that these structure-aware latents can naturally support downstream motion generation tasks. Experiments on DeformingThings4D, Objaverse-XL, and TrueBones demonstrate that RigMo learns smooth, interpretable, and physically plausible rigs, while achieving superior reconstruction and category-level generalization compared to existing auto-rigging and deformation baselines. RigMo establishes a new paradigm for unified, structure-aware, and scalable dynamic 3D modeling.","upvotes":2,"discussionId":"6969e919844a787c4fdea3e7","projectPage":"https://rigmo-page.github.io/","ai_summary":"RigMo is a unified generative framework that simultaneously learns rig and motion from mesh sequences, encoding deformations into compact latent spaces for interpretable and physically plausible 3D animation.","ai_keywords":["generative framework","rig latent","motion latent","SE(3) transformations","auto-rigging","deformation baselines","latent space","mesh sequences","animatable mesh","structure-aware latents"],"organization":{"_id":"63c87c41cd6a490608ce31d1","name":"snap-research","fullname":"Snap Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"}},"publishedAt":"2026-01-09T20:26:28.000Z","title":"RigMo: Unifying Rig and Motion Learning for Generative Animation","summary":"Despite significant progress in 4D generation, rig and motion, the core structural and dynamic components of animation are typically modeled as separate problems. Existing pipelines rely on ground-truth skeletons and skinning weights for motion generation and treat auto-rigging as an independent process, undermining scalability and interpretability. We present RigMo, a unified generative framework that jointly learns rig and motion directly from raw mesh sequences, without any human-provided rig annotations. RigMo encodes per-vertex deformations into two compact latent spaces: a rig latent that decodes into explicit Gaussian bones and skinning weights, and a motion latent that produces time-varying SE(3) transformations. Together, these outputs define an animatable mesh with explicit structure and coherent motion, enabling feed-forward rig and motion inference for deformable objects. Beyond unified rig-motion discovery, we introduce a Motion-DiT model operating in RigMo's latent space and demonstrate that these structure-aware latents can naturally support downstream motion generation tasks. Experiments on DeformingThings4D, Objaverse-XL, and TrueBones demonstrate that RigMo learns smooth, interpretable, and physically plausible rigs, while achieving superior reconstruction and category-level generalization compared to existing auto-rigging and deformation baselines. RigMo establishes a new paradigm for unified, structure-aware, and scalable dynamic 3D modeling.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65712f530ea91e592a1e8126/DxspfQrp3ncqz0T_8Z5rz.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06378.png","numComments":1,"submittedBy":{"_id":"65712f530ea91e592a1e8126","avatarUrl":"/avatars/2ed00ca40b7ff2b88e4a75ef04807447.svg","fullname":"Hao Zhang","name":"haoz19","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"63c87c41cd6a490608ce31d1","name":"snap-research","fullname":"Snap Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.10716","authors":[{"_id":"6969dd8432f0333869ff9529","user":{"_id":"634632aaac1cb29fb2ac9f14","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg","isPro":false,"fullname":"Xuweiyi Chen","user":"Xuweiyi","type":"user"},"name":"Xuweiyi Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:45:11.912Z","hidden":false},{"_id":"6969dd8432f0333869ff952a","user":{"_id":"661c1e129fbab39d0e7c2724","avatarUrl":"/avatars/52c061cd4c56c43621dc247336b8157f.svg","isPro":false,"fullname":"Wentao Zhou","user":"Chasers1","type":"user"},"name":"Wentao Zhou","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:45:05.448Z","hidden":false},{"_id":"6969dd8432f0333869ff952b","user":{"_id":"64834da6c8b6f4a798f9f09d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64834da6c8b6f4a798f9f09d/EOxUisexBKD-L5yCD7Ecs.png","isPro":false,"fullname":"Cheng","user":"Zezhou","type":"user"},"name":"Zezhou Cheng","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:44:58.848Z","hidden":false}],"publishedAt":"2026-01-15T18:59:58.000Z","submittedOnDailyAt":"2026-01-16T04:12:08.639Z","title":"WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments","submittedOnDailyBy":{"_id":"634632aaac1cb29fb2ac9f14","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg","isPro":false,"fullname":"Xuweiyi Chen","user":"Xuweiyi","type":"user"},"summary":"We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.","upvotes":1,"discussionId":"6969dd8432f0333869ff952c","ai_summary":"WildRayZer is a self-supervised framework for novel view synthesis in dynamic environments that uses analysis-by-synthesis to handle moving cameras and objects through motion masking and gradient gating.","ai_keywords":["novel view synthesis","dynamic environments","multi-view consistency","ghosting","hallucinated geometry","pose estimation","analysis-by-synthesis","static renderer","residuals","pseudo motion masks","motion estimator","input tokens","loss gradients","cross-view background completion","real-world dataset","dynamic sequences","transient regions","feed-forward pass","optimization-based baselines","feed-forward baselines"],"organization":{"_id":"66e5cb86ad382452bb7fb680","name":"uva-cv-lab","fullname":"UVA Computer Vision Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/634632aaac1cb29fb2ac9f14/xtQJm_4U1Ta7BDxlP101K.jpeg"}},"publishedAt":"2026-01-15T13:59:58.000Z","title":"WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments","summary":"We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10716.png","numComments":1,"submittedBy":{"_id":"634632aaac1cb29fb2ac9f14","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg","fullname":"Xuweiyi Chen","name":"Xuweiyi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"66e5cb86ad382452bb7fb680","name":"uva-cv-lab","fullname":"UVA Computer Vision Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/634632aaac1cb29fb2ac9f14/xtQJm_4U1Ta7BDxlP101K.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.10124","authors":[{"_id":"696a120e844a787c4fdea48b","user":{"_id":"64b047abb02b95456db10915","avatarUrl":"/avatars/1973377424ff5d586027b91c32f8675c.svg","isPro":false,"fullname":"David Yang","user":"yscript","type":"user"},"name":"Sicheng Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:28:07.506Z","hidden":false},{"_id":"696a120e844a787c4fdea48c","user":{"_id":"632c2100ea6e62428ab201e9","avatarUrl":"/avatars/5bd355e095af93261928198e3d6d5696.svg","isPro":true,"fullname":"Xingzhaohu","user":"xingzhaohu","type":"user"},"name":"Zhaohu Xing","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:41:24.661Z","hidden":false},{"_id":"696a120e844a787c4fdea48d","name":"Lei Zhu","hidden":false}],"publishedAt":"2026-01-15T07:09:00.000Z","submittedOnDailyAt":"2026-01-16T08:01:42.954Z","title":"VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation","submittedOnDailyBy":{"_id":"64b047abb02b95456db10915","avatarUrl":"/avatars/1973377424ff5d586027b91c32f8675c.svg","isPro":false,"fullname":"David Yang","user":"yscript","type":"user"},"summary":"Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.","upvotes":1,"discussionId":"696a120e844a787c4fdea48e","projectPage":"https://github.com/script-Yang/VQ-Seg","githubRepo":"https://github.com/script-Yang/VQ-Seg","githubRepoAddedBy":"user","ai_summary":"VQ-Seg introduces a vector quantization-based perturbation method for medical image segmentation that replaces dropout with a controllable quantized perturbation module while maintaining performance through a dual-branch architecture and foundation model guidance.","ai_keywords":["vector quantization","feature perturbation","consistency learning","semi-supervised medical image segmentation","dropout","Quantized Perturbation Module","dual-branch architecture","foundation model","Post-VQ Feature Adapter"],"githubStars":4},"publishedAt":"2026-01-15T02:09:00.000Z","title":"VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation","summary":"Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10124.png","numComments":1,"submittedBy":{"_id":"64b047abb02b95456db10915","avatarUrl":"/avatars/1973377424ff5d586027b91c32f8675c.svg","fullname":"David Yang","name":"yscript","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.08302","authors":[{"_id":"69674b09c5e371f6b235d213","name":"Marvin Schmitt","hidden":false},{"_id":"69674b09c5e371f6b235d214","name":"Anne Schwerk","hidden":false},{"_id":"69674b09c5e371f6b235d215","user":{"_id":"675c4fefdea6ceb2f6cd4563","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fcORaS32aIDFuIxx_L1PY.jpeg","isPro":false,"fullname":"Sebastian Lempert","user":"slempert","type":"user"},"name":"Sebastian Lempert","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:04:03.780Z","hidden":false}],"publishedAt":"2026-01-13T07:45:36.000Z","submittedOnDailyAt":"2026-01-16T09:23:40.066Z","title":"Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques","submittedOnDailyBy":{"_id":"675c4fefdea6ceb2f6cd4563","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fcORaS32aIDFuIxx_L1PY.jpeg","isPro":false,"fullname":"Sebastian Lempert","user":"slempert","type":"user"},"summary":"This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.","upvotes":1,"discussionId":"69674b09c5e371f6b235d216","githubRepo":"https://github.com/Marvin2108/ESCID-LLM-APET","githubRepoAddedBy":"user","ai_summary":"Advanced prompting techniques significantly enhance large language model performance in sentiment analysis, with optimal strategies varying by model architecture and task complexity.","ai_keywords":["prompt engineering","large language models","few-shot learning","chain-of-thought prompting","self-consistency","sentiment analysis","aspect-based sentiment analysis","irony detection"],"githubStars":0,"organization":{"_id":"65786449528e89e35f4a389f","name":"IUInternationalUniversity","fullname":"IU International University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65703e162cf29b89e9270827/Y1w3qIIqaLowO6Kev8_P4.png"}},"publishedAt":"2026-01-13T02:45:36.000Z","title":"Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques","summary":"This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08302.png","numComments":1,"submittedBy":{"_id":"675c4fefdea6ceb2f6cd4563","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fcORaS32aIDFuIxx_L1PY.jpeg","fullname":"Sebastian Lempert","name":"slempert","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"65786449528e89e35f4a389f","name":"IUInternationalUniversity","fullname":"IU International University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65703e162cf29b89e9270827/Y1w3qIIqaLowO6Kev8_P4.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.08297","authors":[{"_id":"696a84c83f1837bfb8970598","name":"Yuan Cheng","hidden":false},{"_id":"696a84c83f1837bfb8970599","name":"Fengzhuo Zhang","hidden":false},{"_id":"696a84c83f1837bfb897059a","name":"Yunlong Hou","hidden":false},{"_id":"696a84c83f1837bfb897059b","name":"Cunxiao Du","hidden":false},{"_id":"696a84c83f1837bfb897059c","name":"Chao Du","hidden":false},{"_id":"696a84c83f1837bfb897059d","name":"Tianyu Pang","hidden":false},{"_id":"696a84c83f1837bfb897059e","name":"Aixin Sun","hidden":false},{"_id":"696a84c83f1837bfb897059f","name":"Zhuoran Yang","hidden":false}],"publishedAt":"2026-01-13T07:40:57.000Z","submittedOnDailyAt":"2026-01-16T16:05:54.962Z","title":"Demystifying the Slash Pattern in Attention: The Role of RoPE","submittedOnDailyBy":{"_id":"64b8c1a995bd42c7707f7918","avatarUrl":"/avatars/08c2929f8f150ecd6f8e5a06c4cb9034.svg","isPro":false,"fullname":"Fengzhuo Zhang","user":"Fengzhuo","type":"user"},"summary":"Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Δ-th sub-diagonal for some offset Δ. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.","upvotes":1,"discussionId":"696a84c93f1837bfb89705a0","ai_summary":"Slash-Dominant Heads in large language models emerge from specific conditions involving rank-one queries/keys and dominant medium-high frequency components of Rotary Position Embedding, with theoretical proof showing their emergence through gradient descent training dynamics.","ai_keywords":["Slash-Dominant Heads","attention patterns","Rotary Position Embedding","queries","keys","gradient descent","training dynamics","Transformer","rank-one","medium-frequency components","high-frequency components"]},"publishedAt":"2026-01-13T02:40:57.000Z","title":"Demystifying the Slash Pattern in Attention: The Role of RoPE","summary":"Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Δ-th sub-diagonal for some offset Δ. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08297.png","numComments":1,"submittedBy":{"_id":"64b8c1a995bd42c7707f7918","avatarUrl":"/avatars/08c2929f8f150ecd6f8e5a06c4cb9034.svg","fullname":"Fengzhuo Zhang","name":"Fengzhuo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.00756","authors":[{"_id":"696a1f06b23c50a8fcb39c1d","user":{"_id":"6676179e4b1e661916d0c654","avatarUrl":"/avatars/a074b2c7baa49de9324329c752b49dfd.svg","isPro":false,"fullname":"Thomas Katraouras","user":"Tomk187","type":"user"},"name":"Thomas Katraouras","status":"admin_assigned","statusLastChangedAt":"2026-01-16T15:39:51.244Z","hidden":false},{"_id":"696a1f06b23c50a8fcb39c1e","name":"Dimitrios Rafailidis","hidden":false}],"publishedAt":"2026-01-02T17:22:34.000Z","submittedOnDailyAt":"2026-01-16T08:51:41.416Z","title":"Memory Bank Compression for Continual Adaptation of Large Language Models","submittedOnDailyBy":{"_id":"6676179e4b1e661916d0c654","avatarUrl":"/avatars/a074b2c7baa49de9324329c752b49dfd.svg","isPro":false,"fullname":"Thomas Katraouras","user":"Tomk187","type":"user"},"summary":"Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.","upvotes":0,"discussionId":"696a1f06b23c50a8fcb39c1f","githubRepo":"https://github.com/Thomkat/MBC","githubRepoAddedBy":"user","ai_summary":"A memory-augmented continual learning approach for large language models that compresses memory banks through codebook optimization while maintaining retention accuracy.","ai_keywords":["continual learning","large language models","catastrophic forgetting","memory bank","codebook optimization","online resetting mechanism","Key-Value Low-Rank Adaptation","attention layers"],"githubStars":20},"publishedAt":"2026-01-02T12:22:34.000Z","title":"Memory Bank Compression for Continual Adaptation of Large Language Models","summary":"Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00756.png","numComments":1,"submittedBy":{"_id":"6676179e4b1e661916d0c654","avatarUrl":"/avatars/a074b2c7baa49de9324329c752b49dfd.svg","fullname":"Thomas Katraouras","name":"Tomk187","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true}]
[{"paper":{"id":"2601.12993","authors":[{"_id":"69705709a8be625b19c2af1f","user":{"_id":"6708cbdcf8a1d7b26732c038","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CN1WHMPKfjQ8wmfwOe0ni.png","isPro":false,"fullname":"Hao Luo","user":"Lightet","type":"user"},"name":"Hao Luo","status":"claimed_verified","statusLastChangedAt":"2026-01-21T10:15:59.988Z","hidden":false},{"_id":"69705709a8be625b19c2af20","name":"Ye Wang","hidden":false},{"_id":"69705709a8be625b19c2af21","user":{"_id":"640dd700fdeaae139081f598","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg","isPro":false,"fullname":"Wanpeng Zhang","user":"zawnpn","type":"user"},"name":"Wanpeng Zhang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:56.371Z","hidden":false},{"_id":"69705709a8be625b19c2af22","user":{"_id":"64eac1f496f42afd627d439c","avatarUrl":"/avatars/aa46265122b8a1170f57475494d7922e.svg","isPro":false,"fullname":"Sipeng Zheng","user":"sipeng9527","type":"user"},"name":"Sipeng Zheng","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:49:38.507Z","hidden":false},{"_id":"69705709a8be625b19c2af23","user":{"_id":"66c84a9eab23d3d7dfb2a368","avatarUrl":"/avatars/b0a50133c6a95ed340dfb462e87820f4.svg","isPro":false,"fullname":"ziheng xi","user":"zhenqis123","type":"user"},"name":"Ziheng Xi","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:49:45.981Z","hidden":false},{"_id":"69705709a8be625b19c2af24","user":{"_id":"64bdd5cc76a6e2efccb22100","avatarUrl":"/avatars/5a0edc24283616dafc76ce5ec97ab5a0.svg","isPro":false,"fullname":"xuchaoyi","user":"co1one","type":"user"},"name":"Chaoyi Xu","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:49:56.014Z","hidden":false},{"_id":"69705709a8be625b19c2af25","user":{"_id":"68872ff6c18b7e1e13115564","avatarUrl":"/avatars/f908fc3cc89cd81493105359093f299d.svg","isPro":false,"fullname":"Haiweng Xu","user":"Seaman05","type":"user"},"name":"Haiweng Xu","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:50:01.261Z","hidden":false},{"_id":"69705709a8be625b19c2af26","user":{"_id":"644560657a7b94ddc2d445a3","avatarUrl":"/avatars/09d6447da6ff1bd0b2b00c899c9f1b28.svg","isPro":false,"fullname":"Haoqi Yuan","user":"Yaya041","type":"user"},"name":"Haoqi Yuan","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:50:06.048Z","hidden":false},{"_id":"69705709a8be625b19c2af27","name":"Chi Zhang","hidden":false},{"_id":"69705709a8be625b19c2af28","name":"Yiqing Wang","hidden":false},{"_id":"69705709a8be625b19c2af29","name":"Yicheng Feng","hidden":false},{"_id":"69705709a8be625b19c2af2a","user":{"_id":"67d905c0e27ba28109384f5c","avatarUrl":"/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg","isPro":false,"fullname":"Zongqing Lu","user":"chungtsing","type":"user"},"name":"Zongqing Lu","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:50:24.833Z","hidden":false}],"publishedAt":"2026-01-19T12:20:38.000Z","submittedOnDailyAt":"2026-01-21T02:12:40.880Z","title":"Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization","submittedOnDailyBy":{"_id":"640dd700fdeaae139081f598","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg","isPro":false,"fullname":"Wanpeng Zhang","user":"zawnpn","type":"user"},"summary":"We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.","upvotes":59,"discussionId":"69705709a8be625b19c2af2b","projectPage":"https://research.beingbeyond.com/being-h05","githubRepo":"https://github.com/BeingBeyond/Being-H","githubRepoAddedBy":"user","ai_summary":"Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.","ai_keywords":["Vision-Language-Action","cross-embodiment generalization","human-centric learning","multimodal data","Unified Action Space","Mixture-of-Transformers","Mixture-of-Flow","manifold-preserving gating","universal async chunking"],"githubStars":265,"organization":{"_id":"687a8ba5aedd77694bc94386","name":"BeingBeyond","fullname":"BeingBeyond","avatar":"https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}},"publishedAt":"2026-01-19T07:20:38.000Z","title":"Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization","summary":"We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12993.png","numComments":1,"submittedBy":{"_id":"640dd700fdeaae139081f598","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg","fullname":"Wanpeng Zhang","name":"zawnpn","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"organization":{"_id":"687a8ba5aedd77694bc94386","name":"BeingBeyond","fullname":"BeingBeyond","avatar":"https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.11655","authors":[{"_id":"6970436ba8be625b19c2ae97","user":{"_id":"64d668bf54bb9eb7040c477e","avatarUrl":"/avatars/b171b9c1cbb22e2f86e4280099c0bf93.svg","isPro":false,"fullname":"Caihua Li","user":"LoisNotLo","type":"user"},"name":"Caihua Li","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:20:05.617Z","hidden":false},{"_id":"6970436ba8be625b19c2ae98","user":{"_id":"64c52b6de356b52a9868bce3","avatarUrl":"/avatars/43b05cc691f273447e8bc65fe7515176.svg","isPro":false,"fullname":"Guo","user":"glh123456","type":"user"},"name":"Lianghong Guo","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:20:12.738Z","hidden":false},{"_id":"6970436ba8be625b19c2ae99","user":{"_id":"680ef06cce6b5c5af1f29aec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aTLjskuvCwTYs5JwjUtEF.png","isPro":false,"fullname":"DeepSoftwareAnalytics","user":"Yanlin-Wang","type":"user"},"name":"Yanlin Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:20:10.460Z","hidden":false},{"_id":"6970436ba8be625b19c2ae9a","user":{"_id":"653df20eaa1f487614da4db1","avatarUrl":"/avatars/12b27ce2c59f53b7e464039deab36a5d.svg","isPro":false,"fullname":"Daya Guo","user":"guoday","type":"user"},"name":"Daya Guo","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:20:15.421Z","hidden":false},{"_id":"6970436ba8be625b19c2ae9b","user":{"_id":"6355473d525beaee688b7ba1","avatarUrl":"/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg","isPro":false,"fullname":"Wei Tao","user":"itaowe","type":"user"},"name":"Wei Tao","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:20:17.409Z","hidden":false},{"_id":"6970436ba8be625b19c2ae9c","name":"Zhenyu Shan","hidden":false},{"_id":"6970436ba8be625b19c2ae9d","name":"Mingwei Liu","hidden":false},{"_id":"6970436ba8be625b19c2ae9e","name":"Jiachi Chen","hidden":false},{"_id":"6970436ba8be625b19c2ae9f","name":"Haoyu Song","hidden":false},{"_id":"6970436ba8be625b19c2aea0","name":"Duyu Tang","hidden":false},{"_id":"6970436ba8be625b19c2aea1","name":"Hongyu Zhang","hidden":false},{"_id":"6970436ba8be625b19c2aea2","name":"Zibin Zheng","hidden":false}],"publishedAt":"2026-01-15T18:55:03.000Z","submittedOnDailyAt":"2026-01-21T00:52:01.626Z","title":"Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey","submittedOnDailyBy":{"_id":"6355473d525beaee688b7ba1","avatarUrl":"/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg","isPro":false,"fullname":"Wei Tao","user":"itaowe","type":"user"},"summary":"Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.","upvotes":49,"discussionId":"6970436ba8be625b19c2aea3","projectPage":"https://deepsoftwareanalytics.github.io/Awesome-Issue-Resolution/","githubRepo":"https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution","githubRepoAddedBy":"user","ai_summary":"Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.","ai_keywords":["large language models","software engineering","autonomous coding agents","training-free frameworks","supervised fine-tuning","reinforcement learning","data quality","agent behavior"],"githubStars":40,"organization":{"_id":"680ef1aaccefecd5aee18d1d","name":"Deep-Software-Analytics","fullname":"DeepSoftwareAnalytics"}},"publishedAt":"2026-01-15T13:55:03.000Z","title":"Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey","summary":"Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11655.png","numComments":2,"submittedBy":{"_id":"6355473d525beaee688b7ba1","avatarUrl":"/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg","fullname":"Wei Tao","name":"itaowe","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"680ef1aaccefecd5aee18d1d","name":"Deep-Software-Analytics","fullname":"DeepSoftwareAnalytics"},"isAuthorParticipating":true},{"paper":{"id":"2601.14250","authors":[{"_id":"69705b78a8be625b19c2af4c","user":{"_id":"6697765937d24838267b41e7","avatarUrl":"/avatars/682bb4cf0a0009812b42748dc26916f9.svg","isPro":false,"fullname":"PangzeCheung","user":"PangzeCheung","type":"user"},"name":"Pengze Zhang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:32.663Z","hidden":false},{"_id":"69705b78a8be625b19c2af4d","name":"Yanze Wu","hidden":false},{"_id":"69705b78a8be625b19c2af4e","user":{"_id":"6805bdfb344d6d8a8fd5b07a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dGLeWvN2CXLmxVP_b9S4R.png","isPro":false,"fullname":"Mengtian Li","user":"LemonSky1995","type":"user"},"name":"Mengtian Li","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:52:30.670Z","hidden":false},{"_id":"69705b78a8be625b19c2af4f","name":"Xu Bai","hidden":false},{"_id":"69705b78a8be625b19c2af50","name":"Songtao Zhao","hidden":false},{"_id":"69705b78a8be625b19c2af51","user":{"_id":"6339029a76421c0543167075","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png","isPro":false,"fullname":"fulong ye","user":"Alon77777","type":"user"},"name":"Fulong Ye","status":"admin_assigned","statusLastChangedAt":"2026-01-21T11:46:59.343Z","hidden":false},{"_id":"69705b78a8be625b19c2af52","name":"Chong Mou","hidden":false},{"_id":"69705b78a8be625b19c2af53","user":{"_id":"6752cd83ffaeeb979db974ae","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png","isPro":false,"fullname":"Xinghui Li","user":"Crayon-Shinchan","type":"user"},"name":"Xinghui Li","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:52:58.973Z","hidden":false},{"_id":"69705b78a8be625b19c2af54","user":{"_id":"6304e2dabad6ce7fc0287d57","avatarUrl":"/avatars/3fd4a9a62b0ef98db2573411463a9247.svg","isPro":false,"fullname":"Zhuowei_Chen","user":"ZhuoweiChen","type":"user"},"name":"Zhuowei Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:52:20.177Z","hidden":false},{"_id":"69705b78a8be625b19c2af55","name":"Qian He","hidden":false},{"_id":"69705b78a8be625b19c2af56","user":{"_id":"671aa30b496f0bc5ae04da4b","avatarUrl":"/avatars/902d7f9fd56f84953d67d9229bd9d6b7.svg","isPro":false,"fullname":"Mingyuan Gao","user":"GMY1999","type":"user"},"name":"Mingyuan Gao","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:52:13.464Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6697765937d24838267b41e7/309vLCCbUoiHGJkN0u3LH.mp4"],"publishedAt":"2026-01-20T18:58:11.000Z","submittedOnDailyAt":"2026-01-21T02:29:32.365Z","title":"OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer","submittedOnDailyBy":{"_id":"6697765937d24838267b41e7","avatarUrl":"/avatars/682bb4cf0a0009812b42748dc26916f9.svg","isPro":false,"fullname":"PangzeCheung","user":"PangzeCheung","type":"user"},"summary":"Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.","upvotes":29,"discussionId":"69705b78a8be625b19c2af57","projectPage":"https://pangzecheung.github.io/OmniTransfer/","githubRepo":"https://github.com/PangzeCheung/OmniTransfer","githubRepoAddedBy":"user","ai_summary":"OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.","ai_keywords":["video customization","spatio-temporal video transfer","multi-view information","temporal cues","temporal alignment","appearance consistency","reference-decoupled causal learning","task-adaptive multimodal alignment","pose-guided methods"],"githubStars":54,"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}},"publishedAt":"2026-01-20T13:58:11.000Z","title":"OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer","summary":"Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6697765937d24838267b41e7/309vLCCbUoiHGJkN0u3LH.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14250.png","numComments":4,"submittedBy":{"_id":"6697765937d24838267b41e7","avatarUrl":"/avatars/682bb4cf0a0009812b42748dc26916f9.svg","fullname":"PangzeCheung","name":"PangzeCheung","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14192","authors":[{"_id":"69705a68a8be625b19c2af3a","user":{"_id":"6745c589d2d740914ec2574f","avatarUrl":"/avatars/7b2ff6848d42cd140a775df0c2bc9384.svg","isPro":false,"fullname":"Xiaofang Yang","user":"fffovo","type":"user"},"name":"Xiaofang Yang","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:48:39.583Z","hidden":false},{"_id":"69705a68a8be625b19c2af3b","name":"Lijun Li","hidden":false},{"_id":"69705a68a8be625b19c2af3c","user":{"_id":"660d17d6c9be0dcd31a30b3d","avatarUrl":"/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg","isPro":false,"fullname":"Zhou Heng","user":"henggg","type":"user"},"name":"Heng Zhou","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:49.600Z","hidden":false},{"_id":"69705a68a8be625b19c2af3d","name":"Tong Zhu","hidden":false},{"_id":"69705a68a8be625b19c2af3e","name":"Xiaoye Qu","hidden":false},{"_id":"69705a68a8be625b19c2af3f","name":"Yuchen Fan","hidden":false},{"_id":"69705a68a8be625b19c2af40","user":{"_id":"6952244bfbddb08cb2562f3b","avatarUrl":"/avatars/70d67319af29604129378fee3f216757.svg","isPro":false,"fullname":"qianshan wei","user":"b1intern","type":"user"},"name":"Qianshan Wei","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:49:04.253Z","hidden":false},{"_id":"69705a68a8be625b19c2af41","name":"Rui Ye","hidden":false},{"_id":"69705a68a8be625b19c2af42","name":"Li Kang","hidden":false},{"_id":"69705a68a8be625b19c2af43","name":"Yiran Qin","hidden":false},{"_id":"69705a68a8be625b19c2af44","name":"Zhiqiang Kou","hidden":false},{"_id":"69705a68a8be625b19c2af45","name":"Daizong Liu","hidden":false},{"_id":"69705a68a8be625b19c2af46","name":"Qi Li","hidden":false},{"_id":"69705a68a8be625b19c2af47","name":"Ning Ding","hidden":false},{"_id":"69705a68a8be625b19c2af48","user":{"_id":"65257545b017be1fc1915364","avatarUrl":"/avatars/9bffd3fb567d2fa1e5c3546d77560b43.svg","isPro":false,"fullname":"Siheng Chen","user":"sihengchen","type":"user"},"name":"Siheng Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:49:26.261Z","hidden":false},{"_id":"69705a68a8be625b19c2af49","name":"Jing Shao","hidden":false}],"publishedAt":"2026-01-20T17:51:56.000Z","submittedOnDailyAt":"2026-01-21T02:28:39.429Z","title":"Toward Efficient Agents: Memory, Tool learning, and Planning","submittedOnDailyBy":{"_id":"641d3efac3983aa9491677b9","avatarUrl":"/avatars/53565486351c16a1ac8ea863963e2d9b.svg","isPro":false,"fullname":"Lijun Li","user":"adwardlee","type":"user"},"summary":"Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.","upvotes":29,"discussionId":"69705a69a8be625b19c2af4a","projectPage":"https://efficient-agents.github.io/","githubRepo":"https://github.com/yxf203/Awesome-Efficient-Agents","githubRepoAddedBy":"user","ai_summary":"Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.","ai_keywords":["large language models","agentic systems","memory","tool learning","planning","latency","tokens","steps","reinforcement learning","controlled search mechanisms","Pareto frontier","efficiency metrics","evaluation protocols"],"githubStars":27,"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "}},"publishedAt":"2026-01-20T12:51:56.000Z","title":"Toward Efficient Agents: Memory, Tool learning, and Planning","summary":"Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14192.png","numComments":2,"submittedBy":{"_id":"641d3efac3983aa9491677b9","avatarUrl":"/avatars/53565486351c16a1ac8ea863963e2d9b.svg","fullname":"Lijun Li","name":"adwardlee","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "},"isAuthorParticipating":false},{"paper":{"id":"2601.13029","authors":[{"_id":"69708ffea8be625b19c2b04c","user":{"_id":"6575702b15b1ca184b0b2700","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6575702b15b1ca184b0b2700/O9cEodqQmG-gyqMiO_edR.jpeg","isPro":false,"fullname":"Zaibin Zhang","user":"MrBean2024","type":"user"},"name":"Zaibin Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-21T11:47:45.167Z","hidden":false},{"_id":"69708ffea8be625b19c2b04d","name":"Yuhan Wu","hidden":false},{"_id":"69708ffea8be625b19c2b04e","name":"Lianjie Jia","hidden":false},{"_id":"69708ffea8be625b19c2b04f","name":"Yifan Wang","hidden":false},{"_id":"69708ffea8be625b19c2b050","name":"Zhongbo Zhang","hidden":false},{"_id":"69708ffea8be625b19c2b051","user":{"_id":"6965e7d00aa591efb07b220c","avatarUrl":"/avatars/d0f65aafc3b652084213f02a4f93c453.svg","isPro":false,"fullname":"Yijiang Li","user":"luciasnowblack","type":"user"},"name":"Yijiang Li","status":"admin_assigned","statusLastChangedAt":"2026-01-21T11:59:41.952Z","hidden":false},{"_id":"69708ffea8be625b19c2b052","name":"Binghao Ran","hidden":false},{"_id":"69708ffea8be625b19c2b053","name":"Fuxi Zhang","hidden":false},{"_id":"69708ffea8be625b19c2b054","user":{"_id":"68ad6a9106bcf0ebe9624dc5","avatarUrl":"/avatars/309e383889f848c828d4b1eb4542b54a.svg","isPro":false,"fullname":"SunZhuohan","user":"sunz525","type":"user"},"name":"Zhuohan Sun","status":"admin_assigned","statusLastChangedAt":"2026-01-21T11:59:29.495Z","hidden":false},{"_id":"69708ffea8be625b19c2b055","user":{"_id":"64e314ad24809d7fa0f20fbc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bHE0w_hjDFvU-Aul0_E7g.jpeg","isPro":false,"fullname":"Zhenfei Yin","user":"JeremyYin","type":"user"},"name":"Zhenfei Yin","status":"admin_assigned","statusLastChangedAt":"2026-01-21T11:59:18.401Z","hidden":false},{"_id":"69708ffea8be625b19c2b056","name":"Lijun Wang","hidden":false},{"_id":"69708ffea8be625b19c2b057","name":"Huchuan Lu","hidden":false}],"publishedAt":"2026-01-19T13:13:54.000Z","submittedOnDailyAt":"2026-01-21T06:09:04.854Z","title":"Think3D: Thinking with Space for Spatial Reasoning","submittedOnDailyBy":{"_id":"6419309f22270b3ccf177c77","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg","isPro":false,"fullname":"William Li","user":"williamium","type":"user"},"summary":"Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.","upvotes":29,"discussionId":"69708fffa8be625b19c2b058","githubRepo":"https://github.com/zhangzaibin/spagent","githubRepoAddedBy":"user","ai_summary":"Think3D enhances vision-language models' 3D reasoning capabilities by enabling interactive spatial exploration through 3D reconstruction and camera-based operations, improving performance without additional training.","ai_keywords":["vision large models","3D reconstruction models","point clouds","camera poses","spatial reasoning","3D chain-of-thought process","reinforcement learning policy","multimodal agents","3D reasoning"],"githubStars":32},"publishedAt":"2026-01-19T08:13:54.000Z","title":"Think3D: Thinking with Space for Spatial Reasoning","summary":"Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13029.png","numComments":1,"submittedBy":{"_id":"6419309f22270b3ccf177c77","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg","fullname":"William Li","name":"williamium","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.13836","authors":[{"_id":"697067f2a8be625b19c2afa5","name":"Qian Chen","hidden":false},{"_id":"697067f2a8be625b19c2afa6","user":{"_id":"618497ea8aaadc9253c2dfa9","avatarUrl":"/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg","isPro":false,"fullname":"Fu Jinlan","user":"Jinlan","type":"user"},"name":"Jinlan Fu","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:39:51.571Z","hidden":false},{"_id":"697067f2a8be625b19c2afa7","name":"Changsong Li","hidden":false},{"_id":"697067f2a8be625b19c2afa8","name":"See-Kiong Ng","hidden":false},{"_id":"697067f2a8be625b19c2afa9","user":{"_id":"61457b8deff2c9fdb4de4988","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg","isPro":false,"fullname":"Xipeng Qiu","user":"xpqiu","type":"user"},"name":"Xipeng Qiu","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:48:17.801Z","hidden":false}],"publishedAt":"2026-01-20T10:47:20.000Z","submittedOnDailyAt":"2026-01-21T04:50:03.124Z","title":"FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs","submittedOnDailyBy":{"_id":"618497ea8aaadc9253c2dfa9","avatarUrl":"/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg","isPro":false,"fullname":"Fu Jinlan","user":"Jinlan","type":"user"},"summary":"Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).","upvotes":27,"discussionId":"697067f2a8be625b19c2afaa","projectPage":"https://openmoss.github.io/FutureOmni","githubRepo":"https://github.com/OpenMOSS/FutureOmni","githubRepoAddedBy":"user","ai_summary":"FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.","ai_keywords":["Multimodal Large Language Models","audio-visual cues","future forecasting","cross-modal causal reasoning","temporal reasoning","internal knowledge","LLM-assisted pipeline","instruction-tuning dataset","Omni-Modal Future Forecasting training strategy"],"githubStars":8,"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}},"publishedAt":"2026-01-20T05:47:20.000Z","title":"FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs","summary":"Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13836.png","numComments":1,"submittedBy":{"_id":"618497ea8aaadc9253c2dfa9","avatarUrl":"/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg","fullname":"Fu Jinlan","name":"Jinlan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.11969","authors":[{"_id":"69703c1fa8be625b19c2ae73","user":{"_id":"64096ef79e9f790c905b846d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg","isPro":false,"fullname":"Zecheng Tang","user":"ZetangForward","type":"user"},"name":"Zecheng Tang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:20:19.792Z","hidden":false},{"_id":"69703c1fa8be625b19c2ae74","user":{"_id":"65731fc31345577b7071d7df","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65731fc31345577b7071d7df/T8qnqIxnycvy1AP8CKAPb.png","isPro":false,"fullname":"Baibei Ji","user":"iiiiGray","type":"user"},"name":"Baibei Ji","status":"admin_assigned","statusLastChangedAt":"2026-01-21T11:47:12.150Z","hidden":false},{"_id":"69703c1fa8be625b19c2ae75","name":"Ruoxi Sun","hidden":false},{"_id":"69703c1fa8be625b19c2ae76","name":"Haitian Wang","hidden":false},{"_id":"69703c1fa8be625b19c2ae77","name":"WangJie You","hidden":false},{"_id":"69703c1fa8be625b19c2ae78","user":{"_id":"67760e6700d3237a069893fe","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WblRHnyjQsyqoay21Y4cN.png","isPro":false,"fullname":"yijun zhang","user":"zhangyijun166","type":"user"},"name":"Zhang Yijun","status":"admin_assigned","statusLastChangedAt":"2026-01-21T11:47:32.235Z","hidden":false},{"_id":"69703c1fa8be625b19c2ae79","user":{"_id":"690d4c6a262e79c6454e57c8","avatarUrl":"/avatars/bb36807f6fcecf5faadb3aacba4b6e27.svg","isPro":false,"fullname":"Wenpeng Zhu","user":"Christal326","type":"user"},"name":"Wenpeng Zhu","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:51:06.425Z","hidden":false},{"_id":"69703c1fa8be625b19c2ae7a","name":"Ji Qi","hidden":false},{"_id":"69703c1fa8be625b19c2ae7b","user":{"_id":"6670e285b0c03c4e9d6e0985","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg","isPro":false,"fullname":"Juntao Li","user":"douvleplus","type":"user"},"name":"Juntao Li","status":"admin_assigned","statusLastChangedAt":"2026-01-21T10:51:12.383Z","hidden":false},{"_id":"69703c1fa8be625b19c2ae7c","name":"Min Zhang","hidden":false}],"publishedAt":"2026-01-17T09:04:53.000Z","submittedOnDailyAt":"2026-01-21T00:11:34.112Z","title":"MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models","submittedOnDailyBy":{"_id":"64096ef79e9f790c905b846d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg","isPro":false,"fullname":"Zecheng Tang","user":"ZetangForward","type":"user"},"summary":"Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.","upvotes":24,"discussionId":"69703c1fa8be625b19c2ae7d","projectPage":"https://github.com/LCM-Lab/MemRewardBench","githubRepo":"https://github.com/LCM-Lab/MemRewardBench","githubRepoAddedBy":"user","ai_summary":"A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.","ai_keywords":["memory-centric mechanisms","long-context comprehension","long-form generation","reward models","MemoryRewardBench","memory management","large language models"],"githubStars":4,"organization":{"_id":"61f8e653129c9ff1b911293d","name":"SUDA","fullname":"Soochow University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"}},"publishedAt":"2026-01-17T04:04:53.000Z","title":"MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models","summary":"Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11969.png","numComments":1,"submittedBy":{"_id":"64096ef79e9f790c905b846d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg","fullname":"Zecheng Tang","name":"ZetangForward","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"61f8e653129c9ff1b911293d","name":"SUDA","fullname":"Soochow University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14004","authors":[{"_id":"69709468a8be625b19c2b069","user":{"_id":"63870c8388b39a64e1e8cdfa","avatarUrl":"/avatars/1813b49eca6eb7396fa18cccc6e24342.svg","isPro":false,"fullname":"zhanghengyuan","user":"hengyuanya","type":"user"},"name":"Hengyuan Zhang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:18:44.875Z","hidden":false},{"_id":"69709468a8be625b19c2b06a","name":"Zhihao Zhang","hidden":false},{"_id":"69709468a8be625b19c2b06b","user":{"_id":"65b92649ba9f3fa7ed4af967","avatarUrl":"/avatars/d3e072451f3a81b46ffdd142f72547d8.svg","isPro":false,"fullname":"Mingyang Wang","user":"mingyang26","type":"user"},"name":"Mingyang Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T10:15:55.556Z","hidden":false},{"_id":"69709468a8be625b19c2b06c","user":{"_id":"655b4f5cc11dee7f7e882a0c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/655b4f5cc11dee7f7e882a0c/4GaAC5Qt55eYlbNRJpVD5.png","isPro":false,"fullname":"zunhaisu","user":"zunhai","type":"user"},"name":"Zunhai Su","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:25:40.905Z","hidden":false},{"_id":"69709468a8be625b19c2b06d","user":{"_id":"61f2bc2df4eb3a3875013bff","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg","isPro":false,"fullname":"WYW","user":"WANGYIWEI","type":"user"},"name":"Yiwei Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T10:15:52.802Z","hidden":false},{"_id":"69709468a8be625b19c2b06e","user":{"_id":"63bbdb991374e3ef912d0f88","avatarUrl":"/avatars/1e889b1589fccb44b3ec4bc040021fc8.svg","isPro":false,"fullname":"Qianli Wang","user":"qiaw99","type":"user"},"name":"Qianli Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T10:15:57.822Z","hidden":false},{"_id":"69709468a8be625b19c2b06f","user":{"_id":"662ce44c8b8705f30371fba8","avatarUrl":"/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg","isPro":false,"fullname":"Shuzhou Yuan","user":"shuzyuan","type":"user"},"name":"Shuzhou Yuan","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:27:18.543Z","hidden":false},{"_id":"69709468a8be625b19c2b070","name":"Ercong Nie","hidden":false},{"_id":"69709468a8be625b19c2b071","user":{"_id":"64832945413ff0a011f41fe8","avatarUrl":"/avatars/32e576adbaeeead357d903d716638b0e.svg","isPro":true,"fullname":"Xufeng Duan","user":"XufengDuan","type":"user"},"name":"Xufeng Duan","status":"claimed_verified","statusLastChangedAt":"2026-01-21T10:15:50.690Z","hidden":false},{"_id":"69709468a8be625b19c2b072","name":"Qibo Xue","hidden":false},{"_id":"69709468a8be625b19c2b073","name":"Zeping Yu","hidden":false},{"_id":"69709468a8be625b19c2b074","name":"Chenming Shang","hidden":false},{"_id":"69709468a8be625b19c2b075","name":"Xiao Liang","hidden":false},{"_id":"69709468a8be625b19c2b076","name":"Jing Xiong","hidden":false},{"_id":"69709468a8be625b19c2b077","name":"Hui Shen","hidden":false},{"_id":"69709468a8be625b19c2b078","name":"Chaofan Tao","hidden":false},{"_id":"69709468a8be625b19c2b079","name":"Zhengwu Liu","hidden":false},{"_id":"69709468a8be625b19c2b07a","name":"Senjie Jin","hidden":false},{"_id":"69709468a8be625b19c2b07b","name":"Zhiheng Xi","hidden":false},{"_id":"69709468a8be625b19c2b07c","name":"Dongdong Zhang","hidden":false},{"_id":"69709468a8be625b19c2b07d","name":"Sophia Ananiadou","hidden":false},{"_id":"69709468a8be625b19c2b07e","name":"Tao Gui","hidden":false},{"_id":"69709468a8be625b19c2b07f","name":"Ruobing Xie","hidden":false},{"_id":"69709468a8be625b19c2b080","name":"Hayden Kwok-Hay So","hidden":false},{"_id":"69709468a8be625b19c2b081","name":"Hinrich Schtze","hidden":false},{"_id":"69709468a8be625b19c2b082","name":"Xuanjing Huang","hidden":false},{"_id":"69709468a8be625b19c2b083","name":"Qi Zhang","hidden":false},{"_id":"69709468a8be625b19c2b084","name":"Ngai Wong","hidden":false}],"publishedAt":"2026-01-20T14:23:23.000Z","submittedOnDailyAt":"2026-01-21T13:50:15.791Z","title":"Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models","submittedOnDailyBy":{"_id":"63870c8388b39a64e1e8cdfa","avatarUrl":"/avatars/1813b49eca6eb7396fa18cccc6e24342.svg","isPro":false,"fullname":"zhanghengyuan","user":"hengyuanya","type":"user"},"summary":"Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.","upvotes":17,"discussionId":"69709468a8be625b19c2b085","ai_summary":"Mechanistic interpretability is presented as an actionable framework for understanding and optimizing large language models through systematic localization, steering, and improvement methods.","ai_keywords":["Mechanistic Interpretability","Large Language Models","Localizing","Steering","Interpretable Objects","Alignment","Capability","Efficiency"]},"publishedAt":"2026-01-20T09:23:23.000Z","title":"Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models","summary":"Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14004.png","numComments":1,"submittedBy":{"_id":"63870c8388b39a64e1e8cdfa","avatarUrl":"/avatars/1813b49eca6eb7396fa18cccc6e24342.svg","fullname":"zhanghengyuan","name":"hengyuanya","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.11522","authors":[{"_id":"696f604fbfeda8d7e160c9a9","user":{"_id":"67a711a835423a46f4f44583","avatarUrl":"/avatars/1a6a6475f73463f1f4ddbbfc1d59c4ed.svg","isPro":false,"fullname":"ruihengzhang","user":"ZrH42","type":"user"},"name":"Ruiheng Zhang","status":"claimed_verified","statusLastChangedAt":"2026-01-20T14:48:05.051Z","hidden":false},{"_id":"696f604fbfeda8d7e160c9aa","user":{"_id":"67756c9c846a267749304255","avatarUrl":"/avatars/01f09805b561887c55d1b9ad4e96b461.svg","isPro":false,"fullname":"Jingfeng Yao","user":"MapleF9","type":"user"},"name":"Jingfeng Yao","status":"admin_assigned","statusLastChangedAt":"2026-01-21T12:00:32.366Z","hidden":false},{"_id":"696f604fbfeda8d7e160c9ab","name":"Huangxuan Zhao","hidden":false},{"_id":"696f604fbfeda8d7e160c9ac","name":"Hao Yan","hidden":false},{"_id":"696f604fbfeda8d7e160c9ad","name":"Xiao He","hidden":false},{"_id":"696f604fbfeda8d7e160c9ae","name":"Lei Chen","hidden":false},{"_id":"696f604fbfeda8d7e160c9af","name":"Zhou Wei","hidden":false},{"_id":"696f604fbfeda8d7e160c9b0","name":"Yong Luo","hidden":false},{"_id":"696f604fbfeda8d7e160c9b1","name":"Zengmao Wang","hidden":false},{"_id":"696f604fbfeda8d7e160c9b2","name":"Lefei Zhang","hidden":false},{"_id":"696f604fbfeda8d7e160c9b3","name":"Dacheng Tao","hidden":false},{"_id":"696f604fbfeda8d7e160c9b4","name":"Bo Du","hidden":false}],"publishedAt":"2026-01-16T18:59:58.000Z","submittedOnDailyAt":"2026-01-21T00:35:45.455Z","title":"UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation","submittedOnDailyBy":{"_id":"67a711a835423a46f4f44583","avatarUrl":"/avatars/1a6a6475f73463f1f4ddbbfc1d59c4ed.svg","isPro":false,"fullname":"ruihengzhang","user":"ZrH42","type":"user"},"summary":"Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.","upvotes":15,"discussionId":"696f604fbfeda8d7e160c9b5","githubRepo":"https://github.com/ZrH42/UniX","githubRepoAddedBy":"user","ai_summary":"UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.","ai_keywords":["medical foundation models","autoregressive architectures","diffusion models","cross-modal self-attention","data cleaning pipeline","multi-stage training strategy","visual understanding","image generation"],"githubStars":19,"organization":{"_id":"6350bdf559bfa9a85d42fea4","name":"WuhanUniversity","fullname":"Wuhan Univeristy","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"}},"publishedAt":"2026-01-16T13:59:58.000Z","title":"UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation","summary":"Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11522.png","numComments":1,"submittedBy":{"_id":"67a711a835423a46f4f44583","avatarUrl":"/avatars/1a6a6475f73463f1f4ddbbfc1d59c4ed.svg","fullname":"ruihengzhang","name":"ZrH42","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6350bdf559bfa9a85d42fea4","name":"WuhanUniversity","fullname":"Wuhan Univeristy","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.12294","authors":[{"_id":"697055b4a8be625b19c2af18","user":{"_id":"6474e1afb68461d5cf7c41cc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png","isPro":false,"fullname":"Dawei Li","user":"wjldw","type":"user"},"name":"Dawei Li","status":"admin_assigned","statusLastChangedAt":"2026-01-21T12:00:07.519Z","hidden":false},{"_id":"697055b4a8be625b19c2af19","user":{"_id":"644cb05d778ecbfb9783fd8b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QSjINHKRs1OLz8Y34j8Ak.png","isPro":false,"fullname":"Yuguang Yao","user":"yaoyugua","type":"user"},"name":"Yuguang Yao","status":"admin_assigned","statusLastChangedAt":"2026-01-21T12:00:12.781Z","hidden":false},{"_id":"697055b4a8be625b19c2af1a","name":"Zhen Tan","hidden":false},{"_id":"697055b4a8be625b19c2af1b","name":"Huan Liu","hidden":false},{"_id":"697055b4a8be625b19c2af1c","user":{"_id":"65dcb410bda21d181b38321b","avatarUrl":"/avatars/a9caed79c4eb14352b4015377fcae1d7.svg","isPro":false,"fullname":"Ruocheng Guo","user":"rguo12","type":"user"},"name":"Ruocheng Guo","status":"admin_assigned","statusLastChangedAt":"2026-01-21T12:00:23.178Z","hidden":false}],"publishedAt":"2026-01-18T07:48:36.000Z","submittedOnDailyAt":"2026-01-21T01:59:37.515Z","title":"ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents","submittedOnDailyBy":{"_id":"6474e1afb68461d5cf7c41cc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png","isPro":false,"fullname":"Dawei Li","user":"wjldw","type":"user"},"summary":"Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.","upvotes":13,"discussionId":"697055b4a8be625b19c2af1d","ai_summary":"ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.","ai_keywords":["process reward models","tool-using agents","reward-guided search","agent trajectories","step-level rewards","large language models","multi-LLM verification","offline sampling","online sampling"],"organization":{"_id":"64e917fc662874dbc9b6a828","name":"intuit","fullname":"Intuit","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/AGLb0CFLiqEd5BBKLvtPO.jpeg"}},"publishedAt":"2026-01-18T02:48:36.000Z","title":"ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents","summary":"Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12294.png","numComments":1,"submittedBy":{"_id":"6474e1afb68461d5cf7c41cc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png","fullname":"Dawei Li","name":"wjldw","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"64e917fc662874dbc9b6a828","name":"intuit","fullname":"Intuit","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/AGLb0CFLiqEd5BBKLvtPO.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.13247","authors":[{"_id":"69704b80a8be625b19c2aecf","name":"Baochang Ren","hidden":false},{"_id":"69704b80a8be625b19c2aed0","name":"Yunzhi Yao","hidden":false},{"_id":"69704b80a8be625b19c2aed1","name":"Rui Sun","hidden":false},{"_id":"69704b80a8be625b19c2aed2","name":"Shuofei Qiao","hidden":false},{"_id":"69704b80a8be625b19c2aed3","name":"Ningyu Zhang","hidden":false},{"_id":"69704b80a8be625b19c2aed4","name":"Huajun Chen","hidden":false}],"publishedAt":"2026-01-19T17:33:31.000Z","submittedOnDailyAt":"2026-01-21T01:15:35.994Z","title":"Aligning Agentic World Models via Knowledgeable Experience Learning","submittedOnDailyBy":{"_id":"620b3bbb0668e435407c8d0a","avatarUrl":"/avatars/e0fccbb2577d76088e09f054c35cffbc.svg","isPro":true,"fullname":"Ningyu Zhang","user":"Ningyu","type":"user"},"summary":"Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.","upvotes":12,"discussionId":"69704b80a8be625b19c2aed5","projectPage":"https://zjunlp.github.io/WorldMind/","githubRepo":"https://github.com/zjunlp/WorldMind","githubRepoAddedBy":"user","ai_summary":"WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.","ai_keywords":["Large Language Models","world models","physical hallucinations","alignment strategies","parametric encapsulation","symbolic World Knowledge Repository","Process Experience","Goal Experience","EB-ALFRED","EB-Habitat","cross-model transferability","cross-environment transferability"],"githubStars":21,"organization":{"_id":"6345aadf5efccdc07f1365a5","name":"ZhejiangUniversity","fullname":"Zhejiang University"}},"publishedAt":"2026-01-19T12:33:31.000Z","title":"Aligning Agentic World Models via Knowledgeable Experience Learning","summary":"Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13247.png","numComments":1,"submittedBy":{"_id":"620b3bbb0668e435407c8d0a","avatarUrl":"/avatars/e0fccbb2577d76088e09f054c35cffbc.svg","fullname":"Ningyu Zhang","name":"Ningyu","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":36,"isUserFollowing":false},"organization":{"_id":"6345aadf5efccdc07f1365a5","name":"ZhejiangUniversity","fullname":"Zhejiang University"},"isAuthorParticipating":false},{"paper":{"id":"2601.11888","authors":[{"_id":"69707ea4a8be625b19c2b00d","name":"Wenhan Liu","hidden":false},{"_id":"69707ea4a8be625b19c2b00e","name":"Xinyu Ma","hidden":false},{"_id":"69707ea4a8be625b19c2b00f","name":"Yutao Zhu","hidden":false},{"_id":"69707ea4a8be625b19c2b010","name":"Yuchen Li","hidden":false},{"_id":"69707ea4a8be625b19c2b011","name":"Daiting Shi","hidden":false},{"_id":"69707ea4a8be625b19c2b012","name":"Dawei Yin","hidden":false},{"_id":"69707ea4a8be625b19c2b013","name":"Zhicheng Dou","hidden":false}],"publishedAt":"2026-01-17T02:59:54.000Z","submittedOnDailyAt":"2026-01-21T04:55:05.623Z","title":"Agentic-R: Learning to Retrieve for Agentic Search","submittedOnDailyBy":{"_id":"62cd2f13979d883655cd5377","avatarUrl":"/avatars/400c252d20d68aca56e0d0280498ce17.svg","isPro":false,"fullname":"Xinyu Ma","user":"xyma","type":"user"},"summary":"Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed , consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.","upvotes":10,"discussionId":"69707ea4a8be625b19c2b014","ai_summary":"A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.","ai_keywords":["agentic search","multi-step reasoning","on-demand retrieval","similarity-based retrievers","retrieval-augmented generation","multi-turn agentic search","iterative training strategy","bidirectional optimization","evolving queries","answer correctness","passage utility"]},"publishedAt":"2026-01-16T21:59:54.000Z","title":"Agentic-R: Learning to Retrieve for Agentic Search","summary":"Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed , consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11888.png","numComments":1,"submittedBy":{"_id":"62cd2f13979d883655cd5377","avatarUrl":"/avatars/400c252d20d68aca56e0d0280498ce17.svg","fullname":"Xinyu Ma","name":"xyma","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.13288","authors":[{"_id":"69704864a8be625b19c2aec6","name":"Gonzalo Ariel Meyoyan","hidden":false},{"_id":"69704864a8be625b19c2aec7","user":{"_id":"63b81aace60862785afd8ca2","avatarUrl":"/avatars/f96ce78af6ad42514235bab811544789.svg","isPro":false,"fullname":"Luciano Del Corro","user":"lucianodelcorro","type":"user"},"name":"Luciano Del Corro","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:20:02.165Z","hidden":false}],"publishedAt":"2026-01-19T18:40:29.000Z","submittedOnDailyAt":"2026-01-21T01:05:31.239Z","title":"A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification","submittedOnDailyBy":{"_id":"63b81aace60862785afd8ca2","avatarUrl":"/avatars/f96ce78af6ad42514235bab811544789.svg","isPro":false,"fullname":"Luciano Del Corro","user":"lucianodelcorro","type":"user"},"summary":"Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.","upvotes":9,"discussionId":"69704864a8be625b19c2aec8","ai_summary":"Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.","ai_keywords":["hidden states","token-layer hidden-state tensor","representation selection","two-stage aggregator","pooling","attention gate","multi-head self-attention","probes","classification"],"organization":{"_id":"66d5cb037300d333daebedd9","name":"UdeSA","fullname":"Universidad de San Andrs","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6594888e754092f6b1443bbd/C-ZrIbZoTgAj9p-u0v5jZ.png"}},"publishedAt":"2026-01-19T13:40:29.000Z","title":"A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification","summary":"Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13288.png","numComments":1,"submittedBy":{"_id":"63b81aace60862785afd8ca2","avatarUrl":"/avatars/f96ce78af6ad42514235bab811544789.svg","fullname":"Luciano Del Corro","name":"lucianodelcorro","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"66d5cb037300d333daebedd9","name":"UdeSA","fullname":"Universidad de San Andrs","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6594888e754092f6b1443bbd/C-ZrIbZoTgAj9p-u0v5jZ.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14232","authors":[{"_id":"6970b0cfa8be625b19c2b0b9","name":"Egor Cherepanov","hidden":false},{"_id":"6970b0cfa8be625b19c2b0ba","name":"Daniil Zelezetsky","hidden":false},{"_id":"6970b0cfa8be625b19c2b0bb","name":"Alexey K. Kovalev","hidden":false},{"_id":"6970b0cfa8be625b19c2b0bc","name":"Aleksandr I. Panov","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6668687caee0993c95b0eb81/0kEUs6bzsBqEGv1oTlPcw.gif"],"publishedAt":"2026-01-20T18:44:28.000Z","submittedOnDailyAt":"2026-01-21T08:44:49.169Z","title":"KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning","submittedOnDailyBy":{"_id":"6668687caee0993c95b0eb81","avatarUrl":"/avatars/301fe1f395e0a129b1c9785868fa9858.svg","isPro":false,"fullname":"Egor Cherepanov","user":"avanturist","type":"user"},"summary":"Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.","upvotes":7,"discussionId":"6970b0cfa8be625b19c2b0bd","projectPage":"https://avanturist322.github.io/KAGEBench/","githubRepo":"https://github.com/CognitiveAISystems/kage-bench","githubRepoAddedBy":"user","ai_summary":"KAGE-Env is a JAX-native 2D platformer environment that isolates visual shifts from underlying control problems, enabling systematic analysis of visual generalization in reinforcement learning.","ai_keywords":["pixel-based reinforcement learning","visual distribution shift","latent dynamics","reward function","JAX-native","2D platformer","visual axes","state-conditional action distribution","PPO-CNN","environment steps per second"],"githubStars":7},"publishedAt":"2026-01-20T13:44:28.000Z","title":"KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning","summary":"Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6668687caee0993c95b0eb81/0kEUs6bzsBqEGv1oTlPcw.gif"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14232.png","numComments":1,"submittedBy":{"_id":"6668687caee0993c95b0eb81","avatarUrl":"/avatars/301fe1f395e0a129b1c9785868fa9858.svg","fullname":"Egor Cherepanov","name":"avanturist","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.14251","authors":[{"_id":"6970a948a8be625b19c2b098","user":{"_id":"62cd695e94b9dcedbf1818e5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62cd695e94b9dcedbf1818e5/qhKhRxdNdbKPIAUfYBvtI.png","isPro":false,"fullname":"Said Taghadouini","user":"staghado","type":"user"},"name":"Said Taghadouini","status":"claimed_verified","statusLastChangedAt":"2026-01-21T10:39:37.432Z","hidden":false},{"_id":"6970a948a8be625b19c2b099","user":{"_id":"6421a255eaad1bcb28afdd0e","avatarUrl":"/avatars/a00f4981d3da30bf4d1a0cb0af00a37d.svg","isPro":false,"fullname":"Adrien Cavaills","user":"adcavail","type":"user"},"name":"Adrien Cavaills","status":"claimed_verified","statusLastChangedAt":"2026-01-21T10:51:23.521Z","hidden":false},{"_id":"6970a948a8be625b19c2b09a","user":{"_id":"63a042880c6d9efa305ea6cd","avatarUrl":"/avatars/9a6685e69dfcfe46661545e8aa9841fd.svg","isPro":false,"fullname":"Baptiste Aubertin","user":"Bapt120","type":"user"},"name":"Baptiste Aubertin","status":"claimed_verified","statusLastChangedAt":"2026-01-21T10:48:01.934Z","hidden":false}],"publishedAt":"2026-01-20T18:58:32.000Z","submittedOnDailyAt":"2026-01-21T08:07:43.355Z","title":"LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR","submittedOnDailyBy":{"_id":"62cd695e94b9dcedbf1818e5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62cd695e94b9dcedbf1818e5/qhKhRxdNdbKPIAUfYBvtI.png","isPro":false,"fullname":"Said Taghadouini","user":"staghado","type":"user"},"summary":"We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9times smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses.","upvotes":5,"discussionId":"6970a948a8be625b19c2b09b","ai_summary":"LightOnOCR-2-1B is a compact 1B-parameter vision-language model that performs end-to-end document image-to-text conversion with improved localization and robustness through specialized training techniques.","ai_keywords":["vision-language model","OCR pipeline","distillation mix","pretraining","checkpoint averaging","task-arithmetic merging","localization","resume strategy","RLVR","IoU-based rewards"],"organization":{"_id":"6271618b3623fb51324c461e","name":"lightonai","fullname":"LightOn AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1651597775471-62715572ab9243b5d40cbb1d.png"}},"publishedAt":"2026-01-20T13:58:32.000Z","title":"LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR","summary":"We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9times smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14251.png","numComments":1,"submittedBy":{"_id":"62cd695e94b9dcedbf1818e5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62cd695e94b9dcedbf1818e5/qhKhRxdNdbKPIAUfYBvtI.png","fullname":"Said Taghadouini","name":"staghado","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":41,"isUserFollowing":false},"organization":{"_id":"6271618b3623fb51324c461e","name":"lightonai","fullname":"LightOn AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1651597775471-62715572ab9243b5d40cbb1d.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14046","authors":[{"_id":"69706334a8be625b19c2af8f","user":{"_id":"66ccd2fcdba9f642125e3a55","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66ccd2fcdba9f642125e3a55/hTKwQQahkbpNuGlaRhKp7.jpeg","isPro":false,"fullname":"Shikhar Bharadwaj","user":"shikhar7ssu","type":"user"},"name":"Shikhar Bharadwaj","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:30.067Z","hidden":false},{"_id":"69706334a8be625b19c2af90","user":{"_id":"643e6952f6bb3c31a26d3650","avatarUrl":"/avatars/70bfa6a28f71e78446ccac5742bc7be8.svg","isPro":false,"fullname":"Chin-Jou Li","user":"cjli","type":"user"},"name":"Chin-Jou Li","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:27.747Z","hidden":false},{"_id":"69706334a8be625b19c2af91","user":{"_id":"66138e8107b5892cee1fdc84","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66138e8107b5892cee1fdc84/s9tumjav-9OiWnjQoSnGl.jpeg","isPro":false,"fullname":"Yoonjae Kim","user":"y00njaekim","type":"user"},"name":"Yoonjae Kim","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:25.403Z","hidden":false},{"_id":"69706334a8be625b19c2af92","user":{"_id":"64ba7c68710f5e5476877218","avatarUrl":"/avatars/1e5ad96dd0b237db1bfee400aa8ed229.svg","isPro":false,"fullname":"Kwanghee Choi","user":"juice500","type":"user"},"name":"Kwanghee Choi","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:22.863Z","hidden":false},{"_id":"69706334a8be625b19c2af93","user":{"_id":"618b80845dc6e1c37cc46643","avatarUrl":"/avatars/60cdf870213f41d63d077cc5cfaf75cb.svg","isPro":false,"fullname":"Eunjung Yeo","user":"speech31","type":"user"},"name":"Eunjung Yeo","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:20.183Z","hidden":false},{"_id":"69706334a8be625b19c2af94","name":"Ryan Soh-Eun Shim","hidden":false},{"_id":"69706334a8be625b19c2af95","name":"Hanyu Zhou","hidden":false},{"_id":"69706334a8be625b19c2af96","name":"Brendon Boldt","hidden":false},{"_id":"69706334a8be625b19c2af97","name":"Karen Rosero Jacome","hidden":false},{"_id":"69706334a8be625b19c2af98","user":{"_id":"62b1fe30b9bc778fe4e68c5d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62b1fe30b9bc778fe4e68c5d/3qVYitofCIBloSN0be7pp.jpeg","isPro":false,"fullname":"Kalvin Chang","user":"kalbin","type":"user"},"name":"Kalvin Chang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:17.515Z","hidden":false},{"_id":"69706334a8be625b19c2af99","name":"Darsh Agrawal","hidden":false},{"_id":"69706334a8be625b19c2af9a","name":"Keer Xu","hidden":false},{"_id":"69706334a8be625b19c2af9b","name":"Chao-Han Huck Yang","hidden":false},{"_id":"69706334a8be625b19c2af9c","name":"Jian Zhu","hidden":false},{"_id":"69706334a8be625b19c2af9d","name":"Shinji Watanabe","hidden":false},{"_id":"69706334a8be625b19c2af9e","name":"David R. Mortensen","hidden":false}],"publishedAt":"2026-01-20T15:00:36.000Z","submittedOnDailyAt":"2026-01-21T02:58:09.771Z","title":"PRiSM: Benchmarking Phone Realization in Speech Models","submittedOnDailyBy":{"_id":"66ccd2fcdba9f642125e3a55","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66ccd2fcdba9f642125e3a55/hTKwQQahkbpNuGlaRhKp7.jpeg","isPro":false,"fullname":"Shikhar Bharadwaj","user":"shikhar7ssu","type":"user"},"summary":"Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.","upvotes":5,"discussionId":"69706335a8be625b19c2af9f","githubRepo":"https://github.com/changelinglab/prism","githubRepoAddedBy":"user","ai_summary":"PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.","ai_keywords":["phone recognition","cross-lingual speech processing","phonetic analysis","intrinsic evaluation","extrinsic evaluation","transcription-based evaluation","downstream utility","clinical settings","educational settings","multilingual settings","transcription probes","representation probes","language exposure","encoder-CTC models","large audio language models"],"githubStars":2,"organization":{"_id":"67640ecc3fb3ec9c8a1e7177","name":"changelinglab","fullname":"ChangeLing Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62b1fe30b9bc778fe4e68c5d/14dBRYuWvgAOZZxt0vpOl.jpeg"}},"publishedAt":"2026-01-20T10:00:36.000Z","title":"PRiSM: Benchmarking Phone Realization in Speech Models","summary":"Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14046.png","numComments":2,"submittedBy":{"_id":"66ccd2fcdba9f642125e3a55","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66ccd2fcdba9f642125e3a55/hTKwQQahkbpNuGlaRhKp7.jpeg","fullname":"Shikhar Bharadwaj","name":"shikhar7ssu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"67640ecc3fb3ec9c8a1e7177","name":"changelinglab","fullname":"ChangeLing Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62b1fe30b9bc778fe4e68c5d/14dBRYuWvgAOZZxt0vpOl.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.13761","authors":[{"_id":"697058efa8be625b19c2af2d","name":"Shengda Fan","hidden":false},{"_id":"697058efa8be625b19c2af2e","user":{"_id":"68905a353cf91a8e828fd8a1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68905a353cf91a8e828fd8a1/lyXMELqDGOHNlXeYlTL5X.jpeg","isPro":false,"fullname":"Xuyan Ye","user":"LulaCola","type":"user"},"name":"Xuyan Ye","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:52.790Z","hidden":false},{"_id":"697058efa8be625b19c2af2f","name":"Yankai Lin","hidden":false}],"publishedAt":"2026-01-20T09:12:27.000Z","submittedOnDailyAt":"2026-01-21T21:28:05.063Z","title":"DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution","submittedOnDailyBy":{"_id":"68905a353cf91a8e828fd8a1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68905a353cf91a8e828fd8a1/lyXMELqDGOHNlXeYlTL5X.jpeg","isPro":false,"fullname":"Xuyan Ye","user":"LulaCola","type":"user"},"summary":"Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.","upvotes":5,"discussionId":"697058efa8be625b19c2af30","githubRepo":"https://github.com/RUCBM/DARC","githubRepoAddedBy":"user","ai_summary":"A two-stage framework called DARC stabilizes self-play with large language models by decoupling question generation and using asymmetric self-distillation with document-augmented teachers to improve reasoning performance.","ai_keywords":["self-play","large language models","optimization instability","non-stationary objectives","bootstrapping errors","self-generated pseudo-labels","Questioner","Solver","decoupled asymmetric reasoning curriculum","self-distillation","document-augmented teacher","self-evolution process","reasoning benchmarks","backbone models","fully supervised models"],"githubStars":2,"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}},"publishedAt":"2026-01-20T04:12:27.000Z","title":"DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution","summary":"Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13761.png","numComments":1,"submittedBy":{"_id":"68905a353cf91a8e828fd8a1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68905a353cf91a8e828fd8a1/lyXMELqDGOHNlXeYlTL5X.jpeg","fullname":"Xuyan Ye","name":"LulaCola","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.13976","authors":[{"_id":"6970b132a8be625b19c2b0c4","name":"Jing Zuo","hidden":false},{"_id":"6970b132a8be625b19c2b0c5","name":"Lingzhou Mu","hidden":false},{"_id":"6970b132a8be625b19c2b0c6","user":{"_id":"6414106ce7d5f817d204e160","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6414106ce7d5f817d204e160/i-AeOvUzm7CIJ5d3ZacjX.png","isPro":false,"fullname":"Frank Jiang","user":"frankjiang","type":"user"},"name":"Fan Jiang","status":"claimed_verified","statusLastChangedAt":"2026-01-21T11:46:40.735Z","hidden":false},{"_id":"6970b132a8be625b19c2b0c7","name":"Chengcheng Ma","hidden":false},{"_id":"6970b132a8be625b19c2b0c8","name":"Mu Xu","hidden":false},{"_id":"6970b132a8be625b19c2b0c9","name":"Yonggang Qi","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6414106ce7d5f817d204e160/Cnq3BwAFWOfIRHCuxRKCl.jpeg"],"publishedAt":"2026-01-20T13:54:10.000Z","submittedOnDailyAt":"2026-01-21T08:51:32.068Z","title":"FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation","submittedOnDailyBy":{"_id":"6414106ce7d5f817d204e160","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6414106ce7d5f817d204e160/i-AeOvUzm7CIJ5d3ZacjX.png","isPro":false,"fullname":"Frank Jiang","user":"frankjiang","type":"user"},"summary":"Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.","upvotes":4,"discussionId":"6970b132a8be625b19c2b0ca","projectPage":"https://fantasy-amap.github.io/fantasy-vln/","githubRepo":"https://github.com/Fantasy-AMAP/fantasy-vln","githubRepoAddedBy":"user","ai_summary":"FantasyVLN presents a unified implicit reasoning framework for vision-and-language navigation that enhances reasoning capabilities without explicit token overhead, achieving real-time performance with improved accuracy.","ai_keywords":["Chain-of-Thought","Vision-and-Language Navigation","Visual AutoRegressor","implicit reasoning","multimodal CoT","latent space","multi-CoT strategy","real-time navigation","inference latency"],"githubStars":3,"organization":{"_id":"641415d08900ef6afa2fcb73","name":"acvlab","fullname":"Alibaba AMAP CV Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6414106ce7d5f817d204e160/dfveRtrRy8Xn7QpG684zl.png"}},"publishedAt":"2026-01-20T08:54:10.000Z","title":"FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation","summary":"Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6414106ce7d5f817d204e160/Cnq3BwAFWOfIRHCuxRKCl.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13976.png","numComments":1,"submittedBy":{"_id":"6414106ce7d5f817d204e160","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6414106ce7d5f817d204e160/i-AeOvUzm7CIJ5d3ZacjX.png","fullname":"Frank Jiang","name":"frankjiang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"641415d08900ef6afa2fcb73","name":"acvlab","fullname":"Alibaba AMAP CV Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6414106ce7d5f817d204e160/dfveRtrRy8Xn7QpG684zl.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14249","authors":[{"_id":"6970e682572de08d9c7ae914","name":"Yuming Yang","hidden":false},{"_id":"6970e682572de08d9c7ae915","name":"Mingyoung Lai","hidden":false},{"_id":"6970e682572de08d9c7ae916","name":"Wanxu Zhao","hidden":false},{"_id":"6970e682572de08d9c7ae917","name":"Xiaoran Fan","hidden":false},{"_id":"6970e682572de08d9c7ae918","name":"Zhiheng Xi","hidden":false},{"_id":"6970e682572de08d9c7ae919","name":"Mingqi Wu","hidden":false},{"_id":"6970e682572de08d9c7ae91a","name":"Chiyue Huang","hidden":false},{"_id":"6970e682572de08d9c7ae91b","name":"Jun Zhao","hidden":false},{"_id":"6970e682572de08d9c7ae91c","name":"Haijun Lv","hidden":false},{"_id":"6970e682572de08d9c7ae91d","name":"Jian Tong","hidden":false},{"_id":"6970e682572de08d9c7ae91e","name":"Yunhua Zhou","hidden":false},{"_id":"6970e682572de08d9c7ae91f","name":"Yicheng Zou","hidden":false},{"_id":"6970e682572de08d9c7ae920","name":"Qipeng Guo","hidden":false},{"_id":"6970e682572de08d9c7ae921","name":"Tao Gui","hidden":false},{"_id":"6970e682572de08d9c7ae922","name":"Qi Zhang","hidden":false},{"_id":"6970e682572de08d9c7ae923","name":"Xuanjing Huang","hidden":false}],"publishedAt":"2026-01-20T18:58:10.000Z","submittedOnDailyAt":"2026-01-21T12:18:50.870Z","title":"Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment","submittedOnDailyBy":{"_id":"655c6b1abfb531437a54c0e6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg","isPro":false,"fullname":"Yuming Yang","user":"Umean","type":"user"},"summary":"Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.","upvotes":3,"discussionId":"6970e682572de08d9c7ae924","ai_summary":"Researchers introduce a novel metric called Rank-Surprisal Ratio (RSR) to better assess the suitability of reasoning trajectories for distilling knowledge from large language models, demonstrating superior performance compared to existing methods.","ai_keywords":["reasoning trajectories","distillation","teacher-student LLMs","student likelihood","token-wise rank","negative log-likelihood","Spearman correlation"],"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"}},"publishedAt":"2026-01-20T13:58:10.000Z","title":"Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment","summary":"Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14249.png","numComments":1,"submittedBy":{"_id":"655c6b1abfb531437a54c0e6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg","fullname":"Yuming Yang","name":"Umean","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.14209","authors":[{"_id":"6970ff95c1c7409747bf9425","name":"Matthew Y. R. Yang","hidden":false},{"_id":"6970ff95c1c7409747bf9426","name":"Hao Bai","hidden":false},{"_id":"6970ff95c1c7409747bf9427","name":"Ian Wu","hidden":false},{"_id":"6970ff95c1c7409747bf9428","name":"Gene Yang","hidden":false},{"_id":"6970ff95c1c7409747bf9429","name":"Amrith Setlur","hidden":false},{"_id":"6970ff95c1c7409747bf942a","name":"Aviral Kumar","hidden":false}],"publishedAt":"2026-01-20T18:15:38.000Z","submittedOnDailyAt":"2026-01-21T14:03:47.351Z","title":"InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning","submittedOnDailyBy":{"_id":"62927c2e56fedc76e396b3ca","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg","isPro":false,"fullname":"HAO BAI","user":"JackBAI","type":"user"},"summary":"Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.","upvotes":3,"discussionId":"6970ff96c1c7409747bf942b","ai_summary":"Intervention Training improves large language model reasoning by enabling fine-grained credit assignment through targeted corrections that localize errors and enhance reinforcement learning performance.","ai_keywords":["reinforcement learning","credit assignment","process reward model","supervised fine-tuning","intervention training","reasoning traces","error localization","policy optimization"],"organization":{"_id":"679fa8f1fb15b4e60aeab3bf","name":"CMU-AIRe","fullname":"CMU Artificial Intelligence and Reinforcement Learning (AIRe) Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6500bbf5e102da55f9ed43fc/4TwzSKABb_lVnu_ZOP6e1.png"}},"publishedAt":"2026-01-20T13:15:38.000Z","title":"InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning","summary":"Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14209.png","numComments":1,"submittedBy":{"_id":"62927c2e56fedc76e396b3ca","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg","fullname":"HAO BAI","name":"JackBAI","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"679fa8f1fb15b4e60aeab3bf","name":"CMU-AIRe","fullname":"CMU Artificial Intelligence and Reinforcement Learning (AIRe) Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6500bbf5e102da55f9ed43fc/4TwzSKABb_lVnu_ZOP6e1.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.13697","authors":[{"_id":"6970e353572de08d9c7ae90d","name":"Zhihang Yuan","hidden":false},{"_id":"6970e353572de08d9c7ae90e","name":"Chengyu Yue","hidden":false},{"_id":"6970e353572de08d9c7ae90f","name":"Long Huang","hidden":false},{"_id":"6970e353572de08d9c7ae910","name":"Litu Ou","hidden":false},{"_id":"6970e353572de08d9c7ae911","name":"Lei Shi","hidden":false}],"publishedAt":"2026-01-20T07:51:32.000Z","submittedOnDailyAt":"2026-01-21T12:06:08.981Z","title":"Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning","submittedOnDailyBy":{"_id":"622f2feea32d46b4be9ed8c4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NDeZQZQK5U-9m10yQwDVf.png","isPro":false,"fullname":"Litu Ou","user":"learn3r","type":"user"},"summary":"Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.","upvotes":2,"discussionId":"6970e353572de08d9c7ae912","ai_summary":"GRADFILTERING is an uncertainty-aware data selection framework for instruction tuning that uses gradient signal-to-noise ratio to improve LLM adaptation efficiency and performance.","ai_keywords":["instruction tuning","large language models","data selection","gradient datastore","LoRA ensemble","Gradient Signal-to-Noise Ratio","G-SNR","uncertainty-aware scoring","objective-agnostic","LLM-as-a-judge evaluations"],"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}},"publishedAt":"2026-01-20T02:51:32.000Z","title":"Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning","summary":"Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13697.png","numComments":1,"submittedBy":{"_id":"622f2feea32d46b4be9ed8c4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NDeZQZQK5U-9m10yQwDVf.png","fullname":"Litu Ou","name":"learn3r","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.12937","authors":[{"_id":"69708c50a8be625b19c2b01f","user":{"_id":"65ff72e6b9bcfbb87523d680","avatarUrl":"/avatars/344c8e86017ebf8ac84794bea56915b3.svg","isPro":false,"fullname":"M. Bilgehan Ertan","user":"bilgehanertan","type":"user"},"name":"Murat Bilgehan Ertan","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:18:49.845Z","hidden":false},{"_id":"69708c50a8be625b19c2b020","user":{"_id":"62d7e9120d2ce1c626244e7d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62d7e9120d2ce1c626244e7d/t7RTVRjMgM3_ukD9x-mdj.png","isPro":false,"fullname":"emirhan boge","user":"emirhanboge","type":"user"},"name":"Emirhan Bge","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:18:47.470Z","hidden":false},{"_id":"69708c50a8be625b19c2b021","name":"Min Chen","hidden":false},{"_id":"69708c50a8be625b19c2b022","name":"Kaleel Mahmood","hidden":false},{"_id":"69708c50a8be625b19c2b023","name":"Marten van Dijk","hidden":false}],"publishedAt":"2026-01-19T10:46:51.000Z","submittedOnDailyAt":"2026-01-21T06:00:41.684Z","title":"On the Evidentiary Limits of Membership Inference for Copyright Auditing","submittedOnDailyBy":{"_id":"65ff72e6b9bcfbb87523d680","avatarUrl":"/avatars/344c8e86017ebf8ac84794bea56915b3.svg","isPro":false,"fullname":"M. Bilgehan Ertan","user":"bilgehanertan","type":"user"},"summary":"As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.","upvotes":2,"discussionId":"69708c50a8be625b19c2b024","ai_summary":"Membership inference attacks fail to reliably detect copyrighted text usage in large language models when training data is paraphrased using structure-aware methods that preserve semantic content.","ai_keywords":["membership inference attacks","large language models","paraphrasing framework","Sparse Autoencoders","SAE-guided extraction","semantic content","downstream utility","adversarial copyright disputes","training data obfuscation"]},"publishedAt":"2026-01-19T05:46:51.000Z","title":"On the Evidentiary Limits of Membership Inference for Copyright Auditing","summary":"As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12937.png","numComments":1,"submittedBy":{"_id":"65ff72e6b9bcfbb87523d680","avatarUrl":"/avatars/344c8e86017ebf8ac84794bea56915b3.svg","fullname":"M. Bilgehan Ertan","name":"bilgehanertan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.10237","authors":[{"_id":"696cc7fc3f1837bfb8970833","user":{"_id":"65ff72e6b9bcfbb87523d680","avatarUrl":"/avatars/344c8e86017ebf8ac84794bea56915b3.svg","isPro":false,"fullname":"M. Bilgehan Ertan","user":"bilgehanertan","type":"user"},"name":"Murat Bilgehan Ertan","status":"claimed_verified","statusLastChangedAt":"2026-01-19T09:22:34.659Z","hidden":false},{"_id":"696cc7fc3f1837bfb8970834","name":"Marten van Dijk","hidden":false}],"publishedAt":"2026-01-15T09:50:36.000Z","submittedOnDailyAt":"2026-01-21T06:54:08.795Z","title":"Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD","submittedOnDailyBy":{"_id":"65ff72e6b9bcfbb87523d680","avatarUrl":"/avatars/344c8e86017ebf8ac84794bea56915b3.svg","isPro":false,"fullname":"M. Bilgehan Ertan","user":"bilgehanertan","type":"user"},"summary":"Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the f-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with M gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation  which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small . However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier , which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy\n  ge 1{2ln M} quadorquad ge 1{8}!left(1-1{4ln M}right),\n  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as M to infty, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.","upvotes":2,"discussionId":"696cc7fd3f1837bfb8970835","ai_summary":"Differentially private stochastic gradient descent with shuffled sampling faces fundamental privacy-utility trade-offs that require substantial noise for meaningful privacy protection, limiting practical performance.","ai_keywords":["stochastic gradient descent","differential privacy","f-differential privacy","shuffled sampling","Gaussian noise multiplier","hypothesis-testing trade-off curves","privacy separation","adversarial advantage","Poisson subsampling"],"organization":{"_id":"696cc9c7dfb968a33456ac2c","name":"cwiamsterdam","fullname":"Centrum Wiskunde & Informatica","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65ff72e6b9bcfbb87523d680/Pm-ZGGrF0v8M7hwc5p-lc.png"}},"publishedAt":"2026-01-15T04:50:36.000Z","title":"Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD","summary":"Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the f-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with M gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation  which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small . However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier , which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy\n  ge 1{2ln M} quadorquad ge 1{8}!left(1-1{4ln M}right),\n  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as M to infty, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10237.png","numComments":1,"submittedBy":{"_id":"65ff72e6b9bcfbb87523d680","avatarUrl":"/avatars/344c8e86017ebf8ac84794bea56915b3.svg","fullname":"M. Bilgehan Ertan","name":"bilgehanertan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"696cc9c7dfb968a33456ac2c","name":"cwiamsterdam","fullname":"Centrum Wiskunde & Informatica","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65ff72e6b9bcfbb87523d680/Pm-ZGGrF0v8M7hwc5p-lc.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.13591","authors":[{"_id":"6970fef3c1c7409747bf941b","name":"Maojun Sun","hidden":false},{"_id":"6970fef3c1c7409747bf941c","name":"Yifei Xie","hidden":false},{"_id":"6970fef3c1c7409747bf941d","name":"Yue Wu","hidden":false},{"_id":"6970fef3c1c7409747bf941e","name":"Ruijian Han","hidden":false},{"_id":"6970fef3c1c7409747bf941f","name":"Binyan Jiang","hidden":false},{"_id":"6970fef3c1c7409747bf9420","name":"Defeng Sun","hidden":false},{"_id":"6970fef3c1c7409747bf9421","name":"Yancheng Yuan","hidden":false},{"_id":"6970fef3c1c7409747bf9422","name":"Jian Huang","hidden":false}],"publishedAt":"2026-01-20T04:44:36.000Z","submittedOnDailyAt":"2026-01-21T14:03:14.232Z","title":"DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems","submittedOnDailyBy":{"_id":"64c0e071e9263c783d548178","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64c0e071e9263c783d548178/VFwxjv7HFPwIoKIVGWLDg.png","isPro":false,"fullname":"Maojun SUN","user":"Stephen-smj","type":"user"},"summary":"Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.","upvotes":1,"discussionId":"6970fef3c1c7409747bf9423","ai_summary":"A comprehensive benchmark for evaluating LLM-based data agents across diverse data science tasks demonstrates superior performance for multimodal agents while highlighting persistent challenges in unstructured data domains.","ai_keywords":["data science agents","multimodal environment perception","multi-query interactions","multi-dimensional evaluation","agentic LLMs","vision-related tasks","structured data","unstructured data"],"organization":{"_id":"646ecc368d316fde87b3b6e3","name":"PolyUHK","fullname":"The Hong Kong Polytechnic University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"}},"publishedAt":"2026-01-19T23:44:36.000Z","title":"DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems","summary":"Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13591.png","numComments":1,"submittedBy":{"_id":"64c0e071e9263c783d548178","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64c0e071e9263c783d548178/VFwxjv7HFPwIoKIVGWLDg.png","fullname":"Maojun SUN","name":"Stephen-smj","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"646ecc368d316fde87b3b6e3","name":"PolyUHK","fullname":"The Hong Kong Polytechnic University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.13253","authors":[{"_id":"6970725ba8be625b19c2afc9","user":{"_id":"669a4ffa2dbf53ccd2fa585b","avatarUrl":"/avatars/dc4149f72466abdde598d6fde12a5ac3.svg","isPro":false,"fullname":"Ebubekir Tosun","user":"etosun","type":"user"},"name":"Ebubekir Tosun","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:18:58.918Z","hidden":false},{"_id":"6970725ba8be625b19c2afca","name":"Mehmet Emin Buldur","hidden":false},{"_id":"6970725ba8be625b19c2afcb","name":"zay Ezerceli","hidden":false},{"_id":"6970725ba8be625b19c2afcc","user":{"_id":"6422eab8e2029ade06eeee2c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png","isPro":false,"fullname":"Mahmud ElHuseyni ","user":"MElHuseyni","type":"user"},"name":"Mahmoud ElHussieni","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:01.567Z","hidden":false}],"publishedAt":"2026-01-19T17:38:52.000Z","submittedOnDailyAt":"2026-01-21T04:01:25.570Z","title":"A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus","submittedOnDailyBy":{"_id":"6422eab8e2029ade06eeee2c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png","isPro":false,"fullname":"Mahmud ElHuseyni ","user":"MElHuseyni","type":"user"},"summary":"We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.","upvotes":1,"discussionId":"6970725ca8be625b19c2afcd","ai_summary":"A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.","ai_keywords":["FastText embeddings","Agglomerative Clustering","semantic relationship classification","semantic clusters","downstream tasks","top-1 retrieval accuracy","F1-macro"]},"publishedAt":"2026-01-19T12:38:52.000Z","title":"A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus","summary":"We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13253.png","numComments":1,"submittedBy":{"_id":"6422eab8e2029ade06eeee2c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png","fullname":"Mahmud ElHuseyni ","name":"MElHuseyni","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":25,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.13251","authors":[{"_id":"69707202a8be625b19c2afc3","user":{"_id":"669a4ffa2dbf53ccd2fa585b","avatarUrl":"/avatars/dc4149f72466abdde598d6fde12a5ac3.svg","isPro":false,"fullname":"Ebubekir Tosun","user":"etosun","type":"user"},"name":"Ebubekir Tosun","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:06.502Z","hidden":false},{"_id":"69707202a8be625b19c2afc4","name":"Mehmet Emin Buldur","hidden":false},{"_id":"69707202a8be625b19c2afc5","name":"zay Ezerceli","hidden":false},{"_id":"69707202a8be625b19c2afc6","user":{"_id":"6422eab8e2029ade06eeee2c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png","isPro":false,"fullname":"Mahmud ElHuseyni ","user":"MElHuseyni","type":"user"},"name":"Mahmoud ElHussieni","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:19:11.047Z","hidden":false}],"publishedAt":"2026-01-19T17:37:25.000Z","submittedOnDailyAt":"2026-01-21T04:00:39.097Z","title":"Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph","submittedOnDailyBy":{"_id":"6422eab8e2029ade06eeee2c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png","isPro":false,"fullname":"Mahmud ElHuseyni ","user":"MElHuseyni","type":"user"},"summary":"Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.","upvotes":1,"discussionId":"69707203a8be625b19c2afc7","ai_summary":"A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.","ai_keywords":["semantic clustering","neural embeddings","synonymy","antonymy","co-hyponymy","semantic relation discriminator","macro-F1","semantic drift","polysemy","soft-to-hard clustering","topology-aware two-stage expansion-pruning","topological voting"]},"publishedAt":"2026-01-19T12:37:25.000Z","title":"Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph","summary":"Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13251.png","numComments":1,"submittedBy":{"_id":"6422eab8e2029ade06eeee2c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png","fullname":"Mahmud ElHuseyni ","name":"MElHuseyni","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":25,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.13075","authors":[{"_id":"6970c9cb572de08d9c7ae8d0","name":"Abhinav Rajeev Kumar","hidden":false},{"_id":"6970c9cb572de08d9c7ae8d1","name":"Dhruv Trehan","hidden":false},{"_id":"6970c9cb572de08d9c7ae8d2","name":"Paras Chopra","hidden":false}],"publishedAt":"2026-01-19T14:10:35.000Z","submittedOnDailyAt":"2026-01-21T10:13:14.559Z","title":"METIS: Mentoring Engine for Thoughtful Inquiry & Solutions","submittedOnDailyBy":{"_id":"69020c76ec2616129dea30b4","avatarUrl":"/avatars/44d59b5d951a4303def15ea1c6e3387e.svg","isPro":false,"fullname":"Paras Chopra","user":"paraslossfunk","type":"user"},"summary":"Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.","upvotes":1,"discussionId":"6970c9cb572de08d9c7ae8d3","ai_summary":"AI mentor METIS outperforms GPT-5 and Claude Sonnet 4.5 in supporting undergraduate research writing across multiple stages, with higher student scores and improved document-grounded outputs, though challenges remain in tool routing and stage classification.","ai_keywords":["LLM-as-a-judge","pairwise preferences","student-persona rubrics","multi-turn tutoring","evidence/compliance checks","stage-aware routing","groundings failure modes","document-grounded stages"],"organization":{"_id":"67a1298ea7d77f4454f936a2","name":"Lossfunk","fullname":"Lossfunk","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"}},"publishedAt":"2026-01-19T09:10:35.000Z","title":"METIS: Mentoring Engine for Thoughtful Inquiry & Solutions","summary":"Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13075.png","numComments":1,"submittedBy":{"_id":"69020c76ec2616129dea30b4","avatarUrl":"/avatars/44d59b5d951a4303def15ea1c6e3387e.svg","fullname":"Paras Chopra","name":"paraslossfunk","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67a1298ea7d77f4454f936a2","name":"Lossfunk","fullname":"Lossfunk","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.12910","authors":[{"_id":"69707c12a8be625b19c2b009","user":{"_id":"6113da54d08630d2676c9823","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1663060375254-6113da54d08630d2676c9823.png","isPro":false,"fullname":"Tim","user":"timbmg","type":"user"},"name":"Tim Baumgrtner","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:18:54.768Z","hidden":false},{"_id":"69707c12a8be625b19c2b00a","name":"Iryna Gurevych","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6113da54d08630d2676c9823/L8GSTXOHoPM78LHoD2xQH.png"],"publishedAt":"2026-01-19T10:04:33.000Z","submittedOnDailyAt":"2026-01-21T04:56:18.096Z","title":"SciCoQA: Quality Assurance for Scientific Paper--Code Alignment","submittedOnDailyBy":{"_id":"6113da54d08630d2676c9823","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1663060375254-6113da54d08630d2676c9823.png","isPro":false,"fullname":"Tim","user":"timbmg","type":"user"},"summary":"We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\\% of real-world paper-code discrepancies.","upvotes":1,"discussionId":"69707c13a8be625b19c2b00b","projectPage":"https://ukplab.github.io/scicoqa","githubRepo":"https://github.com/ukplab/scicoqa","githubRepoAddedBy":"user","ai_summary":"SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.","ai_keywords":["dataset","synthetic data generation","paper-code discrepancies","computational science","language models","reproducibility"],"githubStars":3,"organization":{"_id":"62de69518960b17bb39a263c","name":"UKPLab","fullname":"Ubiquitous Knowledge Processing Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1658743016913-62de689d86220b5cb895acea.png"}},"publishedAt":"2026-01-19T05:04:33.000Z","title":"SciCoQA: Quality Assurance for Scientific Paper--Code Alignment","summary":"We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\\% of real-world paper-code discrepancies.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6113da54d08630d2676c9823/L8GSTXOHoPM78LHoD2xQH.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12910.png","numComments":1,"submittedBy":{"_id":"6113da54d08630d2676c9823","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1663060375254-6113da54d08630d2676c9823.png","fullname":"Tim","name":"timbmg","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"62de69518960b17bb39a263c","name":"UKPLab","fullname":"Ubiquitous Knowledge Processing Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1658743016913-62de689d86220b5cb895acea.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.10700","authors":[{"_id":"696f567b8c91aa061f07eb96","name":"Gilat Toker","hidden":false},{"_id":"696f567b8c91aa061f07eb97","name":"Nitay Calderon","hidden":false},{"_id":"696f567b8c91aa061f07eb98","name":"Ohad Amosy","hidden":false},{"_id":"696f567b8c91aa061f07eb99","name":"Roi Reichart","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/Cdme_ktpHIXToLXRnBxrz.jpeg"],"publishedAt":"2026-01-15T18:54:50.000Z","submittedOnDailyAt":"2026-01-21T03:26:35.803Z","title":"LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals","submittedOnDailyBy":{"_id":"62d6a0c18faee0ac953c51fa","avatarUrl":"/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg","isPro":false,"fullname":"Nitay Calderon","user":"nitay","type":"user"},"summary":"Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.","upvotes":1,"discussionId":"696f567b8c91aa061f07eb9a","githubRepo":"https://github.com/GilatToker/Liberty-benchmark","githubRepoAddedBy":"user","ai_summary":"A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.","ai_keywords":["concept-based explanations","structural causal models","counterfactuals","LLM-based intervention","interventional benchmark","reference targets","order-faithfulness","demographic concepts","post-training mitigation"],"githubStars":2,"organization":{"_id":"6393322be2364bc1eea56e45","name":"Technion","fullname":"Technion Israel institute of technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}},"publishedAt":"2026-01-15T13:54:50.000Z","title":"LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals","summary":"Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/Cdme_ktpHIXToLXRnBxrz.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10700.png","numComments":1,"submittedBy":{"_id":"62d6a0c18faee0ac953c51fa","avatarUrl":"/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg","fullname":"Nitay Calderon","name":"nitay","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"6393322be2364bc1eea56e45","name":"Technion","fullname":"Technion Israel institute of technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.13677","authors":[{"_id":"6970d8c2572de08d9c7ae8fa","name":"Carsten T. Lth","hidden":false},{"_id":"6970d8c2572de08d9c7ae8fb","name":"Jeremias Traub","hidden":false},{"_id":"6970d8c2572de08d9c7ae8fc","name":"Kim-Celine Kahl","hidden":false},{"_id":"6970d8c2572de08d9c7ae8fd","name":"Till J. Bungert","hidden":false},{"_id":"6970d8c2572de08d9c7ae8fe","name":"Lukas Klein","hidden":false},{"_id":"6970d8c2572de08d9c7ae8ff","name":"Lars Krmer","hidden":false},{"_id":"6970d8c2572de08d9c7ae900","name":"Paul F. Jger","hidden":false},{"_id":"6970d8c2572de08d9c7ae901","name":"Klaus Maier-Hein","hidden":false},{"_id":"6970d8c2572de08d9c7ae902","name":"Fabian Isensee","hidden":false}],"publishedAt":"2026-01-20T07:29:50.000Z","submittedOnDailyAt":"2026-01-21T11:22:28.401Z","title":"Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging","submittedOnDailyBy":{"_id":"63bff638a4dd7828ff534f9f","avatarUrl":"/avatars/7140ad394db7d71301bec12682748cf3.svg","isPro":false,"fullname":"Jeremias Traub","user":"JeremiasTraub","type":"user"},"summary":"Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.","upvotes":0,"discussionId":"6970d8c2572de08d9c7ae903","githubRepo":"https://github.com/MIC-DKFZ/nnActive/tree/nnActive_v2","githubRepoAddedBy":"user","ai_summary":"Class-stratified Scheduled Power Predictive Entropy (ClaSP PE) is a novel active learning strategy that improves 3D biomedical image segmentation by addressing class imbalance and selection redundancy through stratified querying and power noising with decay scheduling.","ai_keywords":["active learning","3D biomedical image segmentation","class-stratified querying","predictive entropy","power noising","decaying schedule","nnActive benchmark","annotation efficiency","segmentation quality"],"githubStars":11,"organization":{"_id":"6819c17a86b01862017668af","name":"MIC-DKFZ","fullname":"MIC at DKFZ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d0461f48276ed25e08047b/G6otR2N9xql06E3EJyOd4.png"}},"publishedAt":"2026-01-20T02:29:50.000Z","title":"Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging","summary":"Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13677.png","numComments":1,"submittedBy":{"_id":"63bff638a4dd7828ff534f9f","avatarUrl":"/avatars/7140ad394db7d71301bec12682748cf3.svg","fullname":"Jeremias Traub","name":"JeremiasTraub","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6819c17a86b01862017668af","name":"MIC-DKFZ","fullname":"MIC at DKFZ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d0461f48276ed25e08047b/G6otR2N9xql06E3EJyOd4.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.11898","authors":[{"_id":"697109acc1c7409747bf942d","name":"Yilmaz Korkmaz","hidden":false},{"_id":"697109acc1c7409747bf942e","name":"Vishal M. Patel","hidden":false}],"publishedAt":"2026-01-17T03:50:00.000Z","submittedOnDailyAt":"2026-01-21T14:46:22.207Z","title":"RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection","submittedOnDailyBy":{"_id":"66ad888bf802f266d8ef7c9f","avatarUrl":"/avatars/6aa33f00aa91520111e21670d6e0080f.svg","isPro":false,"fullname":"Yilmaz Korkmaz","user":"yilmazkorkmaz","type":"user"},"summary":"Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available https://github.com/yilmazkorkmaz1/RemoteVAR{here}.","upvotes":0,"discussionId":"697109acc1c7409747bf942f","githubRepo":"https://github.com/yilmazkorkmaz1/RemoteVAR","githubRepoAddedBy":"user","ai_summary":"RemoteVAR is a visual autoregressive framework for remote sensing change detection that improves upon existing methods through multi-resolution feature fusion and autoregressive training tailored for change map prediction.","ai_keywords":["visual autoregressive models","change detection","bi-temporal features","cross-attention","autoregressive training","dense prediction","exposure bias"],"githubStars":2,"organization":{"_id":"653945b47ba797097a7b4eab","name":"JohnsHopkins","fullname":"Johns Hopkins University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/653944e58e687a41625a4694/qqHzBOarppVrUuZbbjqwh.png"}},"publishedAt":"2026-01-16T22:50:00.000Z","title":"RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection","summary":"Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available https://github.com/yilmazkorkmaz1/RemoteVAR{here}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11898.png","numComments":1,"submittedBy":{"_id":"66ad888bf802f266d8ef7c9f","avatarUrl":"/avatars/6aa33f00aa91520111e21670d6e0080f.svg","fullname":"Yilmaz Korkmaz","name":"yilmazkorkmaz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"653945b47ba797097a7b4eab","name":"JohnsHopkins","fullname":"Johns Hopkins University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/653944e58e687a41625a4694/qqHzBOarppVrUuZbbjqwh.png"},"isAuthorParticipating":false}]
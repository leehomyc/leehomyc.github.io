[{"paper":{"id":"2601.08521","authors":[{"_id":"69674059c5e371f6b235d1d8","user":{"_id":"68920f91bcf2b25e8e121cf6","avatarUrl":"/avatars/4bc69f43828a346a3ee24b026e0edbb4.svg","isPro":false,"fullname":"Fengkai Yang","user":"ShortCatisLong","type":"user"},"name":"Fengkai Yang","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:33:21.899Z","hidden":false},{"_id":"69674059c5e371f6b235d1d9","user":{"_id":"6969715fb2636f5f23a9a8c5","avatarUrl":"/avatars/5e75043891ee41bb980f71fb9e3a33ab.svg","isPro":false,"fullname":"Zherui Chen","user":"chenzherui007","type":"user"},"name":"Zherui Chen","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:33:53.078Z","hidden":false},{"_id":"69674059c5e371f6b235d1da","name":"Xiaohan Wang","hidden":false},{"_id":"69674059c5e371f6b235d1db","name":"Xiaodong Lu","hidden":false},{"_id":"69674059c5e371f6b235d1dc","user":{"_id":"666eb642a119281ee0bfa443","avatarUrl":"/avatars/71317810b00978754ad439837b04faff.svg","isPro":false,"fullname":"Jiajun Chai","user":"PandaChai","type":"user"},"name":"Jiajun Chai","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:37:17.404Z","hidden":false},{"_id":"69674059c5e371f6b235d1dd","name":"Guojun Yin","hidden":false},{"_id":"69674059c5e371f6b235d1de","name":"Wei Lin","hidden":false},{"_id":"69674059c5e371f6b235d1df","name":"Shuai Ma","hidden":false},{"_id":"69674059c5e371f6b235d1e0","name":"Fuzhen Zhuang","hidden":false},{"_id":"69674059c5e371f6b235d1e1","name":"Deqing Wang","hidden":false},{"_id":"69674059c5e371f6b235d1e2","name":"Yaodong Yang","hidden":false},{"_id":"69674059c5e371f6b235d1e3","name":"Jianxin Li","hidden":false},{"_id":"69674059c5e371f6b235d1e4","user":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","isPro":false,"fullname":"Yikun Ban","user":"Yikunb","type":"user"},"name":"Yikun Ban","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:33:50.655Z","hidden":false}],"publishedAt":"2026-01-13T13:03:15.000Z","submittedOnDailyAt":"2026-01-19T00:20:58.837Z","title":"Your Group-Relative Advantage Is Biased","submittedOnDailyBy":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","isPro":false,"fullname":"Yikun Ban","user":"Yikunb","type":"user"},"summary":"Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.","upvotes":95,"discussionId":"6967405ac5e371f6b235d1e5","ai_summary":"Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method that improves performance on mathematical reasoning benchmarks.","ai_keywords":["Reinforcement Learning from Verifier Rewards","group-based methods","GRPO","advantage estimation","bias correction","adaptive reweighting","difficulty weighting","mathematical reasoning","benchmark evaluation"]},"publishedAt":"2026-01-13T08:03:15.000Z","title":"Your Group-Relative Advantage Is Biased","summary":"Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08521.png","numComments":5,"submittedBy":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","fullname":"Yikun Ban","name":"Yikunb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.11496","authors":[{"_id":"696de89f3f1837bfb8970ab3","user":{"_id":"64802fb6c57f629056c59966","avatarUrl":"/avatars/d5ecabaceeba759969855acf512b6649.svg","isPro":false,"fullname":"Eilam Shapira","user":"EilamSha","type":"user"},"name":"Eilam Shapira","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:37:41.780Z","hidden":false},{"_id":"696de89f3f1837bfb8970ab4","name":"Roi Reichart","hidden":false},{"_id":"696de89f3f1837bfb8970ab5","name":"Moshe Tennenholtz","hidden":false}],"publishedAt":"2026-01-16T18:18:03.000Z","submittedOnDailyAt":"2026-01-19T06:58:50.740Z","title":"The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents","submittedOnDailyBy":{"_id":"64802fb6c57f629056c59966","avatarUrl":"/avatars/d5ecabaceeba759969855acf512b6649.svg","isPro":false,"fullname":"Eilam Shapira","user":"EilamSha","type":"user"},"summary":"The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.","upvotes":39,"discussionId":"696de8a03f1837bfb8970ab6","organization":{"_id":"6393322be2364bc1eea56e45","name":"Technion","fullname":"Technion Israel institute of technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}},"publishedAt":"2026-01-16T13:18:03.000Z","title":"The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents","summary":"The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11496.png","numComments":2,"submittedBy":{"_id":"64802fb6c57f629056c59966","avatarUrl":"/avatars/d5ecabaceeba759969855acf512b6649.svg","fullname":"Eilam Shapira","name":"EilamSha","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6393322be2364bc1eea56e45","name":"Technion","fullname":"Technion Israel institute of technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.10355","authors":[{"_id":"6969a11632f0333869ff9390","user":{"_id":"65647e2b50a80d26dbfdf49c","avatarUrl":"/avatars/aff0de9f9e4ed322e05d7f832c3c060d.svg","isPro":false,"fullname":"Xu Zhihao","user":"naiweizi","type":"user"},"name":"Zhihao Xu","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:30:47.984Z","hidden":false},{"_id":"6969a11632f0333869ff9391","name":"Rumei Li","hidden":false},{"_id":"6969a11632f0333869ff9392","name":"Jiahuan Li","hidden":false},{"_id":"6969a11632f0333869ff9393","user":{"_id":"601faf6053442c822abcad19","avatarUrl":"/avatars/47889d2beea63fbaf46c203d00a33494.svg","isPro":false,"fullname":"Rongxiang Weng","user":"wengrx","type":"user"},"name":"Rongxiang Weng","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:38:27.977Z","hidden":false},{"_id":"6969a11632f0333869ff9394","user":{"_id":"647097cbcfd57849518e656b","avatarUrl":"/avatars/c66fe0add29c1bde9e3a98bf4a8793b9.svg","isPro":false,"fullname":"Jingang Wang","user":"bitwjg","type":"user"},"name":"Jingang Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:38:22.544Z","hidden":false},{"_id":"6969a11632f0333869ff9395","name":"Xunliang Cai","hidden":false},{"_id":"6969a11632f0333869ff9396","user":{"_id":"640e962f3830fd441c2e250c","avatarUrl":"/avatars/f88fbe06925064180b1867787b6d9a4d.svg","isPro":false,"fullname":"Wang","user":"Xiting","type":"user"},"name":"Xiting Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:38:07.595Z","hidden":false}],"publishedAt":"2026-01-15T12:58:46.000Z","submittedOnDailyAt":"2026-01-19T00:30:06.659Z","title":"Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text","submittedOnDailyBy":{"_id":"65647e2b50a80d26dbfdf49c","avatarUrl":"/avatars/aff0de9f9e4ed322e05d7f832c3c060d.svg","isPro":false,"fullname":"Xu Zhihao","user":"naiweizi","type":"user"},"summary":"Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.","upvotes":30,"discussionId":"6969a11632f0333869ff9397","ai_summary":"A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.","ai_keywords":["large language models","multi-turn interactions","tool-use data","text corpora","data synthesis pipeline","relevance filtering","workflow extraction","trajectory grounding","complexity refinement","supervised fine-tuning","trajectory synthesizer","BFCL V3 Multi-turn benchmark","τ-bench","inference latency"],"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}},"publishedAt":"2026-01-15T07:58:46.000Z","title":"Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text","summary":"Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10355.png","numComments":2,"submittedBy":{"_id":"65647e2b50a80d26dbfdf49c","avatarUrl":"/avatars/aff0de9f9e4ed322e05d7f832c3c060d.svg","fullname":"Xu Zhihao","name":"naiweizi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.08430","authors":[{"_id":"696b31133f1837bfb8970653","name":"Sunzhu Li","hidden":false},{"_id":"696b31133f1837bfb8970654","name":"Jiale Zhao","hidden":false},{"_id":"696b31133f1837bfb8970655","name":"Miteto Wei","hidden":false},{"_id":"696b31133f1837bfb8970656","user":{"_id":"65c0327d52edc43028968b74","avatarUrl":"/avatars/a0f0a4ef5a808d5b1e4226debafa0061.svg","isPro":false,"fullname":"Huimin Ren","user":"renhuimin","type":"user"},"name":"Huimin Ren","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:38:54.568Z","hidden":false},{"_id":"696b31133f1837bfb8970657","name":"Yang Zhou","hidden":false},{"_id":"696b31133f1837bfb8970658","name":"Jingwen Yang","hidden":false},{"_id":"696b31133f1837bfb8970659","user":{"_id":"6713afea187a20dc579e121b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg","isPro":false,"fullname":"Shunyu Liu","user":"liushunyu","type":"user"},"name":"Shunyu Liu","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:39:28.083Z","hidden":false},{"_id":"696b31133f1837bfb897065a","user":{"_id":"6625fa89f8639638f9226978","avatarUrl":"/avatars/df61c93add66d59d05a33e41f552ff2d.svg","isPro":false,"fullname":"Kaike Zhang","user":"kaikezhang","type":"user"},"name":"Kaike Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:38:44.338Z","hidden":false},{"_id":"696b31133f1837bfb897065b","name":"Wei Chen","hidden":false}],"publishedAt":"2026-01-13T10:56:39.000Z","submittedOnDailyAt":"2026-01-19T00:15:22.448Z","title":"RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation","submittedOnDailyBy":{"_id":"67375a6ae6b1d15ff5359a54","avatarUrl":"/avatars/9d32d9e3bfb43b8d001c6ddeae720ec5.svg","isPro":false,"fullname":"Zela","user":"vzl123","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.","upvotes":25,"discussionId":"696b31143f1837bfb897065c","projectPage":"https://huggingface.co/datasets/sojuL/RubricHub_v1","githubRepo":"https://github.com/teqkilla/RubricHub","githubRepoAddedBy":"user","ai_summary":"RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","rubric-based evaluation","principle-guided synthesis","multi-model aggregation","difficulty evolution","RubricHub","Rubric-based Rejection Sampling Fine-Tuning","Reinforcement Learning"],"githubStars":25},"publishedAt":"2026-01-13T05:56:39.000Z","title":"RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08430.png","numComments":2,"submittedBy":{"_id":"67375a6ae6b1d15ff5359a54","avatarUrl":"/avatars/9d32d9e3bfb43b8d001c6ddeae720ec5.svg","fullname":"Zela","name":"vzl123","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.11000","authors":[{"_id":"696dc5773f1837bfb8970a3b","user":{"_id":"6309bfdab8d7b3889319b588","avatarUrl":"/avatars/572acdad470f765ef2e058ead3741e24.svg","isPro":false,"fullname":"SunZX","user":"Jeryi","type":"user"},"name":"Zhongxiang Sun","status":"claimed_verified","statusLastChangedAt":"2026-01-19T09:22:05.638Z","hidden":false},{"_id":"696dc5773f1837bfb8970a3c","name":"Yi Zhan","hidden":false},{"_id":"696dc5773f1837bfb8970a3d","user":{"_id":"64b15e4fa15d33a1bc6c2fd5","avatarUrl":"/avatars/27bbd2e335c1615bb97a77bace227641.svg","isPro":false,"fullname":"chenglei shen","user":"starrylay","type":"user"},"name":"Chenglei Shen","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:39:47.954Z","hidden":false},{"_id":"696dc5773f1837bfb8970a3e","user":{"_id":"67f22419903c812238ecccf9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/E8RQo_VGJsFvJ_ufq5UI6.png","isPro":false,"fullname":"Weijie Yu","user":"Shikicon","type":"user"},"name":"Weijie Yu","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:39:42.356Z","hidden":false},{"_id":"696dc5773f1837bfb8970a3f","name":"Xiao Zhang","hidden":false},{"_id":"696dc5773f1837bfb8970a40","name":"Ming He","hidden":false},{"_id":"696dc5773f1837bfb8970a41","name":"Jun Xu","hidden":false}],"publishedAt":"2026-01-16T05:20:10.000Z","submittedOnDailyAt":"2026-01-19T04:22:37.620Z","title":"When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs","submittedOnDailyBy":{"_id":"6309bfdab8d7b3889319b588","avatarUrl":"/avatars/572acdad470f765ef2e058ead3741e24.svg","isPro":false,"fullname":"SunZX","user":"Jeryi","type":"user"},"summary":"Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.","upvotes":21,"discussionId":"696dc5773f1837bfb8970a42","ai_summary":"Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.","ai_keywords":["personalized large language models","factual reasoning","personalization-induced hallucinations","representational entanglement","Factuality-Preserving Personalized Steering","PFQABench","inference-time approach","factual accuracy","personalized performance"],"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}},"publishedAt":"2026-01-16T00:20:10.000Z","title":"When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs","summary":"Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11000.png","numComments":3,"submittedBy":{"_id":"6309bfdab8d7b3889319b588","avatarUrl":"/avatars/572acdad470f765ef2e058ead3741e24.svg","fullname":"SunZX","name":"Jeryi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.11404","authors":[{"_id":"696d92733f1837bfb897095c","user":{"_id":"68ff818830c48bce91a89b2c","avatarUrl":"/avatars/1bdabe82a217a737316df34e3ba14537.svg","isPro":false,"fullname":"Linqing Zhong","user":"Linqing94482664","type":"user"},"name":"Linqing Zhong","status":"claimed_verified","statusLastChangedAt":"2026-01-19T09:22:25.917Z","hidden":false},{"_id":"696d92733f1837bfb897095d","name":"Yi Liu","hidden":false},{"_id":"696d92733f1837bfb897095e","name":"Yifei Wei","hidden":false},{"_id":"696d92733f1837bfb897095f","user":{"_id":"64d107229617774ce41b9467","avatarUrl":"/avatars/10d4dff031f2d8dd7047d7ea5ec0d4c1.svg","isPro":false,"fullname":"Ziyu Xiong","user":"JennyZiyu","type":"user"},"name":"Ziyu Xiong","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:40:12.039Z","hidden":false},{"_id":"696d92733f1837bfb8970960","name":"Maoqing Yao","hidden":false},{"_id":"696d92733f1837bfb8970961","name":"Si Liu","hidden":false},{"_id":"696d92733f1837bfb8970962","user":{"_id":"646ec9b135f55eb49e405faa","avatarUrl":"/avatars/a17194be585d20e2a021e77a5a20e213.svg","isPro":false,"fullname":"Guanghui Ren","user":"sundrops","type":"user"},"name":"Guanghui Ren","status":"claimed_verified","statusLastChangedAt":"2026-01-19T09:22:18.181Z","hidden":false}],"publishedAt":"2026-01-16T16:17:06.000Z","submittedOnDailyAt":"2026-01-19T05:00:58.489Z","title":"ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models","submittedOnDailyBy":{"_id":"646ec9b135f55eb49e405faa","avatarUrl":"/avatars/a17194be585d20e2a021e77a5a20e213.svg","isPro":false,"fullname":"Guanghui Ren","user":"sundrops","type":"user"},"summary":"Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.","upvotes":18,"discussionId":"696d92743f1837bfb8970963","ai_summary":"Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.","ai_keywords":["Vision-Language-Model","action space","action chain-of-thought","ACoT","Explicit Action Reasoner","Implicit Action Reasoner","coarse action intents","multimodal input","policy learning"],"organization":{"_id":"676fc7c31c48eff17fac3135","name":"agibot-world","fullname":"AgiBot World","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"}},"publishedAt":"2026-01-16T11:17:06.000Z","title":"ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models","summary":"Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11404.png","numComments":2,"submittedBy":{"_id":"646ec9b135f55eb49e405faa","avatarUrl":"/avatars/a17194be585d20e2a021e77a5a20e213.svg","fullname":"Guanghui Ren","name":"sundrops","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"676fc7c31c48eff17fac3135","name":"agibot-world","fullname":"AgiBot World","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.11037","authors":[{"_id":"696d98a63f1837bfb8970965","user":{"_id":"659c5bd10a456bafa88d2414","avatarUrl":"/avatars/0e63ab07bf33c2cbd373829083a7df25.svg","isPro":false,"fullname":"Shiyu Liu","user":"ShiyuLiu","type":"user"},"name":"Shiyu Liu","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:40:25.580Z","hidden":false},{"_id":"696d98a63f1837bfb8970966","user":{"_id":"64dc5ac0c1209f7d49b0a10f","avatarUrl":"/avatars/ad18a5a335758dfbe8ce671aa92eef69.svg","isPro":false,"fullname":"Yongjing Yin","user":"yongjing","type":"user"},"name":"Yongjing Yin","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:40:30.872Z","hidden":false},{"_id":"696d98a63f1837bfb8970967","user":{"_id":"6086838b19137b3a6ba760e7","avatarUrl":"/avatars/d63eea3e39b22c6e65b82c28192696f1.svg","isPro":false,"fullname":"Jianhao Yan","user":"Elliott","type":"user"},"name":"Jianhao Yan","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:40:36.348Z","hidden":false},{"_id":"696d98a63f1837bfb8970968","name":"Yunbo Tang","hidden":false},{"_id":"696d98a63f1837bfb8970969","name":"Qinggang Zhang","hidden":false},{"_id":"696d98a63f1837bfb897096a","name":"Bei Li","hidden":false},{"_id":"696d98a63f1837bfb897096b","name":"Xin Chen","hidden":false},{"_id":"696d98a63f1837bfb897096c","user":{"_id":"647097cbcfd57849518e656b","avatarUrl":"/avatars/c66fe0add29c1bde9e3a98bf4a8793b9.svg","isPro":false,"fullname":"Jingang Wang","user":"bitwjg","type":"user"},"name":"Jingang Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:41:01.323Z","hidden":false},{"_id":"696d98a63f1837bfb897096d","name":"Xunliang Cai","hidden":false},{"_id":"696d98a63f1837bfb897096e","name":"Jinsong Su","hidden":false}],"publishedAt":"2026-01-16T07:06:58.000Z","submittedOnDailyAt":"2026-01-19T00:06:37.086Z","title":"BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.","upvotes":12,"discussionId":"696d98a63f1837bfb897096f","githubRepo":"https://github.com/Liushiyu-0709/BAPO-Reliable-Search","githubRepoAddedBy":"user","ai_summary":"Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.","ai_keywords":["reinforcement learning","agentic search","large language models","boundary-aware policy optimization","group-based boundary-aware reward","adaptive reward modulator","I DON'T KNOW response","reasoning limits","reliability enhancement"],"githubStars":16},"publishedAt":"2026-01-16T02:06:58.000Z","title":"BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search","summary":"RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11037.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":210,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.10909","authors":[{"_id":"696d9afc3f1837bfb897099b","user":{"_id":"673c86d3cfd6e90464a294e2","avatarUrl":"/avatars/ed778a181442e4a095b8cc02bb4bd169.svg","isPro":false,"fullname":"Chuqiao Li","user":"coralli","type":"user"},"name":"Chuqiao Li","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:42:30.132Z","hidden":false},{"_id":"696d9afc3f1837bfb897099c","user":{"_id":"6966b9cff869182443fb3fde","avatarUrl":"/avatars/925b80085ded250c6bd8d412f2f4b636.svg","isPro":false,"fullname":"Xianghui Xie","user":"xianghuix","type":"user"},"name":"Xianghui Xie","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:42:16.683Z","hidden":false},{"_id":"696d9afc3f1837bfb897099d","user":{"_id":"6170f862544a1c62b298b891","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1634793843654-6170f862544a1c62b298b891.jpeg","isPro":false,"fullname":"Yong Cao","user":"Yongcao","type":"user"},"name":"Yong Cao","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:42:10.525Z","hidden":false},{"_id":"696d9afc3f1837bfb897099e","user":{"_id":"620cae049086f3c07f01e3d5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1644998139538-noauth.jpeg","isPro":false,"fullname":"Andreas Geiger","user":"andreas-geiger","type":"user"},"name":"Andreas Geiger","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:42:04.135Z","hidden":false},{"_id":"696d9afc3f1837bfb897099f","name":"Gerard Pons-Moll","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/CVN5Pk94HIT--aBIwgWo9.mp4"],"publishedAt":"2026-01-15T23:50:07.000Z","submittedOnDailyAt":"2026-01-19T00:16:39.946Z","title":"FrankenMotion: Part-level Human Motion Generation and Composition","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.","upvotes":10,"discussionId":"696d9afc3f1837bfb89709a0","projectPage":"https://coral79.github.io/frankenmotion/","githubRepo":"https://github.com/Coral79/FrankenMotion-Code","githubRepoAddedBy":"user","ai_summary":"A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.","ai_keywords":["diffusion-based","part-aware motion generation","large language models","temporally-aware part-level text annotations","atomic motion annotations","temporally-structured textual prompts","motion generation"],"githubStars":37},"publishedAt":"2026-01-15T18:50:07.000Z","title":"FrankenMotion: Part-level Human Motion Generation and Composition","summary":"Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/CVN5Pk94HIT--aBIwgWo9.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10909.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":210,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.09001","authors":[{"_id":"696e27193f1837bfb8970b8e","user":{"_id":"68114fc5e90462b8c266c852","avatarUrl":"/avatars/821271e22faa05a1489c5e84c9c5e954.svg","isPro":false,"fullname":"Pedro Memoli Buffa","user":"PMemoli","type":"user"},"name":"Pedro Memoli Buffa","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:55:14.674Z","hidden":false},{"_id":"696e27193f1837bfb8970b8f","user":{"_id":"63b81aace60862785afd8ca2","avatarUrl":"/avatars/f96ce78af6ad42514235bab811544789.svg","isPro":false,"fullname":"Luciano Del Corro","user":"lucianodelcorro","type":"user"},"name":"Luciano Del Corro","status":"claimed_verified","statusLastChangedAt":"2026-01-19T12:58:24.541Z","hidden":false}],"publishedAt":"2026-01-13T21:54:38.000Z","submittedOnDailyAt":"2026-01-19T10:14:51.323Z","title":"Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM","submittedOnDailyBy":{"_id":"63b81aace60862785afd8ca2","avatarUrl":"/avatars/f96ce78af6ad42514235bab811544789.svg","isPro":false,"fullname":"Luciano Del Corro","user":"lucianodelcorro","type":"user"},"summary":"Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all \"10 choose k\" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.","upvotes":10,"discussionId":"696e271a3f1837bfb8970b90","ai_summary":"Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.","ai_keywords":["output-entropy profile","next-token probabilities","final-layer","top-k logprobs","instance correctness","domain-level accuracy estimate","STEM reasoning benchmarks","domain shift","data acquisition"],"organization":{"_id":"66d5cb037300d333daebedd9","name":"UdeSA","fullname":"Universidad de San Andrés","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6594888e754092f6b1443bbd/C-ZrIbZoTgAj9p-u0v5jZ.png"}},"publishedAt":"2026-01-13T16:54:38.000Z","title":"Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM","summary":"Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all \"10 choose k\" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09001.png","numComments":2,"submittedBy":{"_id":"63b81aace60862785afd8ca2","avatarUrl":"/avatars/f96ce78af6ad42514235bab811544789.svg","fullname":"Luciano Del Corro","name":"lucianodelcorro","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"66d5cb037300d333daebedd9","name":"UdeSA","fullname":"Universidad de San Andrés","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6594888e754092f6b1443bbd/C-ZrIbZoTgAj9p-u0v5jZ.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.09195","authors":[{"_id":"6969ca6832f0333869ff94c0","user":{"_id":"67be83a727ba0c1993f48c4f","avatarUrl":"/avatars/9b3638972535be4e7b197dc0edc41c2b.svg","isPro":false,"fullname":"Tao Liu","user":"utaotao","type":"user"},"name":"Tao Liu","status":"claimed_verified","statusLastChangedAt":"2026-01-19T14:54:40.157Z","hidden":false},{"_id":"6969ca6832f0333869ff94c1","user":{"_id":"6621cea88850e38ffbb1854f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg","isPro":false,"fullname":"Taki WU","user":"taki555","type":"user"},"name":"Taiqiang Wu","status":"claimed_verified","statusLastChangedAt":"2026-01-19T09:23:59.501Z","hidden":false},{"_id":"6969ca6832f0333869ff94c2","user":{"_id":"66441bbd6df04abec508648e","avatarUrl":"/avatars/dcbc33742318d357ab9d426d12efa89a.svg","isPro":false,"fullname":"Rummy","user":"yang31210999","type":"user"},"name":"Runming Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-19T14:33:54.695Z","hidden":false},{"_id":"6969ca6832f0333869ff94c3","user":{"_id":"656af95af7be0986b44e7eef","avatarUrl":"/avatars/8b0c25ddb1d248a2eed0928f3403d521.svg","isPro":false,"fullname":"Shaoning Sun","user":"shaoningsun","type":"user"},"name":"Shaoning Sun","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:41:33.930Z","hidden":false},{"_id":"6969ca6832f0333869ff94c4","name":"Junjie Wang","hidden":false},{"_id":"6969ca6832f0333869ff94c5","user":{"_id":"64ca1fe838837b12d5e529b7","avatarUrl":"/avatars/44a3ad9e59318784ac531993b5f69f6b.svg","isPro":false,"fullname":"Yujiu Yang","user":"Thu-redrobot","type":"user"},"name":"Yujiu Yang","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:41:43.285Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/pPOt4b6-MlXHcGoicirKU.png"],"publishedAt":"2026-01-14T05:50:40.000Z","submittedOnDailyAt":"2026-01-19T00:19:47.118Z","title":"ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection","submittedOnDailyBy":{"_id":"6621cea88850e38ffbb1854f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg","isPro":false,"fullname":"Taki WU","user":"taki555","type":"user"},"summary":"Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.","upvotes":9,"discussionId":"6969ca6932f0333869ff94c6","githubRepo":"https://github.com/Utaotao/ProFit","githubRepoAddedBy":"user","ai_summary":"Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.","ai_keywords":["supervised fine-tuning","Large Language Models","one-to-many nature","token probability","semantic importance","ProFit","surface-level overfitting"],"githubStars":18,"organization":{"_id":"66f55d53853f0506904d1922","name":"IIGroup","fullname":"Tsinghua IIGroup","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"}},"publishedAt":"2026-01-14T00:50:40.000Z","title":"ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection","summary":"Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6621cea88850e38ffbb1854f/pPOt4b6-MlXHcGoicirKU.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09195.png","numComments":5,"submittedBy":{"_id":"6621cea88850e38ffbb1854f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg","fullname":"Taki WU","name":"taki555","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"66f55d53853f0506904d1922","name":"IIGroup","fullname":"Tsinghua IIGroup","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.10781","authors":[{"_id":"696d9f043f1837bfb89709b6","name":"Kanchana Ranasinghe","hidden":false},{"_id":"696d9f043f1837bfb89709b7","name":"Honglu Zhou","hidden":false},{"_id":"696d9f043f1837bfb89709b8","name":"Yu Fang","hidden":false},{"_id":"696d9f043f1837bfb89709b9","name":"Luyu Yang","hidden":false},{"_id":"696d9f043f1837bfb89709ba","name":"Le Xue","hidden":false},{"_id":"696d9f043f1837bfb89709bb","name":"Ran Xu","hidden":false},{"_id":"696d9f043f1837bfb89709bc","name":"Caiming Xiong","hidden":false},{"_id":"696d9f043f1837bfb89709bd","name":"Silvio Savarese","hidden":false},{"_id":"696d9f043f1837bfb89709be","name":"Michael S Ryoo","hidden":false},{"_id":"696d9f043f1837bfb89709bf","name":"Juan Carlos Niebles","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64125da1227028b63e317ede/hyFtvVxhzMY-E3l8qyawR.mp4"],"publishedAt":"2026-01-15T18:49:48.000Z","submittedOnDailyAt":"2026-01-19T14:43:21.150Z","title":"Future Optical Flow Prediction Improves Robot Control & Video Generation","submittedOnDailyBy":{"_id":"64125da1227028b63e317ede","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678925215042-noauth.jpeg","isPro":false,"fullname":"Kanchana Ranasinghe","user":"kahnchana","type":"user"},"summary":"Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.","upvotes":8,"discussionId":"696d9f053f1837bfb89709c0","projectPage":"https://fofpred.github.io","githubRepo":"https://github.com/SalesforceAIResearch/FOFPred","githubRepoAddedBy":"user","ai_summary":"A novel language-conditioned optical flow forecasting model combines Vision-Language Model and Diffusion architecture to predict future motion from noisy web-scale video data, demonstrating versatility in robotic manipulation and video generation tasks.","ai_keywords":["Vision-Language Model","Diffusion architecture","optical flow forecasting","multimodal reasoning","pixel-level generative fidelity","web-scale human activity data","data preprocessing","image pretraining","robotic manipulation","video generation"],"githubStars":12,"organization":{"_id":"5f6d64475e78cc6b0ed31e4c","name":"Salesforce","fullname":"Salesforce","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"}},"publishedAt":"2026-01-15T13:49:48.000Z","title":"Future Optical Flow Prediction Improves Robot Control & Video Generation","summary":"Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64125da1227028b63e317ede/hyFtvVxhzMY-E3l8qyawR.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10781.png","numComments":1,"submittedBy":{"_id":"64125da1227028b63e317ede","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678925215042-noauth.jpeg","fullname":"Kanchana Ranasinghe","name":"kahnchana","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"5f6d64475e78cc6b0ed31e4c","name":"Salesforce","fullname":"Salesforce","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1602756670970-noauth.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.11514","authors":[{"_id":"696e08453f1837bfb8970b21","user":{"_id":"63fa41a24380ab0cb953655a","avatarUrl":"/avatars/7b5744208e49b8e4dda0b095d214597e.svg","isPro":false,"fullname":"Yawar Nihal","user":"yawarnihal","type":"user"},"name":"Yawar Siddiqui","status":"claimed_verified","statusLastChangedAt":"2026-01-19T17:36:01.202Z","hidden":false},{"_id":"696e08453f1837bfb8970b22","user":{"_id":"65fc12c6fc9132a2dfe9e8b3","avatarUrl":"/avatars/3d04d79721f4ec6b0b090e0bbbf85f70.svg","isPro":false,"fullname":"Duncan Frost","user":"frosd01","type":"user"},"name":"Duncan Frost","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:44:26.492Z","hidden":false},{"_id":"696e08453f1837bfb8970b23","user":{"_id":"66e9580650afdab5fd6ce00a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66e9580650afdab5fd6ce00a/L0sxbhGxl5rrRa9UjLAkK.jpeg","isPro":false,"fullname":"Samir Aroudj","user":"SamirAroudj","type":"user"},"name":"Samir Aroudj","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:44:32.397Z","hidden":false},{"_id":"696e08453f1837bfb8970b24","name":"Armen Avetisyan","hidden":false},{"_id":"696e08453f1837bfb8970b25","user":{"_id":"65d4991111930466954340ae","avatarUrl":"/avatars/ffc07be80323aa4b7dadf8849b70ffde.svg","isPro":false,"fullname":"Henry Howard-Jenkins","user":"henryhj","type":"user"},"name":"Henry Howard-Jenkins","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:44:41.099Z","hidden":false},{"_id":"696e08453f1837bfb8970b26","user":{"_id":"66f1e84ea4d3e85a77bdb545","avatarUrl":"/avatars/4e24b0bdd7a8c86ba5f43f4b042817ea.svg","isPro":false,"fullname":"Daniel DeTone","user":"ddetone","type":"user"},"name":"Daniel DeTone","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:44:47.110Z","hidden":false},{"_id":"696e08453f1837bfb8970b27","user":{"_id":"649b0f28c4afb9f5bcf20a27","avatarUrl":"/avatars/cedf08d01f90494b82eba49b84746f99.svg","isPro":false,"fullname":"Pierre Moulon","user":"TheFrenchLeaf","type":"user"},"name":"Pierre Moulon","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:44:53.004Z","hidden":false},{"_id":"696e08453f1837bfb8970b28","user":{"_id":"631db802f318ed8dfd3293fd","avatarUrl":"/avatars/698cb057456f7868fd3b06acbe08f095.svg","isPro":false,"fullname":"Qirui Wu","user":"qiruiw","type":"user"},"name":"Qirui Wu","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:44:58.602Z","hidden":false},{"_id":"696e08453f1837bfb8970b29","user":{"_id":"64ff5b2569219ce3e47893f1","avatarUrl":"/avatars/36bfb3de85488c0efa75d741eac9c40e.svg","isPro":false,"fullname":"Li","user":"Zhengqin","type":"user"},"name":"Zhengqin Li","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:45:08.258Z","hidden":false},{"_id":"696e08453f1837bfb8970b2a","name":"Julian Straub","hidden":false},{"_id":"696e08453f1837bfb8970b2b","name":"Richard Newcombe","hidden":false},{"_id":"696e08453f1837bfb8970b2c","name":"Jakob Engel","hidden":false}],"publishedAt":"2026-01-16T18:51:24.000Z","submittedOnDailyAt":"2026-01-19T08:04:36.072Z","title":"ShapeR: Robust Conditional 3D Shape Generation from Casual Captures","submittedOnDailyBy":{"_id":"63fa41a24380ab0cb953655a","avatarUrl":"/avatars/7b5744208e49b8e4dda0b095d214597e.svg","isPro":false,"fullname":"Yawar Nihal","user":"yawarnihal","type":"user"},"summary":"Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.","upvotes":7,"discussionId":"696e08453f1837bfb8970b2d","projectPage":"https://facebookresearch.github.io/ShapeR/","githubRepo":"https://github.com/facebookresearch/ShapeR","githubRepoAddedBy":"user","ai_summary":"ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.","ai_keywords":["3D shape generation","visual-inertial SLAM","3D detection algorithms","vision-language models","rectified flow transformer","Chamfer distance"],"githubStars":175,"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}},"publishedAt":"2026-01-16T13:51:24.000Z","title":"ShapeR: Robust Conditional 3D Shape Generation from Casual Captures","summary":"Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11514.png","numComments":2,"submittedBy":{"_id":"63fa41a24380ab0cb953655a","avatarUrl":"/avatars/7b5744208e49b8e4dda0b095d214597e.svg","fullname":"Yawar Nihal","name":"yawarnihal","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.10825","authors":[{"_id":"696d99753f1837bfb8970978","user":{"_id":"63f7e39b02eaf2cb1fd08c54","avatarUrl":"/avatars/ef0002e2ecfb33b7dc77a10fe948950c.svg","isPro":false,"fullname":"Junsol Kim","user":"junsol","type":"user"},"name":"Junsol Kim","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:42:40.247Z","hidden":false},{"_id":"696d99753f1837bfb8970979","user":{"_id":"642a01ed5673845d9853ec11","avatarUrl":"/avatars/2239e5c00d00b8048b65bf19d32b030c.svg","isPro":false,"fullname":"Shiyang Lai","user":"ShiYangLAI","type":"user"},"name":"Shiyang Lai","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:42:45.910Z","hidden":false},{"_id":"696d99753f1837bfb897097a","user":{"_id":"643dc14d37ca9134b589bfe0","avatarUrl":"/avatars/998977d708f205dd5df430705e55ebc9.svg","isPro":false,"fullname":"Nino Scherrer","user":"ninoscherrer","type":"user"},"name":"Nino Scherrer","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:42:51.168Z","hidden":false},{"_id":"696d99753f1837bfb897097b","name":"Blaise Agüera y Arcas","hidden":false},{"_id":"696d99753f1837bfb897097c","name":"James Evans","hidden":false}],"publishedAt":"2026-01-15T19:52:33.000Z","submittedOnDailyAt":"2026-01-19T00:09:55.018Z","title":"Reasoning Models Generate Societies of Thought","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.","upvotes":5,"discussionId":"696d99753f1837bfb897097d","ai_summary":"Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.","ai_keywords":["large language models","reasoning models","chains of thought","multi-agent-like interactions","perspective diversity","personality traits","domain expertise","mechanistic interpretability","conversational behaviors","reinforcement learning","agent organization","collective intelligence","wisdom of crowds"]},"publishedAt":"2026-01-15T14:52:33.000Z","title":"Reasoning Models Generate Societies of Thought","summary":"Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10825.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":210,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.11087","authors":[{"_id":"696da79a3f1837bfb89709ea","name":"Qiyuan Zhang","hidden":false},{"_id":"696da79a3f1837bfb89709eb","user":{"_id":"644fcbea4f7316588267dc80","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg","isPro":false,"fullname":"Biao Gong","user":"BiaoGong","type":"user"},"name":"Biao Gong","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:43:37.154Z","hidden":false},{"_id":"696da79a3f1837bfb89709ec","user":{"_id":"6697b62a27bf71ffe289492f","avatarUrl":"/avatars/f48095f1e2ca69836294343301be1700.svg","isPro":false,"fullname":"Shuai Tan","user":"Shuaishuai0219","type":"user"},"name":"Shuai Tan","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:43:45.871Z","hidden":false},{"_id":"696da79a3f1837bfb89709ed","name":"Zheng Zhang","hidden":false},{"_id":"696da79a3f1837bfb89709ee","user":{"_id":"6969f153bb68e272ac6e2676","avatarUrl":"/avatars/06d79a105c8d36c86f3e024ab41a9998.svg","isPro":false,"fullname":"Yujun Shen","user":"shen12313","type":"user"},"name":"Yujun Shen","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:43:52.528Z","hidden":false},{"_id":"696da79a3f1837bfb89709ef","name":"Xing Zhu","hidden":false},{"_id":"696da79a3f1837bfb89709f0","name":"Yuyuan Li","hidden":false},{"_id":"696da79a3f1837bfb89709f1","name":"Kelu Yao","hidden":false},{"_id":"696da79a3f1837bfb89709f2","name":"Chunhua Shen","hidden":false},{"_id":"696da79a3f1837bfb89709f3","name":"Changqing Zou","hidden":false}],"publishedAt":"2026-01-16T08:40:10.000Z","submittedOnDailyAt":"2026-01-19T01:13:46.189Z","title":"PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models","submittedOnDailyBy":{"_id":"644fcbea4f7316588267dc80","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg","isPro":false,"fullname":"Biao Gong","user":"BiaoGong","type":"user"},"summary":"Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.","upvotes":4,"discussionId":"696da79a3f1837bfb89709f4","ai_summary":"A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.","ai_keywords":["reinforcement learning","video generation","physical collision rules","high-dimensional spaces","physics-aware","Mimicry-Discovery Cycle","PhysRVGBench"]},"publishedAt":"2026-01-16T03:40:10.000Z","title":"PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models","summary":"Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11087.png","numComments":2,"submittedBy":{"_id":"644fcbea4f7316588267dc80","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg","fullname":"Biao Gong","name":"BiaoGong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.09636","authors":[{"_id":"6969ee7d844a787c4fdea3ed","user":{"_id":"65f18441214a053c696e2c63","avatarUrl":"/avatars/d469a7cce6d33f44766509c2c839c6b3.svg","isPro":false,"fullname":"Yibo Lyu","user":"user0102","type":"user"},"name":"Yibo Lyu","status":"claimed_verified","statusLastChangedAt":"2026-01-19T09:23:40.435Z","hidden":false},{"_id":"6969ee7d844a787c4fdea3ee","user":{"_id":"66b4a3575fa7debf39254e5c","avatarUrl":"/avatars/765036bc276a1eb6ffb00e865eb2d40a.svg","isPro":false,"fullname":"Gongwei Chen","user":"cgwfeel","type":"user"},"name":"Gongwei Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:43:08.570Z","hidden":false},{"_id":"6969ee7d844a787c4fdea3ef","name":"Rui Shao","hidden":false},{"_id":"6969ee7d844a787c4fdea3f0","name":"Weili Guan","hidden":false},{"_id":"6969ee7d844a787c4fdea3f1","name":"Liqiang Nie","hidden":false}],"publishedAt":"2026-01-14T17:12:48.000Z","submittedOnDailyAt":"2026-01-19T08:49:23.205Z","title":"PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records","submittedOnDailyBy":{"_id":"65f18441214a053c696e2c63","avatarUrl":"/avatars/d469a7cce6d33f44766509c2c839c6b3.svg","isPro":false,"fullname":"Yibo Lyu","user":"user0102","type":"user"},"summary":"While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.","upvotes":4,"discussionId":"6969ee7d844a787c4fdea3f2","projectPage":"https://jiutian-vl.github.io/PersonalAlign-page/","ai_summary":"PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.","ai_keywords":["Hierarchical Implicit Intent Alignment","Personalized GUI Agent","long-term user records","vague instructions","latent routines","AndroidIntent","Hierarchical Intent Memory Agent","personal memory","user preferences","proactive assistance"]},"publishedAt":"2026-01-14T12:12:48.000Z","title":"PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records","summary":"While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09636.png","numComments":3,"submittedBy":{"_id":"65f18441214a053c696e2c63","avatarUrl":"/avatars/d469a7cce6d33f44766509c2c839c6b3.svg","fullname":"Yibo Lyu","name":"user0102","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.11516","authors":[{"_id":"696d99c23f1837bfb897097f","user":{"_id":"6647488b45ee22196e798ba6","avatarUrl":"/avatars/7c068c30dcd5f07e4d896fd1dce5be2c.svg","isPro":false,"fullname":"Janos Kramar","user":"jkramar","type":"user"},"name":"János Kramár","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:45:38.688Z","hidden":false},{"_id":"696d99c23f1837bfb8970980","name":"Joshua Engels","hidden":false},{"_id":"696d99c23f1837bfb8970981","name":"Zheng Wang","hidden":false},{"_id":"696d99c23f1837bfb8970982","user":{"_id":"64ad563f4beffa272de6efac","avatarUrl":"/avatars/f1a4902a95830cc3936058449626f8e4.svg","isPro":false,"fullname":"Bilal Chughtai","user":"bilalchughtai","type":"user"},"name":"Bilal Chughtai","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:45:47.795Z","hidden":false},{"_id":"696d99c23f1837bfb8970983","name":"Rohin Shah","hidden":false},{"_id":"696d99c23f1837bfb8970984","name":"Neel Nanda","hidden":false},{"_id":"696d99c23f1837bfb8970985","name":"Arthur Conmy","hidden":false}],"publishedAt":"2026-01-16T18:54:29.000Z","submittedOnDailyAt":"2026-01-19T00:11:12.659Z","title":"Building Production-Ready Probes For Gemini","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.\n  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.\n  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.","upvotes":3,"discussionId":"696d99c33f1837bfb8970986","ai_summary":"Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.","ai_keywords":["activation probes","language model","misuse mitigation","context length","probe architecture","cyber-offensive domain","multi-turn conversations","jailbreaks","red teaming","AlphaEvolve","automated AI safety research"]},"publishedAt":"2026-01-16T13:54:29.000Z","title":"Building Production-Ready Probes For Gemini","summary":"Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.\n  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.\n  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11516.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":210,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.11044","authors":[{"_id":"696dd5da3f1837bfb8970a86","user":{"_id":"668e476520e499a0786ea56e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/668e476520e499a0786ea56e/lnvd1_UWW9o9ddrR6ehwR.png","isPro":false,"fullname":"Keyu Li (SII)","user":"weizhihao1","type":"user"},"name":"Keyu Li","status":"claimed_verified","statusLastChangedAt":"2026-01-19T09:22:02.662Z","hidden":false},{"_id":"696dd5da3f1837bfb8970a87","name":"Junhao Shi","hidden":false},{"_id":"696dd5da3f1837bfb8970a88","name":"Yang Xiao","hidden":false},{"_id":"696dd5da3f1837bfb8970a89","name":"Mohan Jiang","hidden":false},{"_id":"696dd5da3f1837bfb8970a8a","name":"Jie Sun","hidden":false},{"_id":"696dd5da3f1837bfb8970a8b","name":"Yunze Wu","hidden":false},{"_id":"696dd5da3f1837bfb8970a8c","name":"Shijie Xia","hidden":false},{"_id":"696dd5da3f1837bfb8970a8d","name":"Xiaojie Cai","hidden":false},{"_id":"696dd5da3f1837bfb8970a8e","name":"Tianze Xu","hidden":false},{"_id":"696dd5da3f1837bfb8970a8f","name":"Weiye Si","hidden":false},{"_id":"696dd5da3f1837bfb8970a90","name":"Wenjie Li","hidden":false},{"_id":"696dd5da3f1837bfb8970a91","name":"Dequan Wang","hidden":false},{"_id":"696dd5da3f1837bfb8970a92","name":"Pengfei Liu","hidden":false}],"publishedAt":"2026-01-16T07:22:20.000Z","submittedOnDailyAt":"2026-01-19T15:01:17.935Z","title":"AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts","submittedOnDailyBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","isPro":true,"fullname":"Rajkumar rawal","user":"rajkumarrawal","type":"user"},"summary":"Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.","upvotes":2,"discussionId":"696dd5db3f1837bfb8970a93","projectPage":"https://agencybench.opensii.ai","githubRepo":"https://github.com/GAIR-NLP/AgencyBench","githubRepoAddedBy":"user","ai_summary":"AgencyBench presents a comprehensive benchmark for evaluating autonomous agents across real-world scenarios, enabling automated evaluation through user simulation and sandbox environments while revealing performance gaps between closed-source and open-source models.","ai_keywords":["large language models","autonomous agents","benchmarks","agentic capabilities","user simulation agent","Docker sandbox","tool calls","token consumption","model architecture","agentic frameworks"],"githubStars":19,"organization":{"_id":"630bc2d186b8b9904c33ce1b","name":"GAIR","fullname":"SII - GAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}},"publishedAt":"2026-01-16T02:22:20.000Z","title":"AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts","summary":"Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11044.png","numComments":1,"submittedBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","fullname":"Rajkumar rawal","name":"rajkumarrawal","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":48,"isUserFollowing":false},"organization":{"_id":"630bc2d186b8b9904c33ce1b","name":"GAIR","fullname":"SII - GAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.07812","authors":[{"_id":"696df8ea3f1837bfb8970b04","name":"Anurag Das","hidden":false},{"_id":"696df8ea3f1837bfb8970b05","user":{"_id":"64a6e0923987f4dd3c37087d","avatarUrl":"/avatars/88bc86ddce1b38a012b07c81f5c61183.svg","isPro":false,"fullname":"Adrian Bulat","user":"adrianb1","type":"user"},"name":"Adrian Bulat","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:54:52.098Z","hidden":false},{"_id":"696df8ea3f1837bfb8970b06","name":"Alberto Baldrati","hidden":false},{"_id":"696df8ea3f1837bfb8970b07","user":{"_id":"6731f7dbd2ef7f48198cdcd7","avatarUrl":"/avatars/a10ddf5e047eb0d8f480478d9fba30a1.svg","isPro":false,"fullname":"Ioannis Maniadis Metaxas","user":"GManiadis","type":"user"},"name":"Ioannis Maniadis Metaxas","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:54:34.134Z","hidden":false},{"_id":"696df8ea3f1837bfb8970b08","name":"Bernt Schiele","hidden":false},{"_id":"696df8ea3f1837bfb8970b09","user":{"_id":"649db0dcd8f9b2e2cb47e4d7","avatarUrl":"/avatars/e1fb64d5302271a3557ba306a85f2363.svg","isPro":false,"fullname":"Georgios Tzimiropoulos","user":"tzimiro","type":"user"},"name":"Georgios Tzimiropoulos","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:55:01.071Z","hidden":false},{"_id":"696df8ea3f1837bfb8970b0a","name":"Brais Martinez","hidden":false}],"publishedAt":"2026-01-12T18:45:13.000Z","submittedOnDailyAt":"2026-01-19T07:00:02.929Z","title":"More Images, More Problems? A Controlled Analysis of VLM Failure Modes","submittedOnDailyBy":{"_id":"64a6e0923987f4dd3c37087d","avatarUrl":"/avatars/88bc86ddce1b38a012b07c81f5c61183.svg","isPro":false,"fullname":"Adrian Bulat","user":"adrianb1","type":"user"},"summary":"Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.","upvotes":2,"discussionId":"696df8ea3f1837bfb8970b0b","githubRepo":"https://github.com/anurag-198/MIMIC","githubRepoAddedBy":"auto","ai_summary":"Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.","ai_keywords":["Large Vision Language Models","multi-image capabilities","benchmark","diagnostic experiments","cross-image aggregation","attention-masking scheme","procedural data-generation strategy"],"githubStars":0,"organization":{"_id":"686df54910a52f2c2cf03c06","name":"SamsungResearch","fullname":"Samsung Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"}},"publishedAt":"2026-01-12T13:45:13.000Z","title":"More Images, More Problems? A Controlled Analysis of VLM Failure Modes","summary":"Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07812.png","numComments":2,"submittedBy":{"_id":"64a6e0923987f4dd3c37087d","avatarUrl":"/avatars/88bc86ddce1b38a012b07c81f5c61183.svg","fullname":"Adrian Bulat","name":"adrianb1","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"686df54910a52f2c2cf03c06","name":"SamsungResearch","fullname":"Samsung Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.11354","authors":[{"_id":"696d99113f1837bfb8970971","name":"Weiyi Wang","hidden":false},{"_id":"696d99113f1837bfb8970972","user":{"_id":"690d8fb50b398ae0a9aeeb51","avatarUrl":"/avatars/6ca9f3d08f12c316d3c37711dfd1b5de.svg","isPro":false,"fullname":"Xinchi Chen","user":"dalstonchen","type":"user"},"name":"Xinchi Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:46:43.643Z","hidden":false},{"_id":"696d99113f1837bfb8970973","name":"Jingjing Gong","hidden":false},{"_id":"696d99113f1837bfb8970974","user":{"_id":"67f9c4ee171948c38302ae0f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Cqb3ijr_sZkpLhEEEEybK.png","isPro":false,"fullname":"Xuanjing Huang","user":"xjhuang","type":"user"},"name":"Xuanjing Huang","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:46:20.037Z","hidden":false},{"_id":"696d99113f1837bfb8970975","user":{"_id":"61457b8deff2c9fdb4de4988","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg","isPro":false,"fullname":"Xipeng Qiu","user":"xpqiu","type":"user"},"name":"Xipeng Qiu","status":"admin_assigned","statusLastChangedAt":"2026-01-19T14:46:14.074Z","hidden":false}],"publishedAt":"2026-01-16T15:02:41.000Z","submittedOnDailyAt":"2026-01-19T00:08:23.333Z","title":"AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.","upvotes":1,"discussionId":"696d99113f1837bfb8970976","githubRepo":"https://github.com/Mtrya/astro-reason","githubRepoAddedBy":"user","ai_summary":"Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.","ai_keywords":["agentic Large Language Models","generalist planners","symbolic environments","weakly grounded environments","physics-constrained domains","Space Planning Problems","heterogeneous objectives","physical constraints","long-horizon decision-making","scheduling regimes","ground station communication","agile Earth observation","agent-oriented interaction protocol","specialized solvers","diagnostic testbed"],"githubStars":4},"publishedAt":"2026-01-16T10:02:41.000Z","title":"AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems","summary":"Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11354.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":210,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.11227","authors":[{"_id":"696df8e03f1837bfb8970af6","user":{"_id":"639ae8dfb49b726255975f86","avatarUrl":"/avatars/3361477fb2de29eaea5484696b2721c6.svg","isPro":false,"fullname":"xushaoyang","user":"beiweixiaoxu","type":"user"},"name":"Shaoyang Xu","status":"claimed_verified","statusLastChangedAt":"2026-01-19T09:27:18.477Z","hidden":false},{"_id":"696df8e03f1837bfb8970af7","name":"Wenxuan Zhang","hidden":false}],"publishedAt":"2026-01-16T12:14:16.000Z","submittedOnDailyAt":"2026-01-19T07:01:26.528Z","title":"Language of Thought Shapes Output Diversity in Large Language Models","submittedOnDailyBy":{"_id":"639ae8dfb49b726255975f86","avatarUrl":"/avatars/3361477fb2de29eaea5484696b2721c6.svg","isPro":false,"fullname":"xushaoyang","user":"beiweixiaoxu","type":"user"},"summary":"Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.","upvotes":1,"discussionId":"696df8e13f1837bfb8970af8","githubRepo":"https://github.com/iNLP-Lab/Multilingual-LoT-Diversity","githubRepoAddedBy":"user","ai_summary":"Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.","ai_keywords":["language of thought","thinking space","multilingual thinking","Single-Language Sampling","Mixed-Language Sampling","output diversity","linguistic heterogeneity","pluralistic alignment"],"githubStars":2,"organization":{"_id":"68b82daee976083ccd80824b","name":"iNLP-Lab","fullname":"iNLP Lab @ SUTD","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60dff6ae19a362a8c27862aa/3Ukf0b4f546tJM84zynTz.png"}},"publishedAt":"2026-01-16T07:14:16.000Z","title":"Language of Thought Shapes Output Diversity in Large Language Models","summary":"Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11227.png","numComments":2,"submittedBy":{"_id":"639ae8dfb49b726255975f86","avatarUrl":"/avatars/3361477fb2de29eaea5484696b2721c6.svg","fullname":"xushaoyang","name":"beiweixiaoxu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"68b82daee976083ccd80824b","name":"iNLP-Lab","fullname":"iNLP Lab @ SUTD","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60dff6ae19a362a8c27862aa/3Ukf0b4f546tJM84zynTz.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.10922","authors":[{"_id":"696e6ff15750539a88acc701","name":"Yosub Shin","hidden":false},{"_id":"696e6ff15750539a88acc702","name":"Michael Buriek","hidden":false},{"_id":"696e6ff15750539a88acc703","name":"Boris Sobolev","hidden":false},{"_id":"696e6ff15750539a88acc704","name":"Pavel Bushuyeu","hidden":false},{"_id":"696e6ff15750539a88acc705","name":"Vikas Kumar","hidden":false},{"_id":"696e6ff15750539a88acc706","name":"Haoyang Xu","hidden":false},{"_id":"696e6ff15750539a88acc707","name":"Samuel Watson","hidden":false},{"_id":"696e6ff15750539a88acc708","name":"Igor Molybog","hidden":false}],"publishedAt":"2026-01-16T00:50:01.000Z","submittedOnDailyAt":"2026-01-19T15:37:55.052Z","title":"What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge","submittedOnDailyBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","isPro":true,"fullname":"Rajkumar rawal","user":"rajkumarrawal","type":"user"},"summary":"We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.","upvotes":1,"discussionId":"696e6ff15750539a88acc709","ai_summary":"Data curation for multimodal reasoning shows that difficulty-based example selection on aligned datasets drives performance gains, while increasing dataset size mainly reduces variance and synthetic augmentation heuristics often degrade performance.","ai_keywords":["data curation","multimodal reasoning","dataset selection","model training","example selection","aligned dataset","saturation-regime evaluation","alignment","difficulty"]},"publishedAt":"2026-01-15T19:50:01.000Z","title":"What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge","summary":"We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10922.png","numComments":1,"submittedBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","fullname":"Rajkumar rawal","name":"rajkumarrawal","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":48,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.09255","authors":[{"_id":"696a064a844a787c4fdea458","user":{"_id":"63710c2bafbe42caa5a4dfc8","avatarUrl":"/avatars/33fd1695466bff2b14c3ab42793826ad.svg","isPro":false,"fullname":"zhaoyibo","user":"zjuyb","type":"user"},"name":"Yibo Zhao","status":"claimed_verified","statusLastChangedAt":"2026-01-16T10:28:24.372Z","hidden":false},{"_id":"696a064a844a787c4fdea459","name":"Hengjia Li","hidden":false},{"_id":"696a064a844a787c4fdea45a","name":"Xiaofei He","hidden":false},{"_id":"696a064a844a787c4fdea45b","name":"Boxi Wu","hidden":false}],"publishedAt":"2026-01-14T07:41:56.000Z","submittedOnDailyAt":"2026-01-19T05:31:10.396Z","title":"PhyRPR: Training-Free Physics-Constrained Video Generation","submittedOnDailyBy":{"_id":"63710c2bafbe42caa5a4dfc8","avatarUrl":"/avatars/33fd1695466bff2b14c3ab42793826ad.svg","isPro":false,"fullname":"zhaoyibo","user":"zjuyb","type":"user"},"summary":"Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\\uline{Reason}--Phy\\uline{Plan}--Phy\\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.","upvotes":1,"discussionId":"696a064a844a787c4fdea45c","ai_summary":"A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.","ai_keywords":["diffusion models","video generation","physical reasoning","motion scaffolding","latent fusion strategy"]},"publishedAt":"2026-01-14T02:41:56.000Z","title":"PhyRPR: Training-Free Physics-Constrained Video Generation","summary":"Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\\uline{Reason}--Phy\\uline{Plan}--Phy\\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09255.png","numComments":2,"submittedBy":{"_id":"63710c2bafbe42caa5a4dfc8","avatarUrl":"/avatars/33fd1695466bff2b14c3ab42793826ad.svg","fullname":"zhaoyibo","name":"zjuyb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true}]
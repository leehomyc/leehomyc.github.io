[{"paper":{"id":"2602.21193","authors":[{"_id":"699e72b4dfbcf0b800aecb63","name":"Renjie Pi","hidden":false},{"_id":"699e72b4dfbcf0b800aecb64","name":"Grace Lam","hidden":false},{"_id":"699e72b4dfbcf0b800aecb65","name":"Mohammad Shoeybi","hidden":false},{"_id":"699e72b4dfbcf0b800aecb66","name":"Pooya Jannaty","hidden":false},{"_id":"699e72b4dfbcf0b800aecb67","name":"Bryan Catanzaro","hidden":false},{"_id":"699e72b4dfbcf0b800aecb68","name":"Wei Ping","hidden":false}],"publishedAt":"2026-02-24T18:51:04.000Z","submittedOnDailyAt":"2026-02-25T01:39:40.130Z","title":"On Data Engineering for Scaling LLM Terminal Capabilities","submittedOnDailyBy":{"_id":"63f45b8d520c14618930d175","avatarUrl":"/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg","isPro":false,"fullname":"renjie","user":"renjiepi","type":"user"},"summary":"Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.","upvotes":60,"discussionId":"699e72b5dfbcf0b800aecb69","projectPage":"https://huggingface.co/collections/nvidia/nemotron-terminal","ai_summary":"Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.","ai_keywords":["large language models","terminal agents","data engineering practices","synthetic task generation","Terminal-Task-Gen","Terminal-Corpus","Nemotron-Terminal","Terminal-Bench 2.0","curriculum learning","long context training","scaling behavior"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-02-24T13:51:04.000Z","title":"On Data Engineering for Scaling LLM Terminal Capabilities","summary":"Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21193.png","numComments":2,"submittedBy":{"_id":"63f45b8d520c14618930d175","avatarUrl":"/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg","fullname":"renjie","name":"renjiepi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.12192","authors":[{"_id":"698ebfdacace060ff123aece","user":{"_id":"6891bbff78946201296b4592","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6891bbff78946201296b4592/ECmWBrlfeonPg0HzmQ_sW.png","isPro":false,"fullname":"Yuqing Li","user":"MindscapeRAG","type":"user"},"name":"Yuqing Li","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:36:07.802Z","hidden":false},{"_id":"698ebfdacace060ff123aecf","name":"Jiangnan Li","hidden":false},{"_id":"698ebfdacace060ff123aed0","user":{"_id":"67af92045a86287292026808","avatarUrl":"/avatars/8bad9272fe73ba04e077b5484837c8d3.svg","isPro":false,"fullname":"Mo","user":"BishopGorov","type":"user"},"name":"Mo Yu","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:32:43.530Z","hidden":false},{"_id":"698ebfdacace060ff123aed1","name":"Guoxuan Ding","hidden":false},{"_id":"698ebfdacace060ff123aed2","name":"Zheng Lin","hidden":false},{"_id":"698ebfdacace060ff123aed3","name":"Weiping Wang","hidden":false},{"_id":"698ebfdacace060ff123aed4","name":"Jie Zhou","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6891bbff78946201296b4592/LPd7mcXZoPkkNY6Q8EE0E.gif"],"publishedAt":"2026-02-12T17:23:38.000Z","submittedOnDailyAt":"2026-02-25T07:37:44.371Z","title":"Query-focused and Memory-aware Reranker for Long Context Processing","submittedOnDailyBy":{"_id":"6891bbff78946201296b4592","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6891bbff78946201296b4592/ECmWBrlfeonPg0HzmQ_sW.png","isPro":false,"fullname":"Yuqing Li","user":"MindscapeRAG","type":"user"},"summary":"Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.","upvotes":35,"discussionId":"698ebfdbcace060ff123aed5","projectPage":"https://qdcassie-li.github.io/QRRanker/","ai_summary":"A lightweight reranking framework uses attention scores from selected heads to estimate passage-query relevance, achieving strong performance across multiple domains and benchmarks.","ai_keywords":["retrieval heads","reranking framework","attention scores","passage-query relevance","listwise solution","candidate shortlist","continuous relevance scores","Likert-scale supervision","pointwise rerankers","listwise rerankers","LoCoMo benchmark","dialogue understanding","memory usage","contextual information","middle layers"],"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-12T12:23:38.000Z","title":"Query-focused and Memory-aware Reranker for Long Context Processing","summary":"Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6891bbff78946201296b4592/LPd7mcXZoPkkNY6Q8EE0E.gif"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12192.png","numComments":2,"submittedBy":{"_id":"6891bbff78946201296b4592","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6891bbff78946201296b4592/ECmWBrlfeonPg0HzmQ_sW.png","fullname":"Yuqing Li","name":"MindscapeRAG","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.20739","authors":[{"_id":"699e674ddfbcf0b800aecae9","user":{"_id":"62c66504031996c36c86976a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png","isPro":false,"fullname":"steve z","user":"stzhao","type":"user"},"name":"Shitian Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:58.018Z","hidden":false},{"_id":"699e674ddfbcf0b800aecaea","name":"Shaoheng Lin","hidden":false},{"_id":"699e674ddfbcf0b800aecaeb","name":"Ming Li","hidden":false},{"_id":"699e674ddfbcf0b800aecaec","user":{"_id":"67ff7f687351095d4b606b84","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67ff7f687351095d4b606b84/KhNPmbBC3zghuP5h1MK-c.png","isPro":false,"fullname":"Haoquan Zhang","user":"haoquan03","type":"user"},"name":"Haoquan Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:51.660Z","hidden":false},{"_id":"699e674ddfbcf0b800aecaed","name":"Wenshuo Peng","hidden":false},{"_id":"699e674ddfbcf0b800aecaee","name":"Kaipeng Zhang","hidden":false},{"_id":"699e674ddfbcf0b800aecaef","name":"Chen Wei","hidden":false}],"publishedAt":"2026-02-24T10:08:33.000Z","submittedOnDailyAt":"2026-02-25T00:41:13.309Z","title":"PyVision-RL: Forging Open Agentic Vision Models via RL","submittedOnDailyBy":{"_id":"62c66504031996c36c86976a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png","isPro":false,"fullname":"steve z","user":"stzhao","type":"user"},"summary":"Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.","upvotes":22,"discussionId":"699e674edfbcf0b800aecaf0","projectPage":"https://agent-x.space/pyvision-rl/","githubRepo":"https://github.com/agents-x-project/PyVision-RL","githubRepoAddedBy":"user","ai_summary":"PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.","ai_keywords":["reinforcement learning","agentic multimodal models","interaction collapse","oversampling-filtering-ranking","accumulative tool reward","unified training pipeline","on-demand context construction","task-relevant frames","visual token usage"],"githubStars":43},"publishedAt":"2026-02-24T05:08:33.000Z","title":"PyVision-RL: Forging Open Agentic Vision Models via RL","summary":"Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20739.png","numComments":1,"submittedBy":{"_id":"62c66504031996c36c86976a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png","fullname":"steve z","name":"stzhao","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":12,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.21015","authors":[{"_id":"699e69fbdfbcf0b800aecafb","user":{"_id":"63369da91ba5d5ece24118a4","avatarUrl":"/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg","isPro":false,"fullname":"wuyuhao","user":"mozhu","type":"user"},"name":"Yuhao Wu","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:26.632Z","hidden":false},{"_id":"699e69fbdfbcf0b800aecafc","name":"Maojia Song","hidden":false},{"_id":"699e69fbdfbcf0b800aecafd","name":"Yihuai Lan","hidden":false},{"_id":"699e69fbdfbcf0b800aecafe","name":"Lei Wang","hidden":false},{"_id":"699e69fbdfbcf0b800aecaff","user":{"_id":"637f228152229c63921119c3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg","isPro":false,"fullname":"Zhiqiang Hu","user":"Zhiqiang007","type":"user"},"name":"Zhiqiang Hu","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:48.417Z","hidden":false},{"_id":"699e69fbdfbcf0b800aecb00","name":"Yao Xiao","hidden":false},{"_id":"699e69fbdfbcf0b800aecb01","user":{"_id":"660d17d6c9be0dcd31a30b3d","avatarUrl":"/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg","isPro":false,"fullname":"Zhou Heng","user":"henggg","type":"user"},"name":"Heng Zhou","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:45.251Z","hidden":false},{"_id":"699e69fbdfbcf0b800aecb02","name":"Weihua Zheng","hidden":false},{"_id":"699e69fbdfbcf0b800aecb03","name":"Dylan Raharja","hidden":false},{"_id":"699e69fbdfbcf0b800aecb04","name":"Soujanya Poria","hidden":false},{"_id":"699e69fbdfbcf0b800aecb05","name":"Roy Ka-Wei Lee","hidden":false}],"publishedAt":"2026-02-24T15:33:02.000Z","submittedOnDailyAt":"2026-02-25T00:55:31.629Z","title":"From Perception to Action: An Interactive Benchmark for Vision Reasoning","submittedOnDailyBy":{"_id":"637f228152229c63921119c3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg","isPro":false,"fullname":"Zhiqiang Hu","user":"Zhiqiang007","type":"user"},"summary":"Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.","upvotes":20,"discussionId":"699e69fbdfbcf0b800aecb06","projectPage":"https://social-ai-studio.github.io/CHAIN/","githubRepo":"https://github.com/Social-AI-Studio/CHAIN","githubRepoAddedBy":"user","ai_summary":"Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.","ai_keywords":["Vision-Language Model","diffusion-based models","physical constraints","causal constraints","interactive 3D","structured action sequences","long-horizon planning"],"githubStars":3},"publishedAt":"2026-02-24T10:33:02.000Z","title":"From Perception to Action: An Interactive Benchmark for Vision Reasoning","summary":"Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21015.png","numComments":2,"submittedBy":{"_id":"637f228152229c63921119c3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg","fullname":"Zhiqiang Hu","name":"Zhiqiang007","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.21204","authors":[{"_id":"699e6e60dfbcf0b800aecb16","name":"Junchen Liu","hidden":false},{"_id":"699e6e60dfbcf0b800aecb17","name":"Sven Elflein","hidden":false},{"_id":"699e6e60dfbcf0b800aecb18","name":"Or Litany","hidden":false},{"_id":"699e6e60dfbcf0b800aecb19","name":"Zan Gojcic","hidden":false},{"_id":"699e6e60dfbcf0b800aecb1a","name":"Ruilong Li","hidden":false}],"publishedAt":"2026-02-24T18:59:30.000Z","submittedOnDailyAt":"2026-02-25T03:15:10.854Z","title":"Test-Time Training with KV Binding Is Secretly Linear Attention","submittedOnDailyBy":{"_id":"663a9ae58cf658ffaf3d02e5","avatarUrl":"/avatars/aa35668dc088e675e794b9ceb935c56f.svg","isPro":false,"fullname":"Junchen Liu","user":"JunchenLiu","type":"user"},"summary":"Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.","upvotes":19,"discussionId":"699e6e61dfbcf0b800aecb1b","projectPage":"https://research.nvidia.com/labs/sil/projects/tttla/","ai_summary":"Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.","ai_keywords":["test-time training","KV binding","online meta-learning","learned linear attention","sequence modeling layer","linear attention operator","architectural simplifications","parallel formulations","representational capacity"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-02-24T13:59:30.000Z","title":"Test-Time Training with KV Binding Is Secretly Linear Attention","summary":"Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21204.png","numComments":1,"submittedBy":{"_id":"663a9ae58cf658ffaf3d02e5","avatarUrl":"/avatars/aa35668dc088e675e794b9ceb935c56f.svg","fullname":"Junchen Liu","name":"JunchenLiu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.21202","authors":[{"_id":"699f21fbebfce7fbcca919cd","name":"Hanxiang Qin","hidden":false},{"_id":"699f21fbebfce7fbcca919ce","name":"Alexander Martin","hidden":false},{"_id":"699f21fbebfce7fbcca919cf","name":"Rohan Jha","hidden":false},{"_id":"699f21fbebfce7fbcca919d0","name":"Chunsheng Zuo","hidden":false},{"_id":"699f21fbebfce7fbcca919d1","name":"Reno Kriz","hidden":false},{"_id":"699f21fbebfce7fbcca919d2","name":"Benjamin Van Durme","hidden":false}],"publishedAt":"2026-02-24T18:57:33.000Z","submittedOnDailyAt":"2026-02-25T13:53:39.027Z","title":"Multi-Vector Index Compression in Any Modality","submittedOnDailyBy":{"_id":"644ffb154f731658826945c8","avatarUrl":"/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg","isPro":false,"fullname":"Alex Martin","user":"alexmartin1722","type":"user"},"summary":"We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.","upvotes":18,"discussionId":"699f21fcebfce7fbcca919d3","githubRepo":"https://github.com/hanxiangqin/omni-col-press","githubRepoAddedBy":"user","ai_summary":"Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.","ai_keywords":["late interaction","multi-vector retrieval","index compression","attention-guided clustering","sequence resizing","memory tokens","hierarchical pooling"],"githubStars":0,"organization":{"_id":"643568fef81a16e74361e659","name":"hltcoe","fullname":"JHU Human Language Technology Center of Excellence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/643568cdf3b08e267d9808f1/u-FGMfs5blYzYGX4ErT7d.png"}},"publishedAt":"2026-02-24T13:57:33.000Z","title":"Multi-Vector Index Compression in Any Modality","summary":"We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21202.png","numComments":1,"submittedBy":{"_id":"644ffb154f731658826945c8","avatarUrl":"/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg","fullname":"Alex Martin","name":"alexmartin1722","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"643568fef81a16e74361e659","name":"hltcoe","fullname":"JHU Human Language Technology Center of Excellence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/643568cdf3b08e267d9808f1/u-FGMfs5blYzYGX4ErT7d.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.20951","authors":[{"_id":"699e74f6dfbcf0b800aecb74","user":{"_id":"6682429bb1f69d3cd0104389","avatarUrl":"/avatars/05745a9fba109c7cf87e03f1630ff8b0.svg","isPro":false,"fullname":"Jaehyun Park","user":"Cabbalett","type":"user"},"name":"Jaehyun Park","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:19.146Z","hidden":false},{"_id":"699e74f6dfbcf0b800aecb75","name":"Minyoung Ahn","hidden":false},{"_id":"699e74f6dfbcf0b800aecb76","name":"Minkyu Kim","hidden":false},{"_id":"699e74f6dfbcf0b800aecb77","user":{"_id":"6632e41743f364c13e3418cd","avatarUrl":"/avatars/e0394f1012d38ea354ea54d53d6c5e2f.svg","isPro":false,"fullname":"Jonghyun Lee","user":"Jhyun17","type":"user"},"name":"Jonghyun Lee","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:16.907Z","hidden":false},{"_id":"699e74f6dfbcf0b800aecb78","name":"Jae-Gil Lee","hidden":false},{"_id":"699e74f6dfbcf0b800aecb79","name":"Dongmin Park","hidden":false}],"publishedAt":"2026-02-24T14:34:13.000Z","submittedOnDailyAt":"2026-02-25T15:09:39.634Z","title":"See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis","submittedOnDailyBy":{"_id":"6682429bb1f69d3cd0104389","avatarUrl":"/avatars/05745a9fba109c7cf87e03f1630ff8b0.svg","isPro":false,"fullname":"Jaehyun Park","user":"Cabbalett","type":"user"},"summary":"Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.","upvotes":11,"discussionId":"699e74f6dfbcf0b800aecb7a","githubRepo":"https://github.com/krafton-ai/ArtiAgent","githubRepoAddedBy":"user","ai_summary":"ArtiAgent automates the creation of real-artifact image pairs using three agents for perception, synthesis, and curation in diffusion transformers.","ai_keywords":["diffusion transformer","artifact injection tools","patch-wise embedding manipulation","artifact-aware methodologies","artifact-annotated datasets","perception agent","synthesis agent","curation agent"],"githubStars":1},"publishedAt":"2026-02-24T09:34:13.000Z","title":"See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis","summary":"Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20951.png","numComments":1,"submittedBy":{"_id":"6682429bb1f69d3cd0104389","avatarUrl":"/avatars/05745a9fba109c7cf87e03f1630ff8b0.svg","fullname":"Jaehyun Park","name":"Cabbalett","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.18940","authors":[{"_id":"699e9a47dfbcf0b800aecbd8","name":"Elad Ben Avraham","hidden":false},{"_id":"699e9a47dfbcf0b800aecbd9","name":"Changhao Li","hidden":false},{"_id":"699e9a47dfbcf0b800aecbda","name":"Ron Dorfman","hidden":false},{"_id":"699e9a47dfbcf0b800aecbdb","name":"Roy Ganz","hidden":false},{"_id":"699e9a47dfbcf0b800aecbdc","name":"Oren Nuriel","hidden":false},{"_id":"699e9a47dfbcf0b800aecbdd","name":"Amir Dudai","hidden":false},{"_id":"699e9a47dfbcf0b800aecbde","name":"Aviad Aberdam","hidden":false},{"_id":"699e9a47dfbcf0b800aecbdf","name":"Noah Flynn","hidden":false},{"_id":"699e9a47dfbcf0b800aecbe0","name":"Elman Mansimov","hidden":false},{"_id":"699e9a47dfbcf0b800aecbe1","name":"Adi Kalyanpur","hidden":false},{"_id":"699e9a47dfbcf0b800aecbe2","name":"Ron Litman","hidden":false}],"publishedAt":"2026-02-21T19:14:31.000Z","submittedOnDailyAt":"2026-02-25T04:20:50.993Z","title":"DREAM: Deep Research Evaluation with Agentic Metrics","submittedOnDailyBy":{"_id":"62f0cb39671fc964b5063aba","avatarUrl":"/avatars/02404cd78c395331b42500dd6ced35eb.svg","isPro":false,"fullname":"Roy Ganz","user":"proy","type":"user"},"summary":"Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.","upvotes":10,"discussionId":"699e9a47dfbcf0b800aecbe3","ai_summary":"Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.","ai_keywords":["Deep Research Agents","analyst-grade reports","Mirage of Synthesis","tool-use capabilities","temporal validity","factual correctness","DREAM","agentic metrics","evaluation protocol","query-agnostic metrics","adaptive metrics","tool-calling agent","temporally aware coverage","grounded verification","systematic reasoning probes"],"organization":{"_id":"6058ec29102f61b42f65ae35","name":"AWS","fullname":"Amazon Web Services","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/wtuzZxWzijQ3do3zYoOFH.png"}},"publishedAt":"2026-02-21T14:14:31.000Z","title":"DREAM: Deep Research Evaluation with Agentic Metrics","summary":"Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18940.png","numComments":1,"submittedBy":{"_id":"62f0cb39671fc964b5063aba","avatarUrl":"/avatars/02404cd78c395331b42500dd6ced35eb.svg","fullname":"Roy Ganz","name":"proy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6058ec29102f61b42f65ae35","name":"AWS","fullname":"Amazon Web Services","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/wtuzZxWzijQ3do3zYoOFH.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14337","authors":[{"_id":"699cea804e37ec6dfa1bc476","name":"Yukang Feng","hidden":false},{"_id":"699cea804e37ec6dfa1bc477","name":"Jianwen Sun","hidden":false},{"_id":"699cea804e37ec6dfa1bc478","name":"Zelai Yang","hidden":false},{"_id":"699cea804e37ec6dfa1bc479","name":"Jiaxin Ai","hidden":false},{"_id":"699cea804e37ec6dfa1bc47a","name":"Chuanhao Li","hidden":false},{"_id":"699cea804e37ec6dfa1bc47b","name":"Zizhen Li","hidden":false},{"_id":"699cea804e37ec6dfa1bc47c","name":"Fanrui Zhang","hidden":false},{"_id":"699cea804e37ec6dfa1bc47d","name":"Kang He","hidden":false},{"_id":"699cea804e37ec6dfa1bc47e","name":"Rui Ma","hidden":false},{"_id":"699cea804e37ec6dfa1bc47f","name":"Jifan Lin","hidden":false},{"_id":"699cea804e37ec6dfa1bc480","name":"Jie Sun","hidden":false},{"_id":"699cea804e37ec6dfa1bc481","name":"Yang Xiao","hidden":false},{"_id":"699cea804e37ec6dfa1bc482","name":"Sizhuo Zhou","hidden":false},{"_id":"699cea804e37ec6dfa1bc483","name":"Wenxiao Wu","hidden":false},{"_id":"699cea804e37ec6dfa1bc484","name":"Yiming Liu","hidden":false},{"_id":"699cea804e37ec6dfa1bc485","name":"Pengfei Liu","hidden":false},{"_id":"699cea804e37ec6dfa1bc486","name":"Yu Qiao","hidden":false},{"_id":"699cea804e37ec6dfa1bc487","name":"Shenglin Zhang","hidden":false},{"_id":"699cea804e37ec6dfa1bc488","name":"Kaipeng Zhang","hidden":false}],"publishedAt":"2026-02-15T23:12:57.000Z","submittedOnDailyAt":"2026-02-25T01:19:35.669Z","title":"LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces","submittedOnDailyBy":{"_id":"65f1713552c38a91e0a445e8","avatarUrl":"/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg","isPro":false,"fullname":"kaipeng","user":"kpzhang996","type":"user"},"summary":"Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.","upvotes":10,"discussionId":"699cea804e37ec6dfa1bc489","projectPage":"https://github.com/finyorko/longcli-bench","githubRepo":"https://github.com/finyorko/longcli-bench","githubRepoAddedBy":"user","ai_summary":"LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.","ai_keywords":["AI-assisted programming","command-line interfaces","long-horizon planning","agent evaluation","requirement fulfillment","regression avoidance","step-level scoring","self-correction","human-agent collaboration","plan injection","interactive guidance"],"githubStars":22},"publishedAt":"2026-02-15T18:12:57.000Z","title":"LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces","summary":"Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14337.png","numComments":2,"submittedBy":{"_id":"65f1713552c38a91e0a445e8","avatarUrl":"/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg","fullname":"kaipeng","name":"kpzhang996","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.20309","authors":[{"_id":"699ecdf3dfbcf0b800aecce9","name":"Jingxuan Zhang","hidden":false},{"_id":"699ecdf3dfbcf0b800aeccea","name":"Yunta Hsieh","hidden":false},{"_id":"699ecdf3dfbcf0b800aecceb","name":"Zhongwei Wang","hidden":false},{"_id":"699ecdf3dfbcf0b800aeccec","name":"Haokun Lin","hidden":false},{"_id":"699ecdf3dfbcf0b800aecced","name":"Xin Wang","hidden":false},{"_id":"699ecdf3dfbcf0b800aeccee","name":"Ziqi Wang","hidden":false},{"_id":"699ecdf3dfbcf0b800aeccef","name":"Yingtie Lei","hidden":false},{"_id":"699ecdf3dfbcf0b800aeccf0","name":"Mi Zhang","hidden":false}],"publishedAt":"2026-02-23T19:55:54.000Z","submittedOnDailyAt":"2026-02-25T07:56:34.044Z","title":"QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models","submittedOnDailyBy":{"_id":"6841378dcb9938c7aed795be","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WNI8gQVHdWyt0OvmRINDp.png","isPro":false,"fullname":"Haokun Lin","user":"Felix1023","type":"user"},"summary":"Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.","upvotes":8,"discussionId":"699ecdf3dfbcf0b800aeccf1","projectPage":"https://quantvla.github.io/","ai_summary":"QuantVLA is a post-training quantization framework for vision-language-action models that enables efficient deployment through selective quantization, attention temperature matching, and output head balancing while maintaining performance and reducing memory and latency.","ai_keywords":["post-training quantization","diffusion transformer","attention projections","attention temperature matching","output head balancing","integer kernels","calibration buffer","end-to-end inference latency"]},"publishedAt":"2026-02-23T14:55:54.000Z","title":"QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models","summary":"Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20309.png","numComments":1,"submittedBy":{"_id":"6841378dcb9938c7aed795be","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WNI8gQVHdWyt0OvmRINDp.png","fullname":"Haokun Lin","name":"Felix1023","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.16990","authors":[{"_id":"699e71d6dfbcf0b800aecb53","user":{"_id":"65d76cc5b9b7b8bf88faa916","avatarUrl":"/avatars/d95232cd0c307efab6197ade1a66190b.svg","isPro":true,"fullname":"Yan Wang","user":"YanAdjeNole","type":"user"},"name":"Yan Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:21.546Z","hidden":false},{"_id":"699e71d6dfbcf0b800aecb54","name":"Yi Han","hidden":false},{"_id":"699e71d6dfbcf0b800aecb55","name":"Lingfei Qian","hidden":false},{"_id":"699e71d6dfbcf0b800aecb56","name":"Yueru He","hidden":false},{"_id":"699e71d6dfbcf0b800aecb57","name":"Xueqing Peng","hidden":false},{"_id":"699e71d6dfbcf0b800aecb58","name":"Dongji Feng","hidden":false},{"_id":"699e71d6dfbcf0b800aecb59","name":"Zhuohan Xie","hidden":false},{"_id":"699e71d6dfbcf0b800aecb5a","name":"Vincent Jim Zhang","hidden":false},{"_id":"699e71d6dfbcf0b800aecb5b","name":"Rosie Guo","hidden":false},{"_id":"699e71d6dfbcf0b800aecb5c","name":"Fengran Mo","hidden":false},{"_id":"699e71d6dfbcf0b800aecb5d","name":"Jimin Huang","hidden":false},{"_id":"699e71d6dfbcf0b800aecb5e","name":"Yankai Chen","hidden":false},{"_id":"699e71d6dfbcf0b800aecb5f","name":"Xue Liu","hidden":false},{"_id":"699e71d6dfbcf0b800aecb60","name":"Jian-Yun Nie","hidden":false}],"publishedAt":"2026-02-19T01:29:50.000Z","submittedOnDailyAt":"2026-02-25T01:23:32.378Z","title":"Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation","submittedOnDailyBy":{"_id":"65d76cc5b9b7b8bf88faa916","avatarUrl":"/avatars/d95232cd0c307efab6197ade1a66190b.svg","isPro":true,"fullname":"Yan Wang","user":"YanAdjeNole","type":"user"},"summary":"Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.","upvotes":8,"discussionId":"699e71d6dfbcf0b800aecb61","githubRepo":"https://github.com/The-FinAI/Conv-FinRe","githubRepoAddedBy":"user","ai_summary":"A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories.","ai_keywords":["recommendation benchmarks","large language models","financial advisory","behavioral imitation","decision quality","multi-view references","investor-specific risk preferences","rational analysis","market volatility","user behavior alignment"],"githubStars":2,"organization":{"_id":"658f4413674349122c0708e9","name":"TheFinAI","fullname":"The Fin AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"}},"publishedAt":"2026-02-18T20:29:50.000Z","title":"Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation","summary":"Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16990.png","numComments":1,"submittedBy":{"_id":"65d76cc5b9b7b8bf88faa916","avatarUrl":"/avatars/d95232cd0c307efab6197ade1a66190b.svg","fullname":"Yan Wang","name":"YanAdjeNole","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"658f4413674349122c0708e9","name":"TheFinAI","fullname":"The Fin AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.16745","authors":[{"_id":"699f27beebfce7fbcca91a00","name":"Zhangyi Liu","hidden":false},{"_id":"699f27beebfce7fbcca91a01","name":"Huaizhi Qu","hidden":false},{"_id":"699f27beebfce7fbcca91a02","name":"Xiaowei Yin","hidden":false},{"_id":"699f27beebfce7fbcca91a03","name":"He Sun","hidden":false},{"_id":"699f27beebfce7fbcca91a04","name":"Yanjun Han","hidden":false},{"_id":"699f27beebfce7fbcca91a05","name":"Tianlong Chen","hidden":false},{"_id":"699f27beebfce7fbcca91a06","name":"Zhun Deng","hidden":false}],"publishedAt":"2026-02-18T03:28:23.000Z","submittedOnDailyAt":"2026-02-25T14:21:02.010Z","title":"PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency","submittedOnDailyBy":{"_id":"6621ea7224210791d94ebde4","avatarUrl":"/avatars/6510b1faa5042755ba80cf3c54b08781.svg","isPro":false,"fullname":"Huaizhi Qu","user":"qhz991029","type":"user"},"summary":"Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.","upvotes":5,"discussionId":"699f27beebfce7fbcca91a07","githubRepo":"https://github.com/ZDCSlab/PETS","githubRepoAddedBy":"user","ai_summary":"Principled and efficient test-time self-consistency method improves model performance by optimizing trajectory allocation through an optimization framework that reduces sampling requirements while maintaining accuracy.","ai_keywords":["test-time scaling","stochastic reasoning trajectories","self-consistency rate","trajectory allocation","crowdsourcing","majority voting","offline regime","online streaming regime","test-time self-consistency"],"githubStars":2,"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}},"publishedAt":"2026-02-17T22:28:23.000Z","title":"PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency","summary":"Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16745.png","numComments":1,"submittedBy":{"_id":"6621ea7224210791d94ebde4","avatarUrl":"/avatars/6510b1faa5042755ba80cf3c54b08781.svg","fullname":"Huaizhi Qu","name":"qhz991029","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.21198","authors":[{"_id":"699e7520dfbcf0b800aecb7c","name":"Yining Hong","hidden":false},{"_id":"699e7520dfbcf0b800aecb7d","name":"Huang Huang","hidden":false},{"_id":"699e7520dfbcf0b800aecb7e","name":"Manling Li","hidden":false},{"_id":"699e7520dfbcf0b800aecb7f","name":"Li Fei-Fei","hidden":false},{"_id":"699e7520dfbcf0b800aecb80","name":"Jiajun Wu","hidden":false},{"_id":"699e7520dfbcf0b800aecb81","name":"Yejin Choi","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6431b64df76c34519e93d1ba/oSZG-BmjUOWwm3C3QdUGm.png"],"publishedAt":"2026-02-24T18:55:18.000Z","submittedOnDailyAt":"2026-02-25T01:37:43.503Z","title":"Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs","submittedOnDailyBy":{"_id":"6431b64df76c34519e93d1ba","avatarUrl":"/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg","isPro":false,"fullname":"Yining Hong","user":"evelynhong","type":"user"},"summary":"Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: reflection-in-action, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and reflection-on-action, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.","upvotes":4,"discussionId":"699e7521dfbcf0b800aecb82","projectPage":"https://reflective-test-time-planning.github.io/","githubRepo":"https://github.com/Reflective-Test-Time-Planning/Reflective-Test-Time-Planning","githubRepoAddedBy":"user","ai_summary":"Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance.","ai_keywords":["embodied LLMs","test-time planning","reflection-in-action","reflection-on-action","retrospective reflection","test-time scaling","test-time training","long-horizon task reasoning","behavioral correction"],"githubStars":0},"publishedAt":"2026-02-24T13:55:18.000Z","title":"Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs","summary":"Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: reflection-in-action, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and reflection-on-action, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6431b64df76c34519e93d1ba/oSZG-BmjUOWwm3C3QdUGm.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21198.png","numComments":1,"submittedBy":{"_id":"6431b64df76c34519e93d1ba","avatarUrl":"/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg","fullname":"Yining Hong","name":"evelynhong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.20945","authors":[{"_id":"699e6691dfbcf0b800aecadc","user":{"_id":"6621cea88850e38ffbb1854f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6621cea88850e38ffbb1854f/LeytEEjSwnnqB-zFN1Tgt.jpeg","isPro":false,"fullname":"Taki WU","user":"taki555","type":"user"},"name":"Taiqiang Wu","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:30:00.562Z","hidden":false},{"_id":"699e6691dfbcf0b800aecadd","name":"Zenan Zu","hidden":false},{"_id":"699e6691dfbcf0b800aecade","name":"Bo Zhou","hidden":false},{"_id":"699e6691dfbcf0b800aecadf","name":"Ngai Wong","hidden":false}],"publishedAt":"2026-02-24T14:28:16.000Z","submittedOnDailyAt":"2026-02-25T00:33:51.481Z","title":"The Art of Efficient Reasoning: Data, Reward, and Optimization","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.","upvotes":4,"discussionId":"699e6691dfbcf0b800aecae0","ai_summary":"Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping.","ai_keywords":["Chain-of-Thought","reinforcement learning","reward shaping","length adaptation","reasoning refinement","token budget","length collapse","generalization"],"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}},"publishedAt":"2026-02-24T09:28:16.000Z","title":"The Art of Efficient Reasoning: Data, Reward, and Optimization","summary":"Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20945.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":239,"isUserFollowing":false},"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.20731","authors":[{"_id":"699eb3f5dfbcf0b800aecc92","name":"Aram Davtyan","hidden":false},{"_id":"699eb3f5dfbcf0b800aecc93","name":"Yusuf Sahin","hidden":false},{"_id":"699eb3f5dfbcf0b800aecc94","name":"Yasaman Haghighi","hidden":false},{"_id":"699eb3f5dfbcf0b800aecc95","name":"Sebastian Stapf","hidden":false},{"_id":"699eb3f5dfbcf0b800aecc96","name":"Pablo Acuaviva","hidden":false},{"_id":"699eb3f5dfbcf0b800aecc97","name":"Alexandre Alahi","hidden":false},{"_id":"699eb3f5dfbcf0b800aecc98","name":"Paolo Favaro","hidden":false}],"publishedAt":"2026-02-24T09:53:50.000Z","submittedOnDailyAt":"2026-02-25T06:32:31.842Z","title":"Communication-Inspired Tokenization for Structured Image Representations","submittedOnDailyBy":{"_id":"655df11f82afda0fc47c421d","avatarUrl":"/avatars/f0ea4317e6010b28392d85bb94dd2230.svg","isPro":false,"fullname":"Aram Davtyan","user":"araachie","type":"user"},"summary":"Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.","upvotes":4,"discussionId":"699eb3f5dfbcf0b800aecc99","projectPage":"https://araachie.github.io/comit/","githubRepo":"https://github.com/araachie/comit","githubRepoAddedBy":"user","ai_summary":"COMiT framework learns structured discrete visual tokens through iterative encoding and flow-matching decoding, improving object-centric representation and compositional generalization.","ai_keywords":["discrete image tokenizers","vision transformers","flow-matching","semantic representation alignment","attentive sequential tokenization","object-centric token structure","compositional generalization","relational reasoning","latent message","transformer model"],"githubStars":3,"organization":{"_id":"655e6783c0a20e9dbb031680","name":"cvg-unibe","fullname":"Computer Vision Group @ University of Bern","avatar":"https://cdn-uploads.huggingface.co/production/uploads/655df11f82afda0fc47c421d/ldC46bZlib6DB8Endhy9N.png"}},"publishedAt":"2026-02-24T04:53:50.000Z","title":"Communication-Inspired Tokenization for Structured Image Representations","summary":"Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20731.png","numComments":1,"submittedBy":{"_id":"655df11f82afda0fc47c421d","avatarUrl":"/avatars/f0ea4317e6010b28392d85bb94dd2230.svg","fullname":"Aram Davtyan","name":"araachie","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"655e6783c0a20e9dbb031680","name":"cvg-unibe","fullname":"Computer Vision Group @ University of Bern","avatar":"https://cdn-uploads.huggingface.co/production/uploads/655df11f82afda0fc47c421d/ldC46bZlib6DB8Endhy9N.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.19633","authors":[{"_id":"699e2e67dfbcf0b800aeca26","user":{"_id":"654224d4eccc4f48dcea725d","avatarUrl":"/avatars/a325691bac8e3ff53dddd96c22d7fa67.svg","isPro":false,"fullname":"Jongwon Jeong","user":"jongwon-jeong","type":"user"},"name":"Jongwon Jeong","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:30:24.033Z","hidden":false},{"_id":"699e2e67dfbcf0b800aeca27","name":"Jungtaek Kim","hidden":false},{"_id":"699e2e67dfbcf0b800aeca28","name":"Kangwook Lee","hidden":false}],"publishedAt":"2026-02-23T09:19:56.000Z","submittedOnDailyAt":"2026-02-25T15:51:24.022Z","title":"TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents","submittedOnDailyBy":{"_id":"654224d4eccc4f48dcea725d","avatarUrl":"/avatars/a325691bac8e3ff53dddd96c22d7fa67.svg","isPro":false,"fullname":"Jongwon Jeong","user":"jongwon-jeong","type":"user"},"summary":"Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here.","upvotes":4,"discussionId":"699e2e67dfbcf0b800aeca29","githubRepo":"https://github.com/UW-Madison-Lee-Lab/TAPE","githubRepoAddedBy":"user","ai_summary":"TAPE framework improves language model agent performance in complex environments through enhanced planning and constrained execution strategies.","ai_keywords":["language model agents","planning","stochastic execution","constrained execution","adaptive planning","external solver","feasible path","constrained decoding","environmental feedback","agent frameworks"],"githubStars":0,"organization":{"_id":"61d090ec03bc10eb8e1c2970","name":"uw-madison","fullname":"University of Wisconsin - Madison","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}},"publishedAt":"2026-02-23T04:19:56.000Z","title":"TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents","summary":"Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19633.png","numComments":1,"submittedBy":{"_id":"654224d4eccc4f48dcea725d","avatarUrl":"/avatars/a325691bac8e3ff53dddd96c22d7fa67.svg","fullname":"Jongwon Jeong","name":"jongwon-jeong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"61d090ec03bc10eb8e1c2970","name":"uw-madison","fullname":"University of Wisconsin - Madison","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.21196","authors":[{"_id":"699e9f0bdfbcf0b800aecbf4","name":"Ravi Ghadia","hidden":false},{"_id":"699e9f0bdfbcf0b800aecbf5","name":"Maksim Abraham","hidden":false},{"_id":"699e9f0bdfbcf0b800aecbf6","name":"Sergei Vorobyov","hidden":false},{"_id":"699e9f0bdfbcf0b800aecbf7","name":"Max Ryabinin","hidden":false}],"publishedAt":"2026-02-24T18:54:39.000Z","submittedOnDailyAt":"2026-02-25T04:41:04.657Z","title":"Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking","submittedOnDailyBy":{"_id":"607d59fb921db717010c7ccc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1625736058289-607d59fb921db717010c7ccc.png","isPro":false,"fullname":"Max Ryabinin","user":"mryab","type":"user"},"summary":"Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8timesH100 node, improving upon prior methods by over 25%.","upvotes":3,"discussionId":"699e9f0bdfbcf0b800aecbf8","projectPage":"https://rghadia.github.io/untied_ulysses_proj/","githubRepo":"https://github.com/togethercomputer/Untied-Ulysses","githubRepoAddedBy":"user","ai_summary":"UPipe enables efficient processing of long sequences in Transformer models through fine-grained chunking at the attention head level, significantly reducing activation memory usage while maintaining training speed.","ai_keywords":["Transformer models","context parallelism","Ring Attention","DeepSpeed Ulysses","Fully Pipelined Distributed Transformer","activation offloading","self-attention","intermediate tensor memory usage","attention head level chunking"],"githubStars":1,"organization":{"_id":"632b803bb2dd35f135623cc2","name":"togethercomputer","fullname":"Together","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1678734258201-6322ad266b1992383fa964ca.png"}},"publishedAt":"2026-02-24T13:54:39.000Z","title":"Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking","summary":"Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8timesH100 node, improving upon prior methods by over 25%.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21196.png","numComments":1,"submittedBy":{"_id":"607d59fb921db717010c7ccc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1625736058289-607d59fb921db717010c7ccc.png","fullname":"Max Ryabinin","name":"mryab","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":30,"isUserFollowing":false},"organization":{"_id":"632b803bb2dd35f135623cc2","name":"togethercomputer","fullname":"Together","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1678734258201-6322ad266b1992383fa964ca.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.21185","authors":[{"_id":"699e7bb0dfbcf0b800aecb84","name":"Justin Deschenaux","hidden":false},{"_id":"699e7bb0dfbcf0b800aecb85","name":"Caglar Gulcehre","hidden":false},{"_id":"699e7bb0dfbcf0b800aecb86","user":{"_id":"661839d73b412cdc851299c1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/661839d73b412cdc851299c1/xicwANPQPTFdWfblisL2-.png","isPro":false,"fullname":"Subham Sekhar Sahoo","user":"s-sahoo","type":"user"},"name":"Subham Sekhar Sahoo","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:14.386Z","hidden":false}],"publishedAt":"2026-02-24T18:35:22.000Z","submittedOnDailyAt":"2026-02-25T02:03:58.889Z","title":"The Diffusion Duality, Chapter II: -Samplers and Efficient Curriculum","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2","upvotes":3,"discussionId":"699e7bb0dfbcf0b800aecb87","projectPage":"https://s-sahoo.com/duo-ch2/","ai_summary":"Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.","ai_keywords":["discrete diffusion models","predictor-corrector samplers","ancestral sampling","generative perplexity","unigram entropy","FID","IS scores","Gaussian relaxation","curriculum training","Masked diffusion"]},"publishedAt":"2026-02-24T13:35:22.000Z","title":"The Diffusion Duality, Chapter II: -Samplers and Efficient Curriculum","summary":"Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21185.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":239,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.20424","authors":[{"_id":"699e70a9dfbcf0b800aecb2d","name":"Ved Sirdeshmukh","hidden":false},{"_id":"699e70a9dfbcf0b800aecb2e","name":"Marc Wetter","hidden":false}],"publishedAt":"2026-02-23T23:46:55.000Z","submittedOnDailyAt":"2026-02-25T01:17:17.713Z","title":"Implicit Intelligence -- Evaluating Agents on What Users Don't Say","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.","upvotes":3,"discussionId":"699e70aadfbcf0b800aecb2f","ai_summary":"AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.","ai_keywords":["AI agents","implicit requirements","contextual reasoning","instruction-following","evaluation framework","Agent-as-a-World","YAML files","language models","interactive worlds","scenario pass rate"]},"publishedAt":"2026-02-23T18:46:55.000Z","title":"Implicit Intelligence -- Evaluating Agents on What Users Don't Say","summary":"Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20424.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":239,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.18998","authors":[{"_id":"699f4416ebfce7fbcca91a1b","name":"Xiaochuan Li","hidden":false},{"_id":"699f4416ebfce7fbcca91a1c","name":"Ryan Ming","hidden":false},{"_id":"699f4416ebfce7fbcca91a1d","name":"Pranav Setlur","hidden":false},{"_id":"699f4416ebfce7fbcca91a1e","name":"Abhijay Paladugu","hidden":false},{"_id":"699f4416ebfce7fbcca91a1f","name":"Andy Tang","hidden":false},{"_id":"699f4416ebfce7fbcca91a20","name":"Hao Kang","hidden":false},{"_id":"699f4416ebfce7fbcca91a21","name":"Shuai Shao","hidden":false},{"_id":"699f4416ebfce7fbcca91a22","name":"Rong Jin","hidden":false},{"_id":"699f4416ebfce7fbcca91a23","name":"Chenyan Xiong","hidden":false}],"publishedAt":"2026-02-22T01:08:02.000Z","submittedOnDailyAt":"2026-02-25T16:20:26.014Z","title":"Benchmark Test-Time Scaling of General LLM Agents","submittedOnDailyBy":{"_id":"64b103cf372d434077206750","avatarUrl":"/avatars/ba0eb4fc712a8b9b93ceb30d11859ec2.svg","isPro":true,"fullname":"Xiaochuan Li","user":"lixiaochuan2020","type":"user"},"summary":"LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.","upvotes":3,"discussionId":"699f4416ebfce7fbcca91a24","projectPage":"https://general-agentbench.github.io/","githubRepo":"https://github.com/cxcscmu/General-AgentBench","githubRepoAddedBy":"user","ai_summary":"General AgentBench evaluates large language model agents across multiple domains and scaling methods, revealing performance degradation and fundamental limitations in sequential and parallel scaling approaches.","ai_keywords":["LLM agents","general-purpose systems","domain-aware environments","unified environment","test-time scaling","sequential scaling","parallel scaling","context ceiling","verification gap"],"githubStars":6,"organization":{"_id":"6474ab086d4dda6f7c6beaee","name":"cmu-lti","fullname":"CMU-LTI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63a4d079658851481f7e4394/HOFKzjMXriLBOGJw6S8uI.png"}},"publishedAt":"2026-02-21T20:08:02.000Z","title":"Benchmark Test-Time Scaling of General LLM Agents","summary":"LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18998.png","numComments":1,"submittedBy":{"_id":"64b103cf372d434077206750","avatarUrl":"/avatars/ba0eb4fc712a8b9b93ceb30d11859ec2.svg","fullname":"Xiaochuan Li","name":"lixiaochuan2020","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"6474ab086d4dda6f7c6beaee","name":"cmu-lti","fullname":"CMU-LTI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63a4d079658851481f7e4394/HOFKzjMXriLBOGJw6S8uI.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.16932","authors":[{"_id":"699b8dbaf723198bd53a62d6","name":"Jinming Nian","hidden":false},{"_id":"699b8dbaf723198bd53a62d7","name":"Fangchen Li","hidden":false},{"_id":"699b8dbaf723198bd53a62d8","name":"Dae Hoon Park","hidden":false},{"_id":"699b8dbaf723198bd53a62d9","name":"Yi Fang","hidden":false}],"publishedAt":"2026-02-18T22:53:18.000Z","submittedOnDailyAt":"2026-02-25T16:58:05.672Z","title":"RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution","submittedOnDailyBy":{"_id":"64aebfa77b3b338666286b5b","avatarUrl":"/avatars/75d23cab7e6912dcf6a6602c994853a3.svg","isPro":false,"fullname":"Jinming Nian","user":"jnian","type":"user"},"summary":"Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.","upvotes":3,"discussionId":"699b8dbaf723198bd53a62da","githubRepo":"https://github.com/fangchenli/ranking-evolved","githubRepoAddedBy":"user","ai_summary":"Large language models guided by evaluators and evolutionary search can automatically discover improved lexical retrieval algorithms through program evolution techniques.","ai_keywords":["retrieval algorithms","BM25","query likelihood","Dirichlet smoothing","large language models","evaluator-guided","evolutionary search","program evolution","AlphaEvolve","ranking algorithms","IR datasets","BEIR","BRIGHT","TREC DL"],"githubStars":4,"organization":{"_id":"67b90a8f4535b84d0b6630fb","name":"SCU-IR","fullname":"Santa Clara University IR Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63656d3112188d67e659a85c/u0yiG_YKp8nZKt0V6YI3F.jpeg"}},"publishedAt":"2026-02-18T17:53:18.000Z","title":"RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution","summary":"Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16932.png","numComments":1,"submittedBy":{"_id":"64aebfa77b3b338666286b5b","avatarUrl":"/avatars/75d23cab7e6912dcf6a6602c994853a3.svg","fullname":"Jinming Nian","name":"jnian","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67b90a8f4535b84d0b6630fb","name":"SCU-IR","fullname":"Santa Clara University IR Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63656d3112188d67e659a85c/u0yiG_YKp8nZKt0V6YI3F.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.16813","authors":[{"_id":"699da378b767f258d0bf1a82","name":"Chanhyuk Lee","hidden":false},{"_id":"699da378b767f258d0bf1a83","name":"Jaehoon Yoo","hidden":false},{"_id":"699da378b767f258d0bf1a84","name":"Manan Agarwal","hidden":false},{"_id":"699da378b767f258d0bf1a85","name":"Sheel Shah","hidden":false},{"_id":"699da378b767f258d0bf1a86","name":"Jerry Huang","hidden":false},{"_id":"699da378b767f258d0bf1a87","name":"Aditi Raghunathan","hidden":false},{"_id":"699da378b767f258d0bf1a88","name":"Seunghoon Hong","hidden":false},{"_id":"699da378b767f258d0bf1a89","name":"Nicholas M. Boffi","hidden":false},{"_id":"699da378b767f258d0bf1a8a","name":"Jinwoo Kim","hidden":false}],"publishedAt":"2026-02-18T19:23:07.000Z","submittedOnDailyAt":"2026-02-25T04:51:51.814Z","title":"One-step Language Modeling via Continuous Denoising","submittedOnDailyBy":{"_id":"63e839ede02ee67e8e58eaf6","avatarUrl":"/avatars/72512273f0e1a4469a2be16c9cfacaa1.svg","isPro":false,"fullname":"Chanhyuk David Lee","user":"david3684","type":"user"},"summary":"Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.","upvotes":3,"discussionId":"699da378b767f258d0bf1a8d","projectPage":"https://one-step-lm.github.io/","githubRepo":"https://github.com/david3684/flm","githubRepoAddedBy":"user","ai_summary":"Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization.","ai_keywords":["language models","discrete diffusion","flow-based continuous denoising","Euclidean denoising","one-hot token encodings","cross entropy objective","time reparameterization","distilled flow map","few-step generation","discrete modalities","generative modeling"],"githubStars":43},"publishedAt":"2026-02-18T14:23:07.000Z","title":"One-step Language Modeling via Continuous Denoising","summary":"Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16813.png","numComments":1,"submittedBy":{"_id":"63e839ede02ee67e8e58eaf6","avatarUrl":"/avatars/72512273f0e1a4469a2be16c9cfacaa1.svg","fullname":"Chanhyuk David Lee","name":"david3684","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.21201","authors":[{"_id":"699e7101dfbcf0b800aecb40","name":"Tony Feng","hidden":false},{"_id":"699e7101dfbcf0b800aecb41","name":"Junehyuk Jung","hidden":false},{"_id":"699e7101dfbcf0b800aecb42","name":"Sang-hyun Kim","hidden":false},{"_id":"699e7101dfbcf0b800aecb43","name":"Carlo Pagano","hidden":false},{"_id":"699e7101dfbcf0b800aecb44","name":"Sergei Gukov","hidden":false},{"_id":"699e7101dfbcf0b800aecb45","name":"Chiang-Chiang Tsai","hidden":false},{"_id":"699e7101dfbcf0b800aecb46","name":"David Woodruff","hidden":false},{"_id":"699e7101dfbcf0b800aecb47","name":"Adel Javanmard","hidden":false},{"_id":"699e7101dfbcf0b800aecb48","name":"Aryan Mokhtari","hidden":false},{"_id":"699e7101dfbcf0b800aecb49","name":"Dawsen Hwang","hidden":false},{"_id":"699e7101dfbcf0b800aecb4a","name":"Yuri Chervonyi","hidden":false},{"_id":"699e7101dfbcf0b800aecb4b","name":"Jonathan N. Lee","hidden":false},{"_id":"699e7101dfbcf0b800aecb4c","name":"Garrett Bingham","hidden":false},{"_id":"699e7101dfbcf0b800aecb4d","name":"Trieu H. Trinh","hidden":false},{"_id":"699e7101dfbcf0b800aecb4e","name":"Vahab Mirrokni","hidden":false},{"_id":"699e7101dfbcf0b800aecb4f","name":"Quoc V. Le","hidden":false},{"_id":"699e7101dfbcf0b800aecb50","name":"Thang Luong","hidden":false}],"publishedAt":"2026-02-24T18:56:10.000Z","submittedOnDailyAt":"2026-02-25T01:42:48.338Z","title":"Aletheia tackles FirstProof autonomously","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.","upvotes":2,"discussionId":"699e7101dfbcf0b800aecb51","projectPage":"https://github.com/google-deepmind/superhuman/tree/main/aletheia","organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-02-24T13:56:10.000Z","title":"Aletheia tackles FirstProof autonomously","summary":"We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21201.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":239,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.20792","authors":[{"_id":"699f3fd0ebfce7fbcca91a17","name":"Muhammad Saif Ullah Khan","hidden":false},{"_id":"699f3fd0ebfce7fbcca91a18","name":"Didier Stricker","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64da1dd841f0e6c0e9638b67/HKUZc-i-3K_SdOAVzTR3D.mp4"],"publishedAt":"2026-02-24T11:31:20.000Z","submittedOnDailyAt":"2026-02-25T16:12:38.891Z","title":"SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking","submittedOnDailyBy":{"_id":"64da1dd841f0e6c0e9638b67","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/AVclbx51CufhItKagXkRi.png","isPro":false,"fullname":"Saif Khan","user":"saifkhichi96","type":"user"},"summary":"Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.","upvotes":2,"discussionId":"699f3fd0ebfce7fbcca91a19","projectPage":"https://saifkhichi.com/research/simspine","githubRepo":"https://github.com/dfki-av/simspine","githubRepoAddedBy":"user","ai_summary":"A biomechanics-aware keypoint simulation framework and the first open dataset, SIMSPINE, provide anatomically consistent 3D spinal annotations for natural full-body motions, enabling data-driven learning of vertebral kinematics and improving spine motion estimation accuracy.","ai_keywords":["biomechanics-aware keypoint simulation framework","musculoskeletal modeling","3D spinal annotations","SIMSPINE dataset","2D detectors","3D pose lifting models","multi-view reconstruction pipelines","vertebral kinematics","digital human modeling"],"githubStars":0,"organization":{"_id":"68e3c1b372607eeeeeaa9662","name":"dfki-av","fullname":"Augmented Vision","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64da1dd841f0e6c0e9638b67/k24siR3YWnto5sMIRAwu9.png"}},"publishedAt":"2026-02-24T06:31:20.000Z","title":"SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking","summary":"Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64da1dd841f0e6c0e9638b67/HKUZc-i-3K_SdOAVzTR3D.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20792.png","numComments":1,"submittedBy":{"_id":"64da1dd841f0e6c0e9638b67","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/AVclbx51CufhItKagXkRi.png","fullname":"Saif Khan","name":"saifkhichi96","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"68e3c1b372607eeeeeaa9662","name":"dfki-av","fullname":"Augmented Vision","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64da1dd841f0e6c0e9638b67/k24siR3YWnto5sMIRAwu9.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.21053","authors":[{"_id":"699e989bdfbcf0b800aecbc6","name":"Shimin Wen","hidden":false},{"_id":"699e989bdfbcf0b800aecbc7","user":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"name":"Zeyu Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:02.490Z","hidden":false},{"_id":"699e989bdfbcf0b800aecbc8","name":"Xingdou Bian","hidden":false},{"_id":"699e989bdfbcf0b800aecbc9","name":"Hongjie Zhu","hidden":false},{"_id":"699e989bdfbcf0b800aecbca","name":"Lulu He","hidden":false},{"_id":"699e989bdfbcf0b800aecbcb","name":"Layi Shama","hidden":false},{"_id":"699e989bdfbcf0b800aecbcc","name":"Daji Ergu","hidden":false},{"_id":"699e989bdfbcf0b800aecbcd","name":"Ying Cai","hidden":false}],"publishedAt":"2026-02-24T16:10:27.000Z","submittedOnDailyAt":"2026-02-25T04:07:41.939Z","title":"OCR-Agent: Agentic OCR with Capability and Memory Reflection","submittedOnDailyBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"summary":"Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.","upvotes":1,"discussionId":"699e989bdfbcf0b800aecbce","githubRepo":"https://github.com/AIGeeksGroup/OCR-Agent","githubRepoAddedBy":"user","ai_summary":"A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks.","ai_keywords":["vision-language models","iterative optimization","self-correction mechanisms","capability reflection","memory reflection","re-reasoning","OCRBench v2","Visual Understanding","Reasoning"],"githubStars":1,"organization":{"_id":"686804a93cd67b3369df8182","name":"AIGeeksGroup","fullname":"AI Geeks","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/cyfudFg0OeqgwcLHwjeCc.png"}},"publishedAt":"2026-02-24T11:10:27.000Z","title":"OCR-Agent: Agentic OCR with Capability and Memory Reflection","summary":"Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21053.png","numComments":1,"submittedBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","fullname":"Zeyu Zhang","name":"SteveZeyuZhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"686804a93cd67b3369df8182","name":"AIGeeksGroup","fullname":"AI Geeks","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/cyfudFg0OeqgwcLHwjeCc.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.21042","authors":[{"_id":"699e984fdfbcf0b800aecbbc","name":"Bonan Liu","hidden":false},{"_id":"699e984fdfbcf0b800aecbbd","user":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"name":"Zeyu Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:06.050Z","hidden":false},{"_id":"699e984fdfbcf0b800aecbbe","name":"Bingbing Meng","hidden":false},{"_id":"699e984fdfbcf0b800aecbbf","name":"Han Wang","hidden":false},{"_id":"699e984fdfbcf0b800aecbc0","name":"Hanshuo Zhang","hidden":false},{"_id":"699e984fdfbcf0b800aecbc1","name":"Chengping Wang","hidden":false},{"_id":"699e984fdfbcf0b800aecbc2","name":"Daji Ergu","hidden":false},{"_id":"699e984fdfbcf0b800aecbc3","name":"Ying Cai","hidden":false}],"publishedAt":"2026-02-24T16:02:49.000Z","submittedOnDailyAt":"2026-02-25T04:06:26.796Z","title":"OmniOCR: Generalist OCR for Ethnic Minority Languages","submittedOnDailyBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"summary":"Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.","upvotes":1,"discussionId":"699e9850dfbcf0b800aecbc4","githubRepo":"https://github.com/AIGeeksGroup/OmniOCR","githubRepoAddedBy":"user","ai_summary":"OmniOCR presents a universal framework for ethnic minority scripts using Dynamic LoRA and sparsity regularization to achieve state-of-the-art accuracy with improved parameter efficiency in low-resource settings.","ai_keywords":["Dynamic Low-Rank Adaptation","Dynamic LoRA","sparsity regularization","zero-shot foundation models","post training","parameter efficiency"],"githubStars":1,"organization":{"_id":"686804a93cd67b3369df8182","name":"AIGeeksGroup","fullname":"AI Geeks","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/cyfudFg0OeqgwcLHwjeCc.png"}},"publishedAt":"2026-02-24T11:02:49.000Z","title":"OmniOCR: Generalist OCR for Ethnic Minority Languages","summary":"Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21042.png","numComments":1,"submittedBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","fullname":"Zeyu Zhang","name":"SteveZeyuZhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"686804a93cd67b3369df8182","name":"AIGeeksGroup","fullname":"AI Geeks","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/cyfudFg0OeqgwcLHwjeCc.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.20743","authors":[{"_id":"699eb3addfbcf0b800aecc8b","user":{"_id":"661d6d36a3e99b518d6b9a71","avatarUrl":"/avatars/4135e3ce0ae64dc62ccabf1e534f98bd.svg","isPro":false,"fullname":"Gabriel Loiseau","user":"gabrielloiseau","type":"user"},"name":"Gabriel Loiseau","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:28:25.687Z","hidden":false},{"_id":"699eb3addfbcf0b800aecc8c","name":"Damien Sileo","hidden":false},{"_id":"699eb3addfbcf0b800aecc8d","name":"Damien Riquet","hidden":false},{"_id":"699eb3addfbcf0b800aecc8e","name":"Maxime Meyer","hidden":false},{"_id":"699eb3addfbcf0b800aecc8f","name":"Marc Tommasi","hidden":false}],"publishedAt":"2026-02-24T10:12:40.000Z","submittedOnDailyAt":"2026-02-25T06:05:40.014Z","title":"Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization","submittedOnDailyBy":{"_id":"661d6d36a3e99b518d6b9a71","avatarUrl":"/avatars/4135e3ce0ae64dc62ccabf1e534f98bd.svg","isPro":false,"fullname":"Gabriel Loiseau","user":"gabrielloiseau","type":"user"},"summary":"Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.","upvotes":1,"discussionId":"699eb3addfbcf0b800aecc90","ai_summary":"Adaptive text anonymization framework automatically adjusts anonymization strategies based on privacy-utility requirements using prompt optimization for language models across diverse domains and constraints.","ai_keywords":["text anonymization","privacy-utility trade-off","language models","prompt optimization","task-specific adaptation","benchmark evaluation"],"organization":{"_id":"648b669e412f81cc0c21a2d7","name":"inria-lille","fullname":"Inria Lille","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5fc0bcb41160c47d1d43856b/p5JUrvHXupxk2r-XlMA1g.png"}},"publishedAt":"2026-02-24T05:12:40.000Z","title":"Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization","summary":"Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20743.png","numComments":1,"submittedBy":{"_id":"661d6d36a3e99b518d6b9a71","avatarUrl":"/avatars/4135e3ce0ae64dc62ccabf1e534f98bd.svg","fullname":"Gabriel Loiseau","name":"gabrielloiseau","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"648b669e412f81cc0c21a2d7","name":"inria-lille","fullname":"Inria Lille","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5fc0bcb41160c47d1d43856b/p5JUrvHXupxk2r-XlMA1g.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.19020","authors":[{"_id":"699f48bfebfce7fbcca91a26","name":"Junjie Oscar Yin","hidden":false},{"_id":"699f48bfebfce7fbcca91a27","name":"John X. Morris","hidden":false},{"_id":"699f48bfebfce7fbcca91a28","name":"Vitaly Shmatikov","hidden":false},{"_id":"699f48bfebfce7fbcca91a29","name":"Sewon Min","hidden":false},{"_id":"699f48bfebfce7fbcca91a2a","name":"Hannaneh Hajishirzi","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/646fe7f4036229a8a71c318d/76LUuAIrFoW8Hv9-08ZvA.jpeg"],"publishedAt":"2026-02-22T03:20:06.000Z","submittedOnDailyAt":"2026-02-25T16:40:52.960Z","title":"Learning to Detect Language Model Training Data via Active Reconstruction","submittedOnDailyBy":{"_id":"646fe7f4036229a8a71c318d","avatarUrl":"/avatars/469b5d4ce2bb8ce0a28a2770f7a0e04a.svg","isPro":false,"fullname":"Junjie Oscar Yin","user":"osieosie","type":"user"},"summary":"Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce Active Data Reconstruction Attack (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are more reconstructible than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, ADRA and its adaptive variant ADRA+, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\\% over the previous runner-up. In particular, \\MethodPlus~improves over Min-K\\%++ by 18.8\\% on BookMIA for pre-training detection and by 7.6\\% on AIME for post-training detection.","upvotes":1,"discussionId":"699f48bfebfce7fbcca91a2b","projectPage":"https://huggingface.co/ADRA-RL","githubRepo":"https://github.com/oseyosey/MIA-RL","githubRepoAddedBy":"user","ai_summary":"Active Data Reconstruction Attack uses reinforcement learning to identify training data by measuring the reconstructibility of text from model behavior, outperforming existing membership inference attacks.","ai_keywords":["membership inference attack","active data reconstruction attack","reinforcement learning","on-policy RL","fine-tuning","reconstruction metrics","contrastive rewards","pre-training","post-training","distillation data"],"githubStars":0,"organization":{"_id":"646d31f04e59b82995e17a8a","name":"uwnlp","fullname":"University of Washington NLP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61a8f52d89ea472e600a0a10/r5imF4kxbsc1l2F1lqz2V.png"}},"publishedAt":"2026-02-21T22:20:06.000Z","title":"Learning to Detect Language Model Training Data via Active Reconstruction","summary":"Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce Active Data Reconstruction Attack (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are more reconstructible than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, ADRA and its adaptive variant ADRA+, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\\% over the previous runner-up. In particular, \\MethodPlus~improves over Min-K\\%++ by 18.8\\% on BookMIA for pre-training detection and by 7.6\\% on AIME for post-training detection.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/646fe7f4036229a8a71c318d/76LUuAIrFoW8Hv9-08ZvA.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19020.png","numComments":1,"submittedBy":{"_id":"646fe7f4036229a8a71c318d","avatarUrl":"/avatars/469b5d4ce2bb8ce0a28a2770f7a0e04a.svg","fullname":"Junjie Oscar Yin","name":"osieosie","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"646d31f04e59b82995e17a8a","name":"uwnlp","fullname":"University of Washington NLP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61a8f52d89ea472e600a0a10/r5imF4kxbsc1l2F1lqz2V.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.18735","authors":[{"_id":"699d85ab9491ce7a9c6889ef","user":{"_id":"668192a3f35c3ff47a8438ee","avatarUrl":"/avatars/6b293341f5dc51f574252c6f57cfd293.svg","isPro":false,"fullname":"Weilong Yan","user":"DavidYan2001","type":"user"},"name":"Weilong Yan","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:31:13.336Z","hidden":false},{"_id":"699d85ab9491ce7a9c6889f0","name":"Haipeng Li","hidden":false},{"_id":"699d85ab9491ce7a9c6889f1","name":"Hao Xu","hidden":false},{"_id":"699d85ab9491ce7a9c6889f2","name":"Nianjin Ye","hidden":false},{"_id":"699d85ab9491ce7a9c6889f3","name":"Yihao Ai","hidden":false},{"_id":"699d85ab9491ce7a9c6889f4","name":"Shuaicheng Liu","hidden":false},{"_id":"699d85ab9491ce7a9c6889f5","name":"Jingyu Hu","hidden":false}],"publishedAt":"2026-02-21T06:55:28.000Z","submittedOnDailyAt":"2026-02-25T04:52:26.065Z","title":"LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency","submittedOnDailyBy":{"_id":"668192a3f35c3ff47a8438ee","avatarUrl":"/avatars/6b293341f5dc51f574252c6f57cfd293.svg","isPro":false,"fullname":"Weilong Yan","user":"DavidYan2001","type":"user"},"summary":"This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First,  harnesses these powerful generative priors for completion through a complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at https://github.com/DavidYan2001/LaS-Comp{LaS-Comp}.","upvotes":1,"discussionId":"699d85ab9491ce7a9c6889f6","githubRepo":"https://github.com/DavidYan2001/LaS-Comp","githubRepoAddedBy":"user","ai_summary":"LaS-Comp presents a zero-shot 3D shape completion method using 3D foundation models with a two-stage approach for faithful reconstruction and seamless boundary refinement.","ai_keywords":["3D shape completion","3D foundation models","geometric priors","zero-shot","category-agnostic","two-stage design","explicit replacement stage","implicit refinement stage","training-free","Omni-Comp","comprehensive benchmark"],"githubStars":12,"organization":{"_id":"6508ab2b349930913196378b","name":"NationalUniversityofSingapore","fullname":"National University of Singapore","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}},"publishedAt":"2026-02-21T01:55:28.000Z","title":"LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency","summary":"This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First,  harnesses these powerful generative priors for completion through a complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at https://github.com/DavidYan2001/LaS-Comp{LaS-Comp}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18735.png","numComments":1,"submittedBy":{"_id":"668192a3f35c3ff47a8438ee","avatarUrl":"/avatars/6b293341f5dc51f574252c6f57cfd293.svg","fullname":"Weilong Yan","name":"DavidYan2001","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6508ab2b349930913196378b","name":"NationalUniversityofSingapore","fullname":"National University of Singapore","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.16603","authors":[{"_id":"699c66c0cf1450b05134df07","user":{"_id":"662dfa31d9b837e4b93dcf11","avatarUrl":"/avatars/470e738a720660f6127aa09f09e0a880.svg","isPro":false,"fullname":"hsiehchiachi","user":"hsiehchiachi","type":"user"},"name":"Chia-chi Hsieh","status":"claimed_verified","statusLastChangedAt":"2026-02-24T09:51:32.902Z","hidden":false},{"_id":"699c66c0cf1450b05134df08","name":"Zan Zong","hidden":false},{"_id":"699c66c0cf1450b05134df09","name":"Xinyang Chen","hidden":false},{"_id":"699c66c0cf1450b05134df0a","name":"Jianjiang Li","hidden":false},{"_id":"699c66c0cf1450b05134df0b","name":"Jidong Zhai","hidden":false},{"_id":"699c66c0cf1450b05134df0c","name":"Lijie Wen","hidden":false}],"publishedAt":"2026-02-18T16:57:45.000Z","submittedOnDailyAt":"2026-02-25T01:01:54.235Z","title":"FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving","submittedOnDailyBy":{"_id":"662dfa31d9b837e4b93dcf11","avatarUrl":"/avatars/470e738a720660f6127aa09f09e0a880.svg","isPro":false,"fullname":"hsiehchiachi","user":"hsiehchiachi","type":"user"},"summary":"The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6times compared to state-of-the-art systems while satisfying heterogeneous SLOs.","upvotes":1,"discussionId":"699c66c0cf1450b05134df0d","githubRepo":"https://github.com/HSIEHCHIACHI/FlowPrefill","githubRepoAddedBy":"user","ai_summary":"FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems.","ai_keywords":["large language models","prefill phase","head-of-line blocking","time-to-first-token","preemption granularity","scheduling frequency","operator-level preemption","event-driven scheduling","goodput","service level objectives"],"githubStars":4},"publishedAt":"2026-02-18T11:57:45.000Z","title":"FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving","summary":"The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6times compared to state-of-the-art systems while satisfying heterogeneous SLOs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16603.png","numComments":1,"submittedBy":{"_id":"662dfa31d9b837e4b93dcf11","avatarUrl":"/avatars/470e738a720660f6127aa09f09e0a880.svg","fullname":"hsiehchiachi","name":"hsiehchiachi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.20903","authors":[{"_id":"699ea351dfbcf0b800aecc1a","user":{"_id":"662213b3c819b7ce3d41f987","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/662213b3c819b7ce3d41f987/cSdLQy5YbbaTokzKXMtrS.jpeg","isPro":false,"fullname":"Hanshen Zhu","user":"CIawevy","type":"user"},"name":"Hanshen Zhu","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:28:42.893Z","hidden":false},{"_id":"699ea351dfbcf0b800aecc1b","name":"Yuliang Liu","hidden":false},{"_id":"699ea351dfbcf0b800aecc1c","name":"Xuecheng Wu","hidden":false},{"_id":"699ea351dfbcf0b800aecc1d","name":"An-Lan Wang","hidden":false},{"_id":"699ea351dfbcf0b800aecc1e","name":"Hao Feng","hidden":false},{"_id":"699ea351dfbcf0b800aecc1f","name":"Dingkang Yang","hidden":false},{"_id":"699ea351dfbcf0b800aecc20","name":"Chao Feng","hidden":false},{"_id":"699ea351dfbcf0b800aecc21","name":"Can Huang","hidden":false},{"_id":"699ea351dfbcf0b800aecc22","name":"Jingqun Tang","hidden":false},{"_id":"699ea351dfbcf0b800aecc23","name":"Xiang Bai","hidden":false}],"publishedAt":"2026-02-24T13:40:23.000Z","submittedOnDailyAt":"2026-02-25T04:57:08.395Z","title":"TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering","submittedOnDailyBy":{"_id":"662213b3c819b7ce3d41f987","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/662213b3c819b7ce3d41f987/cSdLQy5YbbaTokzKXMtrS.jpeg","isPro":false,"fullname":"Hanshen Zhu","user":"CIawevy","type":"user"},"summary":"Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.","upvotes":0,"discussionId":"699ea351dfbcf0b800aecc24","projectPage":"https://github.com/CIawevy/TextPecker","githubRepo":"https://github.com/CIawevy/TextPecker","githubRepoAddedBy":"user","ai_summary":"TextPecker enhances visual text rendering by addressing structural anomalies through a reinforcement learning approach that improves text-to-image generation quality and fidelity.","ai_keywords":["visual text rendering","text-to-image generation","structural anomalies","reinforcement learning","textPecker","character-level annotations","stroke-editing synthesis","structural fidelity","semantic alignment"],"githubStars":4},"publishedAt":"2026-02-24T08:40:23.000Z","title":"TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering","summary":"Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20903.png","numComments":1,"submittedBy":{"_id":"662213b3c819b7ce3d41f987","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/662213b3c819b7ce3d41f987/cSdLQy5YbbaTokzKXMtrS.jpeg","fullname":"Hanshen Zhu","name":"CIawevy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.20540","authors":[{"_id":"699e89c1dfbcf0b800aecb9b","name":"Minseop Kim","hidden":false},{"_id":"699e89c1dfbcf0b800aecb9c","name":"Takhyeong Kim","hidden":false},{"_id":"699e89c1dfbcf0b800aecb9d","user":{"_id":"64ba75761d0a5a5760874197","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ba75761d0a5a5760874197/5V3jxX-vim-wMaU2Zjz5h.jpeg","isPro":false,"fullname":"TaekHyunPark","user":"Thrillcrazyer","type":"user"},"name":"Taekhyun Park","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:29:08.802Z","hidden":false},{"_id":"699e89c1dfbcf0b800aecb9e","name":"Hanbyeol Park","hidden":false},{"_id":"699e89c1dfbcf0b800aecb9f","name":"Hyerim Bae","hidden":false}],"publishedAt":"2026-02-24T04:38:31.000Z","submittedOnDailyAt":"2026-02-25T03:47:43.183Z","title":"Generative AI and Machine Learning Collaboration for Container Dwell Time Prediction via Data Standardization","submittedOnDailyBy":{"_id":"64ba75761d0a5a5760874197","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ba75761d0a5a5760874197/5V3jxX-vim-wMaU2Zjz5h.jpeg","isPro":false,"fullname":"TaekHyunPark","user":"Thrillcrazyer","type":"user"},"summary":"Import container dwell time (ICDT) prediction is a key task for improving productivity in container terminals, as accurate predictions enable the reduction of container re-handling operations by yard cranes. Achieving this objective requires accurately predicting the dwell time of individual containers. However, the primary determinants of dwell time-owner information and cargo information-are recorded as unstructured text, which limits their effective use in machine learning models. This study addresses this limitation by proposing a collaborative framework that integrates generative artificial intelligence (Gen AI) with machine learning. The proposed framework employs Gen AI to standardize unstructured information into standard international codes, with dynamic re-prediction triggered by electronic data interchange state updates, enabling the machine learning model to predict ICDT accurately. Extensive experiments conducted on real container terminal data demonstrate that the proposed methodology achieves a 13.88% improvement in mean absolute error compared to conventional models that do not utilize standardized information. Furthermore, applying the improved predictions to container stacking strategies achieves up to 14.68% reduction in the number of relocations, thereby empirically validating the potential of Gen AI to enhance productivity in container terminal operations. Overall, this study provides both technical and methodological insights into the adoption of Gen AI in port logistics and its effectiveness.","upvotes":0,"discussionId":"699e89c2dfbcf0b800aecba0","ai_summary":"A collaborative framework integrating generative artificial intelligence with machine learning improves container dwell time prediction by standardizing unstructured text data, leading to reduced rehandling operations in container terminals.","ai_keywords":["generative artificial intelligence","machine learning","unstructured text","electronic data interchange","container dwell time","re-handling operations","container stacking strategies"],"organization":{"_id":"6902caeadf78e6ca12c2a398","name":"BAELABPNU","fullname":"BIGDATA ANALYTICS ENGINEERING LAB, Pusan National University, Busan, Korea","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6902c9cb9427990a4948a33e/L59exY-PO66fQXk3lNwQ-.png"}},"publishedAt":"2026-02-23T23:38:31.000Z","title":"Generative AI and Machine Learning Collaboration for Container Dwell Time Prediction via Data Standardization","summary":"Import container dwell time (ICDT) prediction is a key task for improving productivity in container terminals, as accurate predictions enable the reduction of container re-handling operations by yard cranes. Achieving this objective requires accurately predicting the dwell time of individual containers. However, the primary determinants of dwell time-owner information and cargo information-are recorded as unstructured text, which limits their effective use in machine learning models. This study addresses this limitation by proposing a collaborative framework that integrates generative artificial intelligence (Gen AI) with machine learning. The proposed framework employs Gen AI to standardize unstructured information into standard international codes, with dynamic re-prediction triggered by electronic data interchange state updates, enabling the machine learning model to predict ICDT accurately. Extensive experiments conducted on real container terminal data demonstrate that the proposed methodology achieves a 13.88% improvement in mean absolute error compared to conventional models that do not utilize standardized information. Furthermore, applying the improved predictions to container stacking strategies achieves up to 14.68% reduction in the number of relocations, thereby empirically validating the potential of Gen AI to enhance productivity in container terminal operations. Overall, this study provides both technical and methodological insights into the adoption of Gen AI in port logistics and its effectiveness.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20540.png","numComments":1,"submittedBy":{"_id":"64ba75761d0a5a5760874197","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ba75761d0a5a5760874197/5V3jxX-vim-wMaU2Zjz5h.jpeg","fullname":"TaekHyunPark","name":"Thrillcrazyer","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"6902caeadf78e6ca12c2a398","name":"BAELABPNU","fullname":"BIGDATA ANALYTICS ENGINEERING LAB, Pusan National University, Busan, Korea","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6902c9cb9427990a4948a33e/L59exY-PO66fQXk3lNwQ-.png"},"isAuthorParticipating":true}]
[{"paper":{"id":"2601.18491","authors":[{"_id":"697831d9026bdf0473116e5c","name":"Dongrui Liu","hidden":false},{"_id":"697831d9026bdf0473116e5d","user":{"_id":"66e2624a436a1798365e4581","avatarUrl":"/avatars/6c605807d34faa8fb505e135a4b47776.svg","isPro":false,"fullname":"Qihan Ren","user":"jasonrqh","type":"user"},"name":"Qihan Ren","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:31:15.765Z","hidden":false},{"_id":"697831d9026bdf0473116e5e","name":"Chen Qian","hidden":false},{"_id":"697831d9026bdf0473116e5f","name":"Shuai Shao","hidden":false},{"_id":"697831d9026bdf0473116e60","name":"Yuejin Xie","hidden":false},{"_id":"697831d9026bdf0473116e61","name":"Yu Li","hidden":false},{"_id":"697831d9026bdf0473116e62","name":"Zhonghao Yang","hidden":false},{"_id":"697831d9026bdf0473116e63","name":"Haoyu Luo","hidden":false},{"_id":"697831d9026bdf0473116e64","name":"Peng Wang","hidden":false},{"_id":"697831d9026bdf0473116e65","name":"Qingyu Liu","hidden":false},{"_id":"697831d9026bdf0473116e66","name":"Binxin Hu","hidden":false},{"_id":"697831d9026bdf0473116e67","name":"Ling Tang","hidden":false},{"_id":"697831d9026bdf0473116e68","name":"Jilin Mei","hidden":false},{"_id":"697831d9026bdf0473116e69","name":"Dadi Guo","hidden":false},{"_id":"697831d9026bdf0473116e6a","name":"Leitao Yuan","hidden":false},{"_id":"697831d9026bdf0473116e6b","name":"Junyao Yang","hidden":false},{"_id":"697831d9026bdf0473116e6c","name":"Guanxu Chen","hidden":false},{"_id":"697831d9026bdf0473116e6d","name":"Qihao Lin","hidden":false},{"_id":"697831d9026bdf0473116e6e","name":"Yi Yu","hidden":false},{"_id":"697831d9026bdf0473116e6f","name":"Bo Zhang","hidden":false},{"_id":"697831d9026bdf0473116e70","name":"Jiaxuan Guo","hidden":false},{"_id":"697831d9026bdf0473116e71","name":"Jie Zhang","hidden":false},{"_id":"697831d9026bdf0473116e72","name":"Wenqi Shao","hidden":false},{"_id":"697831d9026bdf0473116e73","name":"Huiqi Deng","hidden":false},{"_id":"697831d9026bdf0473116e74","name":"Zhiheng Xi","hidden":false},{"_id":"697831d9026bdf0473116e75","name":"Wenjie Wang","hidden":false},{"_id":"697831d9026bdf0473116e76","name":"Wenxuan Wang","hidden":false},{"_id":"697831d9026bdf0473116e77","name":"Wen Shen","hidden":false},{"_id":"697831d9026bdf0473116e78","name":"Zhikai Chen","hidden":false},{"_id":"697831d9026bdf0473116e79","name":"Haoyu Xie","hidden":false},{"_id":"697831d9026bdf0473116e7a","name":"Jialing Tao","hidden":false},{"_id":"697831d9026bdf0473116e7b","name":"Juntao Dai","hidden":false},{"_id":"697831d9026bdf0473116e7c","name":"Jiaming Ji","hidden":false},{"_id":"697831d9026bdf0473116e7d","name":"Zhongjie Ba","hidden":false},{"_id":"697831d9026bdf0473116e7e","name":"Linfeng Zhang","hidden":false},{"_id":"697831d9026bdf0473116e7f","name":"Yong Liu","hidden":false},{"_id":"697831d9026bdf0473116e80","name":"Quanshi Zhang","hidden":false},{"_id":"697831d9026bdf0473116e81","name":"Lei Zhu","hidden":false},{"_id":"697831d9026bdf0473116e82","name":"Zhihua Wei","hidden":false},{"_id":"697831d9026bdf0473116e83","name":"Hui Xue","hidden":false},{"_id":"697831d9026bdf0473116e84","name":"Chaochao Lu","hidden":false},{"_id":"697831d9026bdf0473116e85","name":"Jing Shao","hidden":false},{"_id":"697831d9026bdf0473116e86","name":"Xia Hu","hidden":false}],"publishedAt":"2026-01-26T13:45:41.000Z","submittedOnDailyAt":"2026-01-28T01:26:49.833Z","title":"AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security","submittedOnDailyBy":{"_id":"66e2624a436a1798365e4581","avatarUrl":"/avatars/6c605807d34faa8fb505e135a4b47776.svg","isPro":false,"fullname":"Qihan Ren","user":"jasonrqh","type":"user"},"summary":"The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.","upvotes":60,"discussionId":"697831d9026bdf0473116e87","githubRepo":"https://github.com/AI45Lab/AgentDoG","githubRepoAddedBy":"user","ai_summary":"AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.","ai_keywords":["agentic guardrail","three-dimensional taxonomy","agentic safety benchmark","Diagnostic Guardrail framework","agent safety and security","agent trajectories","root cause diagnosis","fine-grained monitoring","model variants","state-of-the-art performance"],"githubStars":178,"organization":{"_id":"68f716f832b31e42cbc2be7f","name":"AI45Research","fullname":"AI45Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}},"publishedAt":"2026-01-26T08:45:41.000Z","title":"AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security","summary":"The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18491.png","numComments":6,"submittedBy":{"_id":"66e2624a436a1798365e4581","avatarUrl":"/avatars/6c605807d34faa8fb505e135a4b47776.svg","fullname":"Qihan Ren","name":"jasonrqh","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"68f716f832b31e42cbc2be7f","name":"AI45Research","fullname":"AI45Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.18631","authors":[{"_id":"6978a169026bdf0473117088","user":{"_id":"66aca01e33f6b27979856f6f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg","isPro":false,"fullname":"Mingyang Song","user":"hitsmy","type":"user"},"name":"Mingyang Song","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:29:30.581Z","hidden":false},{"_id":"6978a169026bdf0473117089","user":{"_id":"63a2a51ef30c464227924fc6","avatarUrl":"/avatars/e109e85abd25b97bb29dbbe007119e34.svg","isPro":false,"fullname":"Haoyu Sun","user":"Mikivis","type":"user"},"name":"Haoyu Sun","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:29:26.154Z","hidden":false},{"_id":"6978a169026bdf047311708a","user":{"_id":"645b4819f9d4ec91fdd54852","avatarUrl":"/avatars/e12efb8e030688a0afcc72176b453fb3.svg","isPro":false,"fullname":"Jiawei Gu","user":"kuvvi","type":"user"},"name":"Jiawei Gu","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:29:28.392Z","hidden":false},{"_id":"6978a169026bdf047311708b","name":"Linjie Li","hidden":false},{"_id":"6978a169026bdf047311708c","name":"Luxin Xu","hidden":false},{"_id":"6978a169026bdf047311708d","name":"Ranjay Krishna","hidden":false},{"_id":"6978a169026bdf047311708e","name":"Yu Cheng","hidden":false}],"publishedAt":"2026-01-26T16:04:43.000Z","submittedOnDailyAt":"2026-01-28T01:52:20.218Z","title":"AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning","submittedOnDailyBy":{"_id":"66aca01e33f6b27979856f6f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg","isPro":false,"fullname":"Mingyang Song","user":"hitsmy","type":"user"},"summary":"When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.","upvotes":38,"discussionId":"6978a16a026bdf047311708f","projectPage":"https://adareasoner.github.io/","githubRepo":"https://github.com/ssmisya/AdaReasoner","githubRepoAddedBy":"user","ai_summary":"AdaReasoner enables multimodal models to learn tool usage as a general reasoning skill through scalable data curation, reinforcement learning for tool selection, and adaptive learning mechanisms that improve performance on complex visual reasoning tasks.","ai_keywords":["multimodal large language models","tool use","reinforcement learning","end-task success","adaptive learning mechanism","visual reasoning","multimodal models","tool selection","tool sequencing","long-horizon interactions"],"githubStars":44,"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"}},"publishedAt":"2026-01-26T11:04:43.000Z","title":"AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning","summary":"When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18631.png","numComments":3,"submittedBy":{"_id":"66aca01e33f6b27979856f6f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg","fullname":"Mingyang Song","name":"hitsmy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.18692","authors":[{"_id":"697989f6df44b75fa47e4803","user":{"_id":"69773d4ee6878183fb90a8c7","avatarUrl":"/avatars/3e9e4081e3beaf3f69c380387b8ee4c2.svg","isPro":false,"fullname":"Wei Wu","user":"Weiww99","type":"user"},"name":"Wei Wu","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:15:53.890Z","hidden":false},{"_id":"697989f6df44b75fa47e4804","name":"Fan Lu","hidden":false},{"_id":"697989f6df44b75fa47e4805","name":"Yunnan Wang","hidden":false},{"_id":"697989f6df44b75fa47e4806","user":{"_id":"64548f6c363bb3aaf9cba136","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64548f6c363bb3aaf9cba136/HqJL9HQ5CWVOJCsHyuMOm.jpeg","isPro":false,"fullname":"Shuai Yang","user":"ShuaiYang03","type":"user"},"name":"Shuai Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:14:06.626Z","hidden":false},{"_id":"697989f6df44b75fa47e4807","name":"Shi Liu","hidden":false},{"_id":"697989f6df44b75fa47e4808","name":"Fangjing Wang","hidden":false},{"_id":"697989f6df44b75fa47e4809","name":"Qian Zhu","hidden":false},{"_id":"697989f6df44b75fa47e480a","user":{"_id":"68234b1167281062097fc34e","avatarUrl":"/avatars/20ab2a15f27035c8b23d9a123e5ce22c.svg","isPro":false,"fullname":"HeSun","user":"he777771","type":"user"},"name":"He Sun","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:14:08.792Z","hidden":false},{"_id":"697989f6df44b75fa47e480b","name":"Yong Wang","hidden":false},{"_id":"697989f6df44b75fa47e480c","name":"Shuailei Ma","hidden":false},{"_id":"697989f6df44b75fa47e480d","name":"Yiyu Ren","hidden":false},{"_id":"697989f6df44b75fa47e480e","name":"Kejia Zhang","hidden":false},{"_id":"697989f6df44b75fa47e480f","name":"Hui Yu","hidden":false},{"_id":"697989f6df44b75fa47e4810","name":"Jingmei Zhao","hidden":false},{"_id":"697989f6df44b75fa47e4811","name":"Shuai Zhou","hidden":false},{"_id":"697989f6df44b75fa47e4812","name":"Zhenqi Qiu","hidden":false},{"_id":"697989f6df44b75fa47e4813","name":"Houlong Xiong","hidden":false},{"_id":"697989f6df44b75fa47e4814","name":"Ziyu Wang","hidden":false},{"_id":"697989f6df44b75fa47e4815","name":"Zechen Wang","hidden":false},{"_id":"697989f6df44b75fa47e4816","name":"Ran Cheng","hidden":false},{"_id":"697989f6df44b75fa47e4817","name":"Yong-Lu Li","hidden":false},{"_id":"697989f6df44b75fa47e4818","name":"Yongtao Huang","hidden":false},{"_id":"697989f6df44b75fa47e4819","name":"Xing Zhu","hidden":false},{"_id":"697989f6df44b75fa47e481a","name":"Yujun Shen","hidden":false},{"_id":"697989f6df44b75fa47e481b","name":"Kecheng Zheng","hidden":false}],"publishedAt":"2026-01-26T17:08:04.000Z","submittedOnDailyAt":"2026-01-28T03:13:43.716Z","title":"A Pragmatic VLA Foundation Model","submittedOnDailyBy":{"_id":"64252045a4f3051f54dd1d53","avatarUrl":"/avatars/0e423a3291091be3b4736a14da3ce495.svg","isPro":false,"fullname":"kecheng zheng","user":"zkcys001","type":"user"},"summary":"Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8times (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.","upvotes":26,"discussionId":"697989f7df44b75fa47e481c","projectPage":"https://technology.robbyant.com/lingbot-vla","githubRepo":"https://github.com/robbyant/lingbot-vla","githubRepoAddedBy":"auto","ai_summary":"A Vision-Language-Action model trained on extensive real-world robotic data demonstrates superior performance and generalization across multiple platforms while offering enhanced efficiency through optimized training infrastructure.","ai_keywords":["Vision-Language-Action","real-world data","robotic manipulation","generalization","efficient codebase","throughput","VLA-oriented codebases"],"githubStars":245,"organization":{"_id":"69709f892cd08371c1011a2e","name":"robbyant","fullname":"Robbyant","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}},"publishedAt":"2026-01-26T12:08:04.000Z","title":"A Pragmatic VLA Foundation Model","summary":"Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8times (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18692.png","numComments":2,"submittedBy":{"_id":"64252045a4f3051f54dd1d53","avatarUrl":"/avatars/0e423a3291091be3b4736a14da3ce495.svg","fullname":"kecheng zheng","name":"zkcys001","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"69709f892cd08371c1011a2e","name":"robbyant","fullname":"Robbyant","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.19834","authors":[{"_id":"69797e24df44b75fa47e4761","user":{"_id":"643b866bff50448bcfc7d1d1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg","isPro":false,"fullname":"Jialong Wu","user":"manchery","type":"user"},"name":"Jialong Wu","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:16:37.217Z","hidden":false},{"_id":"69797e24df44b75fa47e4762","name":"Xiaoying Zhang","hidden":false},{"_id":"69797e24df44b75fa47e4763","name":"Hongyi Yuan","hidden":false},{"_id":"69797e24df44b75fa47e4764","name":"Xiangcheng Zhang","hidden":false},{"_id":"69797e24df44b75fa47e4765","name":"Tianhao Huang","hidden":false},{"_id":"69797e24df44b75fa47e4766","name":"Changjing He","hidden":false},{"_id":"69797e24df44b75fa47e4767","name":"Chaoyi Deng","hidden":false},{"_id":"69797e24df44b75fa47e4768","name":"Renrui Zhang","hidden":false},{"_id":"69797e24df44b75fa47e4769","name":"Youbin Wu","hidden":false},{"_id":"69797e24df44b75fa47e476a","name":"Mingsheng Long","hidden":false}],"publishedAt":"2026-01-27T17:40:07.000Z","submittedOnDailyAt":"2026-01-28T00:46:44.173Z","title":"Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models","submittedOnDailyBy":{"_id":"643b866bff50448bcfc7d1d1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg","isPro":false,"fullname":"Jialong Wu","user":"manchery","type":"user"},"summary":"Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.","upvotes":19,"discussionId":"69797e24df44b75fa47e476b","projectPage":"https://thuml.github.io/Reasoning-Visual-World/","githubRepo":"https://github.com/thuml/Reasoning-Visual-World","githubRepoAddedBy":"user","ai_summary":"Visual generation enhances reasoning capabilities in multimodal models by providing more natural world models for physical and spatial tasks, while verbal reasoning remains sufficient for abstract domains.","ai_keywords":["chain-of-thought reasoning","large language models","multimodal models","visual generation","world models","visual superiority hypothesis","internal world modeling","interleaved reasoning","VisWorld-Eval"],"githubStars":36,"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}},"publishedAt":"2026-01-27T12:40:07.000Z","title":"Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models","summary":"Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19834.png","numComments":3,"submittedBy":{"_id":"643b866bff50448bcfc7d1d1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg","fullname":"Jialong Wu","name":"manchery","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.17645","authors":[{"_id":"6978296a026bdf0473116dc0","name":"Xilin Jiang","hidden":false},{"_id":"6978296a026bdf0473116dc1","user":{"_id":"68fc33d3705bf8aa76ef5215","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8kks5zJgOMoU2GuXUgRZq.jpeg","isPro":true,"fullname":"Qiaolin Wang","user":"QiaolinWang","type":"user"},"name":"Qiaolin Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:31:22.668Z","hidden":false},{"_id":"6978296a026bdf0473116dc2","user":{"_id":"665867a5d258e0fe41dcfb7a","avatarUrl":"/avatars/dfc23ec27c1e5177f44b2c881e12d458.svg","isPro":false,"fullname":"Junkai Wu","user":"wjk0925","type":"user"},"name":"Junkai Wu","status":"claimed_verified","statusLastChangedAt":"2026-01-27T09:42:46.216Z","hidden":false},{"_id":"6978296a026bdf0473116dc3","name":"Xiaomin He","hidden":false},{"_id":"6978296a026bdf0473116dc4","name":"Zhongweiyang Xu","hidden":false},{"_id":"6978296a026bdf0473116dc5","name":"Yinghao Ma","hidden":false},{"_id":"6978296a026bdf0473116dc6","name":"Minshuo Piao","hidden":false},{"_id":"6978296a026bdf0473116dc7","name":"Kaiyi Yang","hidden":false},{"_id":"6978296a026bdf0473116dc8","name":"Xiuwen Zheng","hidden":false},{"_id":"6978296a026bdf0473116dc9","name":"Riki Shimizu","hidden":false},{"_id":"6978296a026bdf0473116dca","name":"Yicong Chen","hidden":false},{"_id":"6978296a026bdf0473116dcb","name":"Arsalan Firoozi","hidden":false},{"_id":"6978296a026bdf0473116dcc","name":"Gavin Mischler","hidden":false},{"_id":"6978296a026bdf0473116dcd","name":"Sukru Samet Dindar","hidden":false},{"_id":"6978296a026bdf0473116dce","name":"Richard Antonello","hidden":false},{"_id":"6978296a026bdf0473116dcf","name":"Linyang He","hidden":false},{"_id":"6978296a026bdf0473116dd0","name":"Tsun-An Hsieh","hidden":false},{"_id":"6978296a026bdf0473116dd1","name":"Xulin Fan","hidden":false},{"_id":"6978296a026bdf0473116dd2","name":"Yulun Wu","hidden":false},{"_id":"6978296a026bdf0473116dd3","name":"Yuesheng Ma","hidden":false},{"_id":"6978296a026bdf0473116dd4","name":"Chaitanya Amballa","hidden":false},{"_id":"6978296a026bdf0473116dd5","name":"Weixiong Chen","hidden":false},{"_id":"6978296a026bdf0473116dd6","name":"Jiarui Hai","hidden":false},{"_id":"6978296a026bdf0473116dd7","name":"Ruisi Li","hidden":false},{"_id":"6978296a026bdf0473116dd8","name":"Vishal Choudhari","hidden":false},{"_id":"6978296a026bdf0473116dd9","name":"Cong Han","hidden":false},{"_id":"6978296a026bdf0473116dda","name":"Yinghao Aaron Li","hidden":false},{"_id":"6978296a026bdf0473116ddb","name":"Adeen Flinker","hidden":false},{"_id":"6978296a026bdf0473116ddc","name":"Mounya Elhilali","hidden":false},{"_id":"6978296a026bdf0473116ddd","name":"Emmanouil Benetos","hidden":false},{"_id":"6978296a026bdf0473116dde","name":"Mark Hasegawa-Johnson","hidden":false},{"_id":"6978296a026bdf0473116ddf","name":"Romit Roy Choudhury","hidden":false},{"_id":"6978296a026bdf0473116de0","name":"Nima Mesgarani","hidden":false}],"publishedAt":"2026-01-25T01:40:15.000Z","submittedOnDailyAt":"2026-01-28T00:44:43.594Z","title":"AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking","submittedOnDailyBy":{"_id":"6531a65daed617662c7f1007","avatarUrl":"/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg","isPro":false,"fullname":"Xilin Jiang","user":"xi-j","type":"user"},"summary":"Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public","upvotes":19,"discussionId":"6978296b026bdf0473116de1","projectPage":"https://avmemeexam.github.io/public","ai_summary":"Current multimodal models demonstrate limited understanding of cultural and contextual audio-visual content, particularly excelling only in surface-level analysis rather than deeper semantic comprehension.","ai_keywords":["multimodal large language models","audio-visual clips","human-curated benchmark","iconic Internet sounds","cultural context","surface content","semantic comprehension"],"organization":{"_id":"63f68badb607296857bb2441","name":"columbia","fullname":"Columbia University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png"}},"publishedAt":"2026-01-24T20:40:15.000Z","title":"AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking","summary":"Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17645.png","numComments":2,"submittedBy":{"_id":"6531a65daed617662c7f1007","avatarUrl":"/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg","fullname":"Xilin Jiang","name":"xi-j","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"63f68badb607296857bb2441","name":"columbia","fullname":"Columbia University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.19798","authors":[{"_id":"6979818fdf44b75fa47e477a","name":"Zhixiang Wei","hidden":false},{"_id":"6979818fdf44b75fa47e477b","name":"Yi Li","hidden":false},{"_id":"6979818fdf44b75fa47e477c","name":"Zhehan Kan","hidden":false},{"_id":"6979818fdf44b75fa47e477d","name":"Xinghua Jiang","hidden":false},{"_id":"6979818fdf44b75fa47e477e","name":"Zuwei Long","hidden":false},{"_id":"6979818fdf44b75fa47e477f","name":"Shifeng Liu","hidden":false},{"_id":"6979818fdf44b75fa47e4780","name":"Hongze Shen","hidden":false},{"_id":"6979818fdf44b75fa47e4781","name":"Wei Liu","hidden":false},{"_id":"6979818fdf44b75fa47e4782","name":"Xiaoyu Tan","hidden":false},{"_id":"6979818fdf44b75fa47e4783","name":"Haojia Lin","hidden":false},{"_id":"6979818fdf44b75fa47e4784","name":"Yubo Zhu","hidden":false},{"_id":"6979818fdf44b75fa47e4785","name":"Qianyu Li","hidden":false},{"_id":"6979818fdf44b75fa47e4786","user":{"_id":"63fc75f9b9db84750cea9c5c","avatarUrl":"/avatars/2c5bf9685e0cfc4b5785a4a86c34e0db.svg","isPro":false,"fullname":"DI YIN","user":"DIYIN","type":"user"},"name":"Di Yin","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:16:25.761Z","hidden":false},{"_id":"6979818fdf44b75fa47e4787","name":"Haoyu Cao","hidden":false},{"_id":"6979818fdf44b75fa47e4788","name":"Weibo Gu","hidden":false},{"_id":"6979818fdf44b75fa47e4789","user":{"_id":"667016b9ccff4d0862460d88","avatarUrl":"/avatars/96959ed963af9eae2814079c5241af1c.svg","isPro":false,"fullname":"Xin Li","user":"fujikoli","type":"user"},"name":"Xin Li","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:28:49.228Z","hidden":false},{"_id":"6979818fdf44b75fa47e478a","user":{"_id":"6959daf62da262f6caa6b3b9","avatarUrl":"/avatars/3986907912cd2d502570b8969b02abcc.svg","isPro":false,"fullname":"Yinsong Liu","user":"Yinsongliu","type":"user"},"name":"Yinsong Liu","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:16:33.643Z","hidden":false},{"_id":"6979818fdf44b75fa47e478b","name":"Deqiang Jiang","hidden":false},{"_id":"6979818fdf44b75fa47e478c","user":{"_id":"647401e50da364bd0d002f2a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png","isPro":false,"fullname":"XING SUN","user":"tedsun","type":"user"},"name":"Xing Sun","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:16:28.308Z","hidden":false},{"_id":"6979818fdf44b75fa47e478d","name":"Yunsheng Wu","hidden":false},{"_id":"6979818fdf44b75fa47e478e","name":"Mingkong Tang","hidden":false},{"_id":"6979818fdf44b75fa47e478f","name":"Shuangyin Liu","hidden":false},{"_id":"6979818fdf44b75fa47e4790","name":"Lexiang Tang","hidden":false},{"_id":"6979818fdf44b75fa47e4791","name":"Haodong Lin","hidden":false},{"_id":"6979818fdf44b75fa47e4792","user":{"_id":"64ddbc3f1f2dad27e1a05ac1","avatarUrl":"/avatars/4fb2753b7998c8536bfd4780d3b10a6d.svg","isPro":false,"fullname":"Junrulu","user":"Junrulu","type":"user"},"name":"Junru Lu","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:16:30.779Z","hidden":false},{"_id":"6979818fdf44b75fa47e4793","name":"Jiarui Qin","hidden":false},{"_id":"6979818fdf44b75fa47e4794","name":"Lingfeng Qiao","hidden":false},{"_id":"6979818fdf44b75fa47e4795","name":"Ruizhi Qiao","hidden":false},{"_id":"6979818fdf44b75fa47e4796","name":"Bo Ke","hidden":false},{"_id":"6979818fdf44b75fa47e4797","name":"Jianfeng He","hidden":false},{"_id":"6979818fdf44b75fa47e4798","name":"Ke Li","hidden":false},{"_id":"6979818fdf44b75fa47e4799","name":"Yangning Li","hidden":false},{"_id":"6979818fdf44b75fa47e479a","name":"Yunhang Shen","hidden":false},{"_id":"6979818fdf44b75fa47e479b","name":"Mengdan Zhang","hidden":false},{"_id":"6979818fdf44b75fa47e479c","name":"Peixian Chen","hidden":false},{"_id":"6979818fdf44b75fa47e479d","name":"Kun Yin","hidden":false},{"_id":"6979818fdf44b75fa47e479e","name":"Bing Liu","hidden":false},{"_id":"6979818fdf44b75fa47e479f","name":"Yunfei Wu","hidden":false},{"_id":"6979818fdf44b75fa47e47a0","name":"Huang Chen","hidden":false},{"_id":"6979818fdf44b75fa47e47a1","name":"Zhongpeng Cai","hidden":false},{"_id":"6979818fdf44b75fa47e47a2","name":"Xiaotian Li","hidden":false}],"publishedAt":"2026-01-27T17:01:16.000Z","submittedOnDailyAt":"2026-01-28T19:57:05.016Z","title":"Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision","submittedOnDailyBy":{"_id":"610a70f35a40a8bfebfbf09b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1659922312540-610a70f35a40a8bfebfbf09b.jpeg","isPro":true,"fullname":"Daniel Bourke","user":"mrdbourke","type":"user"},"summary":"Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.","upvotes":16,"discussionId":"69798190df44b75fa47e47a3","projectPage":"https://youtu-tip.com/#llm","githubRepo":"https://github.com/TencentCloudADP/youtu-vl","githubRepoAddedBy":"user","ai_summary":"Youtu-VL addresses limitations in Vision-Language Models by introducing a unified autoregressive supervision paradigm that treats visual signals as target outputs rather than passive inputs, enabling improved multimodal comprehension and vision-centric task performance.","ai_keywords":["Vision-Language Models","autoregressive supervision","vision-as-target","vision-as-input","visual tokens","multimodal comprehension","vision-centric tasks","generalist visual agents"],"githubStars":35,"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-01-27T12:01:16.000Z","title":"Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision","summary":"Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19798.png","numComments":1,"submittedBy":{"_id":"610a70f35a40a8bfebfbf09b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1659922312540-610a70f35a40a8bfebfbf09b.jpeg","fullname":"Daniel Bourke","name":"mrdbourke","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":192,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.09150","authors":[{"_id":"6979a32cdf44b75fa47e4831","name":"Jianwen Sun","hidden":false},{"_id":"6979a32cdf44b75fa47e4832","name":"Yukang Feng","hidden":false},{"_id":"6979a32cdf44b75fa47e4833","name":"Kaining Ying","hidden":false},{"_id":"6979a32cdf44b75fa47e4834","name":"Chuanhao Li","hidden":false},{"_id":"6979a32cdf44b75fa47e4835","name":"Zizhen Li","hidden":false},{"_id":"6979a32cdf44b75fa47e4836","name":"Fanrui Zhang","hidden":false},{"_id":"6979a32cdf44b75fa47e4837","name":"Jiaxin Ai","hidden":false},{"_id":"6979a32cdf44b75fa47e4838","name":"Yifan Chang","hidden":false},{"_id":"6979a32cdf44b75fa47e4839","name":"Yu Dai","hidden":false},{"_id":"6979a32cdf44b75fa47e483a","name":"Yifei Huang","hidden":false},{"_id":"6979a32cdf44b75fa47e483b","name":"Kaipeng Zhang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/dWe8NkE2o-SBkMqd897MA.mp4"],"publishedAt":"2026-01-14T04:45:05.000Z","submittedOnDailyAt":"2026-01-28T03:30:08.387Z","title":"World Craft: Agentic Framework to Create Visualizable Worlds via Text","submittedOnDailyBy":{"_id":"65f1713552c38a91e0a445e8","avatarUrl":"/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg","isPro":false,"fullname":"kaipeng","user":"kpzhang996","type":"user"},"summary":"Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a ``dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (\\eg environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.","upvotes":15,"discussionId":"6979a32cdf44b75fa47e483c","githubRepo":"https://github.com/HerzogFL/World-Craft","githubRepoAddedBy":"user","ai_summary":"World Craft enables non-expert users to create executable and visualizable AI environments through textual descriptions by combining structured scaffolding and multi-agent intent analysis.","ai_keywords":["generative agent simulation","AI Town","large language models","agentic world creation framework","world scaffold","world guild","multi-agent framework","scene construction","narrative intent conveyance"],"githubStars":40,"organization":{"_id":"689f08c50df4fcf7fddc0b08","name":"ShandaAI","fullname":"Shanda AI Research Tokyo","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6099290247dc3dbf8a976612/OV-XOpG-1Pf-yzhFdkQse.png"}},"publishedAt":"2026-01-13T23:45:05.000Z","title":"World Craft: Agentic Framework to Create Visualizable Worlds via Text","summary":"Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a ``dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (\\eg environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/dWe8NkE2o-SBkMqd897MA.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09150.png","numComments":2,"submittedBy":{"_id":"65f1713552c38a91e0a445e8","avatarUrl":"/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg","fullname":"kaipeng","name":"kpzhang996","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"689f08c50df4fcf7fddc0b08","name":"ShandaAI","fullname":"Shanda AI Research Tokyo","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6099290247dc3dbf8a976612/OV-XOpG-1Pf-yzhFdkQse.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.18292","authors":[{"_id":"6979d0aedf44b75fa47e48d5","name":"Zhewen Tan","hidden":false},{"_id":"6979d0aedf44b75fa47e48d6","name":"Wenhan Yu","hidden":false},{"_id":"6979d0aedf44b75fa47e48d7","name":"Jianfeng Si","hidden":false},{"_id":"6979d0aedf44b75fa47e48d8","name":"Tongxin Liu","hidden":false},{"_id":"6979d0aedf44b75fa47e48d9","name":"Kaiqi Guan","hidden":false},{"_id":"6979d0aedf44b75fa47e48da","name":"Huiyan Jin","hidden":false},{"_id":"6979d0aedf44b75fa47e48db","name":"Jiawen Tao","hidden":false},{"_id":"6979d0aedf44b75fa47e48dc","name":"Xiaokun Yuan","hidden":false},{"_id":"6979d0aedf44b75fa47e48dd","name":"Duohe Ma","hidden":false},{"_id":"6979d0aedf44b75fa47e48de","name":"Xiangzheng Zhang","hidden":false},{"_id":"6979d0aedf44b75fa47e48df","name":"Tong Yang","hidden":false},{"_id":"6979d0aedf44b75fa47e48e0","user":{"_id":"632c30576bcb864974cc40a8","avatarUrl":"/avatars/96aa948ad1dd35d355e20b5765a2563a.svg","isPro":false,"fullname":"sunlin","user":"lincharliesun","type":"user"},"name":"Lin Sun","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:12:39.873Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632c30576bcb864974cc40a8/hgTgbVzSd572bZlvwCB-N.png"],"publishedAt":"2026-01-26T09:21:43.000Z","submittedOnDailyAt":"2026-01-28T06:53:04.070Z","title":"TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment","submittedOnDailyBy":{"_id":"632c30576bcb864974cc40a8","avatarUrl":"/avatars/96aa948ad1dd35d355e20b5765a2563a.svg","isPro":false,"fullname":"sunlin","user":"lincharliesun","type":"user"},"summary":"In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.","upvotes":9,"discussionId":"6979d0aedf44b75fa47e48e1","ai_summary":"A closed-loop reinforcement learning framework enables iterative collaboration between attacker, defender, and evaluator roles for improved large language model safety alignment without manual annotations.","ai_keywords":["reinforcement learning","large language models","safety alignment","adversarial prompt generation","safety defense","response assessment","closed-loop framework","co-improving collaboration","iterative learning"],"organization":{"_id":"6606990280543d0b74d38438","name":"qihoo360","fullname":"","avatar":"https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"}},"publishedAt":"2026-01-26T04:21:43.000Z","title":"TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment","summary":"In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632c30576bcb864974cc40a8/hgTgbVzSd572bZlvwCB-N.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18292.png","numComments":2,"submittedBy":{"_id":"632c30576bcb864974cc40a8","avatarUrl":"/avatars/96aa948ad1dd35d355e20b5765a2563a.svg","fullname":"sunlin","name":"lincharliesun","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"6606990280543d0b74d38438","name":"qihoo360","fullname":"","avatar":"https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.18116","authors":[{"_id":"6979d078df44b75fa47e48cd","user":{"_id":"632c30576bcb864974cc40a8","avatarUrl":"/avatars/96aa948ad1dd35d355e20b5765a2563a.svg","isPro":false,"fullname":"sunlin","user":"lincharliesun","type":"user"},"name":"Lin Sun","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:12:43.095Z","hidden":false},{"_id":"6979d078df44b75fa47e48ce","name":"Linglin Zhang","hidden":false},{"_id":"6979d078df44b75fa47e48cf","name":"Jingang Huang","hidden":false},{"_id":"6979d078df44b75fa47e48d0","name":"Change Jia","hidden":false},{"_id":"6979d078df44b75fa47e48d1","name":"Zhengwei Cheng","hidden":false},{"_id":"6979d078df44b75fa47e48d2","name":"Xiangzheng Zhang","hidden":false}],"publishedAt":"2026-01-26T04:00:56.000Z","submittedOnDailyAt":"2026-01-28T06:55:03.102Z","title":"FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning","submittedOnDailyBy":{"_id":"632c30576bcb864974cc40a8","avatarUrl":"/avatars/96aa948ad1dd35d355e20b5765a2563a.svg","isPro":false,"fullname":"sunlin","user":"lincharliesun","type":"user"},"summary":"The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.\n  We present FABLE, a Forest-based Adaptive Bi-path LLM-Enhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.\n  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.","upvotes":9,"discussionId":"6979d078df44b75fa47e48d3","ai_summary":"FABLE is a forest-based adaptive bi-path retrieval framework that enhances LLM-based information retrieval through hierarchical indexing and structured evidence acquisition, achieving superior performance with reduced token usage compared to traditional RAG methods.","ai_keywords":["Retrieval-Augmented Generation","long-context Large Language Models","hierarchical forest indexes","bi-path strategy","LLM-guided hierarchical traversal","structure-aware propagation","multi-granularity semantic structures","token reduction"],"organization":{"_id":"6606990280543d0b74d38438","name":"qihoo360","fullname":"","avatar":"https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"}},"publishedAt":"2026-01-25T23:00:56.000Z","title":"FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning","summary":"The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.\n  We present FABLE, a Forest-based Adaptive Bi-path LLM-Enhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.\n  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18116.png","numComments":2,"submittedBy":{"_id":"632c30576bcb864974cc40a8","avatarUrl":"/avatars/96aa948ad1dd35d355e20b5765a2563a.svg","fullname":"sunlin","name":"lincharliesun","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"6606990280543d0b74d38438","name":"qihoo360","fullname":"","avatar":"https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.19895","authors":[{"_id":"697990bddf44b75fa47e4827","name":"Chen Chen","hidden":false},{"_id":"697990bddf44b75fa47e4828","user":{"_id":"69798a0962e9590654013cab","avatarUrl":"/avatars/b60e3d096a463b55cd3b98677ae568d7.svg","isPro":false,"fullname":"Lai Wei","user":"laiwei-seed","type":"user"},"name":"Lai Wei","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:14:04.627Z","hidden":false}],"publishedAt":"2026-01-27T18:58:46.000Z","submittedOnDailyAt":"2026-01-28T08:48:49.669Z","title":"Post-LayerNorm Is Back: Stable, ExpressivE, and Deep","submittedOnDailyBy":{"_id":"69798a0962e9590654013cab","avatarUrl":"/avatars/b60e3d096a463b55cd3b98677ae568d7.svg","isPro":false,"fullname":"Lai Wei","user":"laiwei-seed","type":"user"},"summary":"Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.","upvotes":8,"discussionId":"697990bedf44b75fa47e4829","ai_summary":"A novel Post-LayerNorm Transformer architecture called Keel addresses training instability in extremely deep networks by replacing residual connections with Highway-style connections, enabling stable training beyond 1000 layers.","ai_keywords":["Post-LayerNorm","Pre-LayerNorm","Transformer architectures","residual pathways","gradient vanishing","Highway-style connections","deep learning","neural network training","depth scaling","perplexity"],"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}},"publishedAt":"2026-01-27T13:58:46.000Z","title":"Post-LayerNorm Is Back: Stable, ExpressivE, and Deep","summary":"Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19895.png","numComments":2,"submittedBy":{"_id":"69798a0962e9590654013cab","avatarUrl":"/avatars/b60e3d096a463b55cd3b98677ae568d7.svg","fullname":"Lai Wei","name":"laiwei-seed","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.19375","authors":[{"_id":"697981b2df44b75fa47e47a5","user":{"_id":"645b663eca5d8a297712f2e1","avatarUrl":"/avatars/c61dcba43feb879088b15b525e441cb9.svg","isPro":false,"fullname":"Quy-Anh Dang","user":"quyanh","type":"user"},"name":"Quy-Anh Dang","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:16:05.195Z","hidden":false},{"_id":"697981b2df44b75fa47e47a6","name":"Chris Ngo","hidden":false}],"publishedAt":"2026-01-27T08:56:25.000Z","submittedOnDailyAt":"2026-01-28T01:02:05.884Z","title":"Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection","submittedOnDailyBy":{"_id":"645b663eca5d8a297712f2e1","avatarUrl":"/avatars/c61dcba43feb879088b15b525e441cb9.svg","isPro":false,"fullname":"Quy-Anh Dang","user":"quyanh","type":"user"},"summary":"Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering","upvotes":5,"discussionId":"697981b2df44b75fa47e47a7","projectPage":"https://knoveleng.github.io/steering/","githubRepo":"https://github.com/knoveleng/steering","githubRepoAddedBy":"user","ai_summary":"Selective Steering enables continuous, norm-preserving control of language model behavior through targeted layer selection and mathematically rigorous rotation techniques.","ai_keywords":["activation steering","angular steering","norm preservation","layer selection","feature representations","activation distribution integrity","attack success rates","perplexity violations","capability retention","language models"],"githubStars":3,"organization":{"_id":"6735b5052769638944f432c9","name":"knoveleng","fullname":"Knovel Engineering","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63105f7463b70252b4775783/ya7YB4IA3MXjfsfx_hjB_.png"}},"publishedAt":"2026-01-27T03:56:25.000Z","title":"Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection","summary":"Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19375.png","numComments":2,"submittedBy":{"_id":"645b663eca5d8a297712f2e1","avatarUrl":"/avatars/c61dcba43feb879088b15b525e441cb9.svg","fullname":"Quy-Anh Dang","name":"quyanh","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"6735b5052769638944f432c9","name":"knoveleng","fullname":"Knovel Engineering","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63105f7463b70252b4775783/ya7YB4IA3MXjfsfx_hjB_.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.19362","authors":[{"_id":"69797926df44b75fa47e4748","user":{"_id":"63510eea0b94548566dad923","avatarUrl":"/avatars/629eaaf810718259bf7588dc2e6cc0d5.svg","isPro":false,"fullname":"Xinyi Wan","user":"ufotalent","type":"user"},"name":"Xinyi Wan","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:16:39.482Z","hidden":false},{"_id":"69797926df44b75fa47e4749","name":"Penghui Qi","hidden":false},{"_id":"69797926df44b75fa47e474a","name":"Guangxing Huang","hidden":false},{"_id":"69797926df44b75fa47e474b","name":"Chaoyi Ruan","hidden":false},{"_id":"69797926df44b75fa47e474c","name":"Min Lin","hidden":false},{"_id":"69797926df44b75fa47e474d","name":"Jialin Li","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63510eea0b94548566dad923/W0qTS4_tXdhIrKSgx7r9m.jpeg","https://cdn-uploads.huggingface.co/production/uploads/63510eea0b94548566dad923/CGtDjIW1W23_uGvOEXCLr.png"],"publishedAt":"2026-01-27T08:44:46.000Z","submittedOnDailyAt":"2026-01-28T00:33:12.158Z","title":"Revisiting Parameter Server in LLM Post-Training","submittedOnDailyBy":{"_id":"63510eea0b94548566dad923","avatarUrl":"/avatars/629eaaf810718259bf7588dc2e6cc0d5.svg","isPro":false,"fullname":"Xinyi Wan","user":"ufotalent","type":"user"},"summary":"Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose On-Demand Communication (ODC), which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.","upvotes":4,"discussionId":"69797926df44b75fa47e474e","githubRepo":"https://github.com/sail-sg/odc","githubRepoAddedBy":"user","ai_summary":"On-Demand Communication (ODC) adapts parameter server principles to Fully Sharded Data Parallel training by replacing collective communication with point-to-point communication, improving device utilization and throughput in imbalanced large language model training scenarios.","ai_keywords":["data parallel","parameter servers","collective communication","Fully Sharded Data Parallel","On-Demand Communication","all-gather","reduce-scatter","point-to-point communication","load balancing","device utilization","training throughput"],"githubStars":8,"organization":{"_id":"61f4e841c771e23a1abb61ff","name":"sail","fullname":"Sea AI Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"}},"publishedAt":"2026-01-27T03:44:46.000Z","title":"Revisiting Parameter Server in LLM Post-Training","summary":"Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose On-Demand Communication (ODC), which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63510eea0b94548566dad923/W0qTS4_tXdhIrKSgx7r9m.jpeg","https://cdn-uploads.huggingface.co/production/uploads/63510eea0b94548566dad923/CGtDjIW1W23_uGvOEXCLr.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19362.png","numComments":2,"submittedBy":{"_id":"63510eea0b94548566dad923","avatarUrl":"/avatars/629eaaf810718259bf7588dc2e6cc0d5.svg","fullname":"Xinyi Wan","name":"ufotalent","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"organization":{"_id":"61f4e841c771e23a1abb61ff","name":"sail","fullname":"Sea AI Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.19228","authors":[{"_id":"6979b0b9df44b75fa47e486d","name":"Tianhui Song","hidden":false},{"_id":"6979b0b9df44b75fa47e486e","name":"Haoyu Lu","hidden":false},{"_id":"6979b0b9df44b75fa47e486f","name":"Hao Yang","hidden":false},{"_id":"6979b0b9df44b75fa47e4870","name":"Lin Sui","hidden":false},{"_id":"6979b0b9df44b75fa47e4871","name":"Haoning Wu","hidden":false},{"_id":"6979b0b9df44b75fa47e4872","name":"Zaida Zhou","hidden":false},{"_id":"6979b0b9df44b75fa47e4873","name":"Zhiqi Huang","hidden":false},{"_id":"6979b0b9df44b75fa47e4874","name":"Yiping Bao","hidden":false},{"_id":"6979b0b9df44b75fa47e4875","name":"Y. Charles","hidden":false},{"_id":"6979b0b9df44b75fa47e4876","name":"Xinyu Zhou","hidden":false},{"_id":"6979b0b9df44b75fa47e4877","name":"Limin Wang","hidden":false}],"publishedAt":"2026-01-27T05:50:40.000Z","submittedOnDailyAt":"2026-01-28T17:02:09.945Z","title":"Towards Pixel-Level VLM Perception via Simple Points Prediction","submittedOnDailyBy":{"_id":"649e7693a83143427691769c","avatarUrl":"/avatars/d04f7b3d417423abaa053375212da21f.svg","isPro":false,"fullname":"Tianhui Song","user":"sthui","type":"user"},"summary":"We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SFtoRL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/","upvotes":4,"discussionId":"6979b0b9df44b75fa47e4878","projectPage":"https://simpleseg.github.io/","githubRepo":"https://github.com/songtianhui/SimpleSeg","githubRepoAddedBy":"user","ai_summary":"SimpleSeg enables multimodal large language models to perform pixel-level segmentation by predicting point sequences within language space, achieving competitive results without specialized architectures.","ai_keywords":["Multimodal Large Language Models","segmentation","sequence generation","point prediction","Reinforcement Learning","IoU-based reward","two-stage training","pixel-level perception","language space"],"githubStars":17,"organization":{"_id":"6425a114812813f8f4a9b02c","name":"moonshotai","fullname":"Moonshot AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}},"publishedAt":"2026-01-27T00:50:40.000Z","title":"Towards Pixel-Level VLM Perception via Simple Points Prediction","summary":"We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SFtoRL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19228.png","numComments":1,"submittedBy":{"_id":"649e7693a83143427691769c","avatarUrl":"/avatars/d04f7b3d417423abaa053375212da21f.svg","fullname":"Tianhui Song","name":"sthui","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"6425a114812813f8f4a9b02c","name":"moonshotai","fullname":"Moonshot AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.18724","authors":[{"_id":"6979b424df44b75fa47e487a","name":"Yusuke Sakai","hidden":false},{"_id":"6979b424df44b75fa47e487b","name":"Hidetaka Kamigaito","hidden":false},{"_id":"6979b424df44b75fa47e487c","name":"Taro Watanabe","hidden":false}],"publishedAt":"2026-01-26T17:48:23.000Z","submittedOnDailyAt":"2026-01-28T08:11:22.031Z","title":"HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences","submittedOnDailyBy":{"_id":"6508463c423b46492eec64e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6508463c423b46492eec64e2/WSU7NSqjk92Pr2xUIWjCk.png","isPro":false,"fullname":"Penghui Yang","user":"phyang","type":"user"},"summary":"Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as \"HalluCitation\" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.","upvotes":3,"discussionId":"6979b425df44b75fa47e487d","ai_summary":"Hallucinated citations, defined as false references to non-existent works, are prevalent in recent NLP conference publications, with significant implications for scientific reliability and conference credibility.","ai_keywords":[""]},"publishedAt":"2026-01-26T12:48:23.000Z","title":"HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences","summary":"Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as \"HalluCitation\" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18724.png","numComments":1,"submittedBy":{"_id":"6508463c423b46492eec64e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6508463c423b46492eec64e2/WSU7NSqjk92Pr2xUIWjCk.png","fullname":"Penghui Yang","name":"phyang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.15968","authors":[{"_id":"697a0cdfdf3e800774f139ec","name":"Xin Xie","hidden":false},{"_id":"697a0cdfdf3e800774f139ed","name":"Jiaxian Guo","hidden":false},{"_id":"697a0cdfdf3e800774f139ee","user":{"_id":"6978aa1e2904a5468ef339f6","avatarUrl":"/avatars/a854e189fbd81d1eec1153b715d51291.svg","isPro":false,"fullname":"Dong Gong","user":"dginf","type":"user"},"name":"Dong Gong","status":"claimed_verified","statusLastChangedAt":"2026-01-28T14:41:37.251Z","hidden":false}],"publishedAt":"2026-01-22T13:49:47.000Z","submittedOnDailyAt":"2026-01-28T13:37:43.031Z","title":"HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models","submittedOnDailyBy":{"_id":"6978aa1e2904a5468ef339f6","avatarUrl":"/avatars/a854e189fbd81d1eec1153b715d51291.svg","isPro":false,"fullname":"Dong Gong","user":"dginf","type":"user"},"summary":"Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.","upvotes":3,"discussionId":"697a0ce0df3e800774f139ef","ai_summary":"HyperAlign enhances diffusion model output quality by using a hypernetwork to dynamically adjust denoising trajectories based on input conditions and rewards, improving semantic consistency and visual appeal.","ai_keywords":["diffusion models","hypernetwork","low-rank adaptation","denoising trajectory","reward-conditioned alignment","reward score objective","preference data","Stable Diffusion","FLUX"]},"publishedAt":"2026-01-22T08:49:47.000Z","title":"HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models","summary":"Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15968.png","numComments":1,"submittedBy":{"_id":"6978aa1e2904a5468ef339f6","avatarUrl":"/avatars/a854e189fbd81d1eec1153b715d51291.svg","fullname":"Dong Gong","name":"dginf","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.19897","authors":[{"_id":"697983e4df44b75fa47e47d5","name":"Idan Shenfeld","hidden":false},{"_id":"697983e4df44b75fa47e47d6","name":"Mehul Damani","hidden":false},{"_id":"697983e4df44b75fa47e47d7","name":"Jonas Hbotter","hidden":false},{"_id":"697983e4df44b75fa47e47d8","name":"Pulkit Agrawal","hidden":false}],"publishedAt":"2026-01-27T18:59:08.000Z","submittedOnDailyAt":"2026-01-28T21:24:19.223Z","title":"Self-Distillation Enables Continual Learning","submittedOnDailyBy":{"_id":"6963da277c221fad2d15fddd","avatarUrl":"/avatars/838d83b1128697bd77837a9f8f962be0.svg","isPro":false,"fullname":"improbable ai","user":"improbableaimit","type":"user"},"summary":"Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.","upvotes":2,"discussionId":"697983e4df44b75fa47e47d9","ai_summary":"Self-Distillation Fine-Tuning enables on-policy learning from demonstrations, reducing catastrophic forgetting and allowing continuous skill accumulation in foundation models.","ai_keywords":["continual learning","reinforcement learning","supervised fine-tuning","on-policy learning","off-policy learning","catastrophic forgetting","in-context learning","self-distillation","demonstration-conditioned model","skill acquisition"],"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}},"publishedAt":"2026-01-27T13:59:08.000Z","title":"Self-Distillation Enables Continual Learning","summary":"Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19897.png","numComments":1,"submittedBy":{"_id":"6963da277c221fad2d15fddd","avatarUrl":"/avatars/838d83b1128697bd77837a9f8f962be0.svg","fullname":"improbable ai","name":"improbableaimit","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.19532","authors":[{"_id":"6979ec22df44b75fa47e4920","user":{"_id":"685c115b18f9b3926bcf1070","avatarUrl":"/avatars/1051de2673e0aa9378d31c829043ad13.svg","isPro":false,"fullname":"Marthe Ballon","user":"martheballon","type":"user"},"name":"Marthe Ballon","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:12:21.981Z","hidden":false},{"_id":"6979ec22df44b75fa47e4921","name":"Andres Algaba","hidden":false},{"_id":"6979ec22df44b75fa47e4922","name":"Brecht Verbeken","hidden":false},{"_id":"6979ec22df44b75fa47e4923","name":"Vincent Ginis","hidden":false}],"publishedAt":"2026-01-27T12:20:44.000Z","submittedOnDailyAt":"2026-01-28T08:33:43.569Z","title":"Benchmarks Saturate When The Model Gets Smarter Than The Judge","submittedOnDailyBy":{"_id":"685c115b18f9b3926bcf1070","avatarUrl":"/avatars/1051de2673e0aa9378d31c829043ad13.svg","isPro":false,"fullname":"Marthe Ballon","user":"martheballon","type":"user"},"summary":"Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset (n{=}4181) and a tagged, non-standard subset (n{=}247). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in 96.4% of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.","upvotes":1,"discussionId":"6979ec23df44b75fa47e4924","ai_summary":"A revised mathematical benchmark dataset was created through manual auditing to reduce noise and improve model performance assessment accuracy.","ai_keywords":["Large Language Models","Omni-MATH dataset","benchmark evaluation","dataset cleaning","judge reliability","model performance assessment"]},"publishedAt":"2026-01-27T07:20:44.000Z","title":"Benchmarks Saturate When The Model Gets Smarter Than The Judge","summary":"Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset (n{=}4181) and a tagged, non-standard subset (n{=}247). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in 96.4% of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19532.png","numComments":2,"submittedBy":{"_id":"685c115b18f9b3926bcf1070","avatarUrl":"/avatars/1051de2673e0aa9378d31c829043ad13.svg","fullname":"Marthe Ballon","name":"martheballon","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.19149","authors":[{"_id":"697986c2df44b75fa47e47e7","user":{"_id":"6310a812a23f0327bce68778","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg","isPro":false,"fullname":"Ethan Ning","user":"ethanning","type":"user"},"name":"Jingjie Ning","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:15:58.913Z","hidden":false},{"_id":"697986c2df44b75fa47e47e8","name":"Xiangzhen Shen","hidden":false},{"_id":"697986c2df44b75fa47e47e9","name":"Li Hou","hidden":false},{"_id":"697986c2df44b75fa47e47ea","name":"Shiyi Shen","hidden":false},{"_id":"697986c2df44b75fa47e47eb","name":"Jiahao Yang","hidden":false},{"_id":"697986c2df44b75fa47e47ec","name":"Junrui Li","hidden":false},{"_id":"697986c2df44b75fa47e47ed","name":"Hong Shan","hidden":false},{"_id":"697986c2df44b75fa47e47ee","name":"Sanan Wu","hidden":false},{"_id":"697986c2df44b75fa47e47ef","name":"Sihan Gao","hidden":false},{"_id":"697986c2df44b75fa47e47f0","name":"Huaqiang Eric Xu","hidden":false},{"_id":"697986c2df44b75fa47e47f1","name":"Xinheng He","hidden":false}],"publishedAt":"2026-01-27T03:27:04.000Z","submittedOnDailyAt":"2026-01-28T01:17:38.797Z","title":"GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery","submittedOnDailyBy":{"_id":"6310a812a23f0327bce68778","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg","isPro":false,"fullname":"Ethan Ning","user":"ethanning","type":"user"},"summary":"G protein-coupled receptors (GPCRs) govern diverse physiological processes and are central to modern pharmacology. Yet discovering GPCR modulators remains challenging because receptor activation often arises from complex allosteric effects rather than direct binding affinity, and conventional assays are slow, costly, and not optimized for capturing these dynamics. Here we present GPCR-Filter, a deep learning framework specifically developed for GPCR modulator discovery. We assembled a high-quality dataset of over 90,000 experimentally validated GPCR-ligand pairs, providing a robust foundation for training and evaluation. GPCR-Filter integrates the ESM-3 protein language model for high-fidelity GPCR sequence representations with graph neural networks that encode ligand structures, coupled through an attention-based fusion mechanism that learns receptor-ligand functional relationships. Across multiple evaluation settings, GPCR-Filter consistently outperforms state-of-the-art compound-protein interaction models and exhibits strong generalization to unseen receptors and ligands. Notably, the model successfully identified micromolar-level agonists of the 5-HT1A receptor with distinct chemical frameworks. These results establish GPCR-Filter as a scalable and effective computational approach for GPCR modulator discovery, advancing AI-assisted drug development for complex signaling systems.","upvotes":1,"discussionId":"697986c2df44b75fa47e47f2","ai_summary":"GPCR-Filter is a deep learning framework that combines protein language models and graph neural networks to identify GPCR modulators with high accuracy and generalization across unseen receptors and ligands.","ai_keywords":["GPCR","deep learning framework","ESM-3 protein language model","graph neural networks","attention-based fusion mechanism","compound-protein interaction models","receptor-ligand functional relationships"],"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"}},"publishedAt":"2026-01-26T22:27:04.000Z","title":"GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery","summary":"G protein-coupled receptors (GPCRs) govern diverse physiological processes and are central to modern pharmacology. Yet discovering GPCR modulators remains challenging because receptor activation often arises from complex allosteric effects rather than direct binding affinity, and conventional assays are slow, costly, and not optimized for capturing these dynamics. Here we present GPCR-Filter, a deep learning framework specifically developed for GPCR modulator discovery. We assembled a high-quality dataset of over 90,000 experimentally validated GPCR-ligand pairs, providing a robust foundation for training and evaluation. GPCR-Filter integrates the ESM-3 protein language model for high-fidelity GPCR sequence representations with graph neural networks that encode ligand structures, coupled through an attention-based fusion mechanism that learns receptor-ligand functional relationships. Across multiple evaluation settings, GPCR-Filter consistently outperforms state-of-the-art compound-protein interaction models and exhibits strong generalization to unseen receptors and ligands. Notably, the model successfully identified micromolar-level agonists of the 5-HT1A receptor with distinct chemical frameworks. These results establish GPCR-Filter as a scalable and effective computational approach for GPCR modulator discovery, advancing AI-assisted drug development for complex signaling systems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19149.png","numComments":3,"submittedBy":{"_id":"6310a812a23f0327bce68778","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6310a812a23f0327bce68778/dp9JherZV-RYiSRKnvxsh.jpeg","fullname":"Ethan Ning","name":"ethanning","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.18923","authors":[{"_id":"697a1e70df3e800774f13a09","name":"Manthan Patel","hidden":false},{"_id":"697a1e70df3e800774f13a0a","name":"Jonas Frey","hidden":false},{"_id":"697a1e70df3e800774f13a0b","name":"Mayank Mittal","hidden":false},{"_id":"697a1e70df3e800774f13a0c","name":"Fan Yang","hidden":false},{"_id":"697a1e70df3e800774f13a0d","name":"Alexander Hansson","hidden":false},{"_id":"697a1e70df3e800774f13a0e","name":"Amir Bar","hidden":false},{"_id":"697a1e70df3e800774f13a0f","name":"Cesar Cadena","hidden":false},{"_id":"697a1e70df3e800774f13a10","name":"Marco Hutter","hidden":false}],"publishedAt":"2026-01-26T19:45:31.000Z","submittedOnDailyAt":"2026-01-28T13:52:37.589Z","title":"DeFM: Learning Foundation Representations from Depth for Robotics","submittedOnDailyBy":{"_id":"66fa8b357aa2514b7fb320f3","avatarUrl":"/avatars/40e6aee914ce624703d26fc61aafb119.svg","isPro":false,"fullname":"Manthan Patel","user":"manthanhp","type":"user"},"summary":"Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/","upvotes":1,"discussionId":"697a1e70df3e800774f13a11","ai_summary":"DeFM is a self-supervised foundation model for depth representation learning that achieves state-of-the-art performance in robotic tasks through geometric and semantic feature extraction.","ai_keywords":["self-supervised foundation model","DINO-style self-distillation","depth images","metric awareness","input normalization strategy","compact models","robotic applications","geometric representations","semantic representations","sim-to-real transfer"],"organization":{"_id":"6798bc47495916be7c624bc2","name":"leggedrobotics","fullname":"Robotic Systems Lab - ETH Zrich","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b85828324dd1598bad739f/NxcyiB-gpFlU5sLTnepe2.png"}},"publishedAt":"2026-01-26T14:45:31.000Z","title":"DeFM: Learning Foundation Representations from Depth for Robotics","summary":"Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18923.png","numComments":1,"submittedBy":{"_id":"66fa8b357aa2514b7fb320f3","avatarUrl":"/avatars/40e6aee914ce624703d26fc61aafb119.svg","fullname":"Manthan Patel","name":"manthanhp","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6798bc47495916be7c624bc2","name":"leggedrobotics","fullname":"Robotic Systems Lab - ETH Zrich","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b85828324dd1598bad739f/NxcyiB-gpFlU5sLTnepe2.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.18067","authors":[{"_id":"69790825df44b75fa47e4691","user":{"_id":"65d067194c89a331b365ca0f","avatarUrl":"/avatars/bff4f9ceb06aba273cfaea9c13a39ebc.svg","isPro":false,"fullname":"weipo hsin","user":"weiber2002","type":"user"},"name":"Wei-Po Hsin","status":"claimed_verified","statusLastChangedAt":"2026-01-28T11:28:46.696Z","hidden":false},{"_id":"69790825df44b75fa47e4692","name":"Ren-Hao Deng","hidden":false},{"_id":"69790825df44b75fa47e4693","name":"Yao-Ting Hsieh","hidden":false},{"_id":"69790825df44b75fa47e4694","name":"En-Ming Huang","hidden":false},{"_id":"69790825df44b75fa47e4695","name":"Shih-Hao Hung","hidden":false}],"publishedAt":"2026-01-26T01:53:54.000Z","submittedOnDailyAt":"2026-01-28T10:29:39.930Z","title":"EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization","submittedOnDailyBy":{"_id":"65d067194c89a331b365ca0f","avatarUrl":"/avatars/bff4f9ceb06aba273cfaea9c13a39ebc.svg","isPro":false,"fullname":"weipo hsin","user":"weiber2002","type":"user"},"summary":"Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.","upvotes":1,"discussionId":"69790825df44b75fa47e4696","githubRepo":"https://github.com/weiber2002/ICRTL","githubRepoAddedBy":"admin","githubStars":3,"organization":{"_id":"673248e121823ee4ea594099","name":"nationaltaiwan","fullname":"","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"}},"publishedAt":"2026-01-25T20:53:54.000Z","title":"EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization","summary":"Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18067.png","numComments":2,"submittedBy":{"_id":"65d067194c89a331b365ca0f","avatarUrl":"/avatars/bff4f9ceb06aba273cfaea9c13a39ebc.svg","fullname":"weipo hsin","name":"weiber2002","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"673248e121823ee4ea594099","name":"nationaltaiwan","fullname":"","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67324880c1f20c742be144b8/CE1UiOtpMeC8pmtdGP4Nn.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.13295","authors":[{"_id":"697515255d41524304c1334c","name":"Arpandeep Khatua","hidden":false},{"_id":"697515255d41524304c1334d","name":"Hao Zhu","hidden":false},{"_id":"697515255d41524304c1334e","name":"Peter Tran","hidden":false},{"_id":"697515255d41524304c1334f","name":"Arya Prabhudesai","hidden":false},{"_id":"697515255d41524304c13350","name":"Frederic Sadrieh","hidden":false},{"_id":"697515255d41524304c13351","name":"Johann K. Lieberwirth","hidden":false},{"_id":"697515255d41524304c13352","name":"Xinkai Yu","hidden":false},{"_id":"697515255d41524304c13353","name":"Yicheng Fu","hidden":false},{"_id":"697515255d41524304c13354","name":"Michael J. Ryan","hidden":false},{"_id":"697515255d41524304c13355","name":"Jiaxin Pei","hidden":false},{"_id":"697515255d41524304c13356","name":"Diyi Yang","hidden":false}],"publishedAt":"2026-01-19T18:48:37.000Z","submittedOnDailyAt":"2026-01-28T19:41:02.076Z","title":"CooperBench: Why Coding Agents Cannot be Your Teammates Yet","submittedOnDailyBy":{"_id":"61aa376688c20eebf1e8deb3","avatarUrl":"/avatars/7c11dcb232c73547d7d87834be287822.svg","isPro":false,"fullname":"Hao Zhu","user":"ProKil","type":"user"},"summary":"Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.","upvotes":1,"discussionId":"697515265d41524304c13357","projectPage":"https://cooperbench.com","githubRepo":"https://github.com/cooperbench/CooperBench","githubRepoAddedBy":"user","githubStars":3,"organization":{"_id":"6112d84f8c2e1f4060908c9e","name":"stanfordnlp","fullname":"Stanford NLP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"}},"publishedAt":"2026-01-19T13:48:37.000Z","title":"CooperBench: Why Coding Agents Cannot be Your Teammates Yet","summary":"Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13295.png","numComments":2,"submittedBy":{"_id":"61aa376688c20eebf1e8deb3","avatarUrl":"/avatars/7c11dcb232c73547d7d87834be287822.svg","fullname":"Hao Zhu","name":"ProKil","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"organization":{"_id":"6112d84f8c2e1f4060908c9e","name":"stanfordnlp","fullname":"Stanford NLP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1628624969199-6032802e1f993496bc14d9e3.png"},"isAuthorParticipating":false}]
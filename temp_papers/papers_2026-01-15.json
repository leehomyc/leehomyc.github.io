[{"paper":{"id":"2601.07348","authors":[{"_id":"696855610ac10a06522f69cf","user":{"_id":"662911a202f5ad9a5195932f","avatarUrl":"/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg","isPro":false,"fullname":"Tu Hu","user":"Blackteaxxx","type":"user"},"name":"Tu Hu","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:18.320Z","hidden":false},{"_id":"696855610ac10a06522f69d0","name":"Ronghao Chen","hidden":false},{"_id":"696855610ac10a06522f69d1","user":{"_id":"65562edfb7bad186e877c724","avatarUrl":"/avatars/bb91f42b102e113208bbe3238916a015.svg","isPro":false,"fullname":"zhangshuo","user":"mcflurryshuoz","type":"user"},"name":"Shuo Zhang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:16.329Z","hidden":false},{"_id":"696855610ac10a06522f69d2","name":"Jianghao Yin","hidden":false},{"_id":"696855610ac10a06522f69d3","name":"Mou Xiao Feng","hidden":false},{"_id":"696855610ac10a06522f69d4","name":"Jingping Liu","hidden":false},{"_id":"696855610ac10a06522f69d5","name":"Shaolei Zhang","hidden":false},{"_id":"696855610ac10a06522f69d6","name":"Wenqi Jiang","hidden":false},{"_id":"696855610ac10a06522f69d7","name":"Yuqi Fang","hidden":false},{"_id":"696855610ac10a06522f69d8","name":"Sen Hu","hidden":false},{"_id":"696855610ac10a06522f69d9","name":"Yi Xu","hidden":false},{"_id":"696855610ac10a06522f69da","user":{"_id":"6603d56ab4344a2b07cd6d21","avatarUrl":"/avatars/1569bb60166532317c85e80da722ba1c.svg","isPro":false,"fullname":"Huacan Wang","user":"Huacan-Wang","type":"user"},"name":"Huacan Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:20.275Z","hidden":false}],"publishedAt":"2026-01-12T09:23:13.000Z","submittedOnDailyAt":"2026-01-15T00:23:14.421Z","title":"Controlled Self-Evolution for Algorithmic Code Optimization","submittedOnDailyBy":{"_id":"6603d56ab4344a2b07cd6d21","avatarUrl":"/avatars/1569bb60166532317c85e80da722ba1c.svg","isPro":false,"fullname":"Huacan Wang","user":"Huacan-Wang","type":"user"},"summary":"Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.","upvotes":94,"discussionId":"696855610ac10a06522f69db","githubRepo":"https://github.com/QuantaAlpha/EvoControl","githubRepoAddedBy":"user","ai_summary":"Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.","ai_keywords":["self-evolution methods","generate-verify-refine cycles","exploration efficiency","initialization bias","stochastic operations","feedback guidance","genetic evolution","targeted mutation","compositional crossover","hierarchical evolution memory","LLM backbones","EffiBench-X"],"githubStars":79,"organization":{"_id":"68b33ab6a9ed99140481cf44","name":"QuantaAlpha","fullname":"QuantaAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}},"publishedAt":"2026-01-12T04:23:13.000Z","title":"Controlled Self-Evolution for Algorithmic Code Optimization","summary":"Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png","numComments":3,"submittedBy":{"_id":"6603d56ab4344a2b07cd6d21","avatarUrl":"/avatars/1569bb60166532317c85e80da722ba1c.svg","fullname":"Huacan Wang","name":"Huacan-Wang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"68b33ab6a9ed99140481cf44","name":"QuantaAlpha","fullname":"QuantaAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.09688","authors":[{"_id":"696864c90ac10a06522f6a4a","name":"Yibo Wang","hidden":false},{"_id":"696864c90ac10a06522f6a4b","name":"Lei Wang","hidden":false},{"_id":"696864c90ac10a06522f6a4c","name":"Yue Deng","hidden":false},{"_id":"696864c90ac10a06522f6a4d","user":{"_id":"66bf00ca5b4e241fe266059d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png","isPro":false,"fullname":"Keming Wu","user":"wukeming11","type":"user"},"name":"Keming Wu","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:02:22.232Z","hidden":false},{"_id":"696864c90ac10a06522f6a4e","name":"Yao Xiao","hidden":false},{"_id":"696864c90ac10a06522f6a4f","name":"Huanjin Yao","hidden":false},{"_id":"696864c90ac10a06522f6a50","name":"Liwei Kang","hidden":false},{"_id":"696864c90ac10a06522f6a51","name":"Hai Ye","hidden":false},{"_id":"696864c90ac10a06522f6a52","name":"Yongcheng Jing","hidden":false},{"_id":"696864c90ac10a06522f6a53","name":"Lidong Bing","hidden":false}],"publishedAt":"2026-01-14T18:38:31.000Z","submittedOnDailyAt":"2026-01-15T01:33:59.520Z","title":"DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation","submittedOnDailyBy":{"_id":"66bf00ca5b4e241fe266059d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png","isPro":false,"fullname":"Keming Wu","user":"wukeming11","type":"user"},"summary":"Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.","upvotes":90,"discussionId":"696864c90ac10a06522f6a54","githubRepo":"https://github.com/Infinity-AILab/DeepResearchEval","githubRepoAddedBy":"user","ai_summary":"DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.","ai_keywords":["automated framework","deep research task construction","agentic evaluation","persona-driven pipeline","task qualification","search necessity","adaptive point-wise quality evaluation","active fact-checking","web search","multi-source evidence integration"],"githubStars":67,"organization":{"_id":"6948e6c46d88786b0ec9cf9d","name":"Infinity-AILab","fullname":"Infinity Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}},"publishedAt":"2026-01-14T13:38:31.000Z","title":"DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation","summary":"Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09688.png","numComments":1,"submittedBy":{"_id":"66bf00ca5b4e241fe266059d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png","fullname":"Keming Wu","name":"wukeming11","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"6948e6c46d88786b0ec9cf9d","name":"Infinity-AILab","fullname":"Infinity Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.09259","authors":[{"_id":"696856230ac10a06522f69dd","name":"Jian Zhang","hidden":false},{"_id":"696856230ac10a06522f69de","user":{"_id":"67e0dc49daf1e39a7d15e67f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png","isPro":false,"fullname":"Zhiyuan Wang","user":"Pekku","type":"user"},"name":"Zhiyuan Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:12.229Z","hidden":false},{"_id":"696856230ac10a06522f69df","name":"Zhangqi Wang","hidden":false},{"_id":"696856230ac10a06522f69e0","name":"Yu He","hidden":false},{"_id":"696856230ac10a06522f69e1","name":"Haoran Luo","hidden":false},{"_id":"696856230ac10a06522f69e2","name":"li yuan","hidden":false},{"_id":"696856230ac10a06522f69e3","name":"Lingling Zhang","hidden":false},{"_id":"696856230ac10a06522f69e4","name":"Rui Mao","hidden":false},{"_id":"696856230ac10a06522f69e5","user":{"_id":"66ac77011cfb12c087605acb","avatarUrl":"/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg","isPro":false,"fullname":"Lin","user":"Qika","type":"user"},"name":"Qika Lin","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:14.086Z","hidden":false},{"_id":"696856230ac10a06522f69e6","name":"Jun Liu","hidden":false}],"publishedAt":"2026-01-14T07:48:00.000Z","submittedOnDailyAt":"2026-01-15T00:22:01.292Z","title":"MAXS: Meta-Adaptive Exploration with LLM Agents","submittedOnDailyBy":{"_id":"658be7fe135580745c510323","avatarUrl":"/avatars/830e5cec4565efdc23226a86a0fcef0e.svg","isPro":false,"fullname":"Jian Zhang","user":"VentureZJ","type":"user"},"summary":"Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.","upvotes":81,"discussionId":"696856230ac10a06522f69e7","githubRepo":"https://github.com/exoskeletonzj/MAXS","githubRepoAddedBy":"user","ai_summary":"MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.","ai_keywords":["LLM agents","tool execution","reasoning planning","lookahead strategy","advantage value","step consistency variance","inter-step trend slopes","trajectory convergence","multi-tool reasoning","inference efficiency"],"githubStars":5,"organization":{"_id":"66a92d5a58cff488d93ab512","name":"XianJiaotongUniversity","fullname":"Xi'an Jiaotong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}},"publishedAt":"2026-01-14T02:48:00.000Z","title":"MAXS: Meta-Adaptive Exploration with LLM Agents","summary":"Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09259.png","numComments":3,"submittedBy":{"_id":"658be7fe135580745c510323","avatarUrl":"/avatars/830e5cec4565efdc23226a86a0fcef0e.svg","fullname":"Jian Zhang","name":"VentureZJ","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"66a92d5a58cff488d93ab512","name":"XianJiaotongUniversity","fullname":"Xi'an Jiaotong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.09274","authors":[{"_id":"6968568f0ac10a06522f69e9","name":"Jian Zhang","hidden":false},{"_id":"6968568f0ac10a06522f69ea","name":"Yu He","hidden":false},{"_id":"6968568f0ac10a06522f69eb","user":{"_id":"67e0dc49daf1e39a7d15e67f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png","isPro":false,"fullname":"Zhiyuan Wang","user":"Pekku","type":"user"},"name":"Zhiyuan Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:02.764Z","hidden":false},{"_id":"6968568f0ac10a06522f69ec","name":"Zhangqi Wang","hidden":false},{"_id":"6968568f0ac10a06522f69ed","name":"Kai He","hidden":false},{"_id":"6968568f0ac10a06522f69ee","name":"Fangzhi Xu","hidden":false},{"_id":"6968568f0ac10a06522f69ef","user":{"_id":"66ac77011cfb12c087605acb","avatarUrl":"/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg","isPro":false,"fullname":"Lin","user":"Qika","type":"user"},"name":"Qika Lin","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:05.035Z","hidden":false},{"_id":"6968568f0ac10a06522f69f0","name":"Jun Liu","hidden":false}],"publishedAt":"2026-01-14T08:17:41.000Z","submittedOnDailyAt":"2026-01-15T00:23:45.077Z","title":"A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation","submittedOnDailyBy":{"_id":"658be7fe135580745c510323","avatarUrl":"/avatars/830e5cec4565efdc23226a86a0fcef0e.svg","isPro":false,"fullname":"Jian Zhang","user":"VentureZJ","type":"user"},"summary":"Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.","upvotes":74,"discussionId":"6968568f0ac10a06522f69f1","projectPage":"https://a3-bench.github.io/","githubRepo":"https://github.com/exoskeletonzj/A3-Bench","githubRepoAddedBy":"user","githubStars":0,"organization":{"_id":"66a92d5a58cff488d93ab512","name":"XianJiaotongUniversity","fullname":"Xi'an Jiaotong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}},"publishedAt":"2026-01-14T03:17:41.000Z","title":"A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation","summary":"Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09274.png","numComments":2,"submittedBy":{"_id":"658be7fe135580745c510323","avatarUrl":"/avatars/830e5cec4565efdc23226a86a0fcef0e.svg","fullname":"Jian Zhang","name":"VentureZJ","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"66a92d5a58cff488d93ab512","name":"XianJiaotongUniversity","fullname":"Xi'an Jiaotong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.09088","authors":[{"_id":"69688bbc0ac10a06522f6aeb","user":{"_id":"6463345cd2044cd1d7c613a8","avatarUrl":"/avatars/242cbf2479877e836f931d17a6190660.svg","isPro":false,"fullname":"Shaotian","user":"ystluffy","type":"user"},"name":"Shaotian Yan","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:27.639Z","hidden":false},{"_id":"69688bbc0ac10a06522f6aec","name":"Kaiyuan Liu","hidden":false},{"_id":"69688bbc0ac10a06522f6aed","user":{"_id":"64b73e3830a0b8ff60145a29","avatarUrl":"/avatars/297469812b57b2ddf7d52b9391d80bde.svg","isPro":false,"fullname":"Chen Shen","user":"zjushenchen","type":"user"},"name":"Chen Shen","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:19.260Z","hidden":false},{"_id":"69688bbc0ac10a06522f6aee","user":{"_id":"6225b0d87f5fba1007d62fae","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6225b0d87f5fba1007d62fae/clONu5C-lkoSswcJjcG0u.jpeg","isPro":false,"fullname":"Bing Wang","user":"wangbing1416","type":"user"},"name":"Bing Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:17.091Z","hidden":false},{"_id":"69688bbc0ac10a06522f6aef","user":{"_id":"694a226980a37f293a4ce7c0","avatarUrl":"/avatars/7a48f4eeb80a1b5688cbfb10a59765a0.svg","isPro":false,"fullname":"Sinan Fan","user":"sinan25","type":"user"},"name":"Sinan Fan","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:14.748Z","hidden":false},{"_id":"69688bbc0ac10a06522f6af0","name":"Jun Zhang","hidden":false},{"_id":"69688bbc0ac10a06522f6af1","name":"Yue Wu","hidden":false},{"_id":"69688bbc0ac10a06522f6af2","name":"Zheng Wang","hidden":false},{"_id":"69688bbc0ac10a06522f6af3","name":"Jieping Ye","hidden":false}],"publishedAt":"2026-01-14T02:43:17.000Z","submittedOnDailyAt":"2026-01-15T07:24:58.461Z","title":"Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning","submittedOnDailyBy":{"_id":"6463345cd2044cd1d7c613a8","avatarUrl":"/avatars/242cbf2479877e836f931d17a6190660.svg","isPro":false,"fullname":"Shaotian","user":"ystluffy","type":"user"},"summary":"In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.","upvotes":43,"discussionId":"69688bbc0ac10a06522f6af4","projectPage":"https://github.com/D2I-ai/dasd-thinking","githubRepo":"https://github.com/D2I-ai/dasd-thinking","githubRepoAddedBy":"user","ai_summary":"A lightweight open-source reasoning model achieves state-of-the-art performance through enhanced sequence-level distillation that addresses limitations in current teacher-student knowledge transfer methods.","ai_keywords":["sequence-level distillation","teacher-student distillation","SFT","heuristic rules","output distribution","generalization capability","exposure bias","teacher-forced training","autoregressive inference"],"githubStars":16,"organization":{"_id":"693005c327917f8ddef415f4","name":"Alibaba-Apsara","fullname":"Alibaba Cloud Apsara Lab ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"}},"publishedAt":"2026-01-13T21:43:17.000Z","title":"Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning","summary":"In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09088.png","numComments":4,"submittedBy":{"_id":"6463345cd2044cd1d7c613a8","avatarUrl":"/avatars/242cbf2479877e836f931d17a6190660.svg","fullname":"Shaotian","name":"ystluffy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"693005c327917f8ddef415f4","name":"Alibaba-Apsara","fullname":"Alibaba Cloud Apsara Lab ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.09708","authors":[{"_id":"69684f740ac10a06522f69ba","user":{"_id":"64705d224be5cf1f3348d6bc","avatarUrl":"/avatars/270bff7c7cb326528dc192fc38561a8b.svg","isPro":false,"fullname":"Chi-Pin Huang","user":"jasper0314-huang","type":"user"},"name":"Chi-Pin Huang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:32.512Z","hidden":false},{"_id":"69684f740ac10a06522f69bb","name":"Yunze Man","hidden":false},{"_id":"69684f740ac10a06522f69bc","name":"Zhiding Yu","hidden":false},{"_id":"69684f740ac10a06522f69bd","user":{"_id":"64ae22dd1aee69ece065cdcd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png","isPro":false,"fullname":"Min-Hung Chen","user":"cmhungsteve","type":"user"},"name":"Min-Hung Chen","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:27.724Z","hidden":false},{"_id":"69684f740ac10a06522f69be","name":"Jan Kautz","hidden":false},{"_id":"69684f740ac10a06522f69bf","name":"Yu-Chiang Frank Wang","hidden":false},{"_id":"69684f740ac10a06522f69c0","user":{"_id":"6312cab05beb528b5c1500e3","avatarUrl":"/avatars/a328e8cc99fb031b2d5c911c4b577e7e.svg","isPro":false,"fullname":"Fu-En Yang","user":"FuEnYang","type":"user"},"name":"Fu-En Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:30.478Z","hidden":false}],"publishedAt":"2026-01-14T18:59:59.000Z","submittedOnDailyAt":"2026-01-15T00:10:27.528Z","title":"Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning","submittedOnDailyBy":{"_id":"64705d224be5cf1f3348d6bc","avatarUrl":"/avatars/270bff7c7cb326528dc192fc38561a8b.svg","isPro":false,"fullname":"Chi-Pin Huang","user":"jasper0314-huang","type":"user"},"summary":"Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.","upvotes":36,"discussionId":"69684f740ac10a06522f69c1","projectPage":"https://jasper0314-huang.github.io/fast-thinkact/","ai_summary":"Fast-ThinkAct is an efficient vision-language-action framework that reduces inference latency by 89.3% through compact latent reasoning while maintaining long-horizon planning and few-shot adaptation capabilities.","ai_keywords":["chain-of-thought","latent reasoning","preference-guided objective","embodied control","policy learning","inference latency","vision-language-action"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-01-14T13:59:59.000Z","title":"Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning","summary":"Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09708.png","numComments":1,"submittedBy":{"_id":"64705d224be5cf1f3348d6bc","avatarUrl":"/avatars/270bff7c7cb326528dc192fc38561a8b.svg","fullname":"Chi-Pin Huang","name":"jasper0314-huang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.09136","authors":[{"_id":"6968555c0ac10a06522f69c3","name":"Lijun Liu","hidden":false},{"_id":"6968555c0ac10a06522f69c4","name":"Linwei Chen","hidden":false},{"_id":"6968555c0ac10a06522f69c5","name":"Zhishou Zhang","hidden":false},{"_id":"6968555c0ac10a06522f69c6","name":"Meng Tian","hidden":false},{"_id":"6968555c0ac10a06522f69c7","name":"Hengfu Cui","hidden":false},{"_id":"6968555c0ac10a06522f69c8","name":"Ruiyang Li","hidden":false},{"_id":"6968555c0ac10a06522f69c9","name":"Zhaocheng Liu","hidden":false},{"_id":"6968555c0ac10a06522f69ca","user":{"_id":"62dcdb86d36b2070f928a51e","avatarUrl":"/avatars/a341e4305217f8abd14cff97201a24aa.svg","isPro":false,"fullname":"sdujq","user":"sdujq","type":"user"},"name":"Qiang Ju","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:25.438Z","hidden":false},{"_id":"6968555c0ac10a06522f69cb","name":"Qianxi Li","hidden":false},{"_id":"6968555c0ac10a06522f69cc","name":"Hong-Yu Zhou","hidden":false}],"publishedAt":"2026-01-14T04:21:07.000Z","submittedOnDailyAt":"2026-01-15T00:59:44.722Z","title":"SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL","submittedOnDailyBy":{"_id":"642438eaa3adbc7142c3ca0f","avatarUrl":"/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg","isPro":false,"fullname":"CharlesChen","user":"CharlesChen2023","type":"user"},"summary":"General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.","upvotes":36,"discussionId":"6968555c0ac10a06522f69cd","ai_summary":"SkinFlow introduces a novel framework for dermatological vision-language modeling that improves diagnostic accuracy through optimized visual information transmission efficiency rather than parameter scaling alone.","ai_keywords":["Large Vision-Language Models","diffuse attention","Virtual-Width Dynamic Vision Encoder","reinforcement learning","visual information transmission efficiency","diagnostic reasoning","Fitzpatrick17k benchmark","Top-1 accuracy","Top-6 accuracy"],"organization":{"_id":"648457d38cf0b32b0ba0a913","name":"baichuan-inc","fullname":"Baichuan Intelligent Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}},"publishedAt":"2026-01-13T23:21:07.000Z","title":"SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL","summary":"General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09136.png","numComments":4,"submittedBy":{"_id":"642438eaa3adbc7142c3ca0f","avatarUrl":"/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg","fullname":"CharlesChen","name":"CharlesChen2023","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"648457d38cf0b32b0ba0a913","name":"baichuan-inc","fullname":"Baichuan Intelligent Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.09575","authors":[{"_id":"696869800ac10a06522f6a92","name":"Sheng-Yu Huang","hidden":false},{"_id":"696869800ac10a06522f6a93","name":"Jaesung Choe","hidden":false},{"_id":"696869800ac10a06522f6a94","name":"Yu-Chiang Frank Wang","hidden":false},{"_id":"696869800ac10a06522f6a95","name":"Cheng Sun","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Mf0UQtcxycDL3jlgJpk6d.jpeg"],"publishedAt":"2026-01-14T15:45:57.000Z","submittedOnDailyAt":"2026-01-15T01:44:30.828Z","title":"OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.","upvotes":22,"discussionId":"696869810ac10a06522f6a96","projectPage":"https://peterjohnsonhuang.github.io/openvoxel-pages/","ai_summary":"OpenVoxel enables open-vocabulary 3D scene understanding through training-free grouping and captioning of sparse voxels using Vision Language Models and Multi-modal Large Language Models.","ai_keywords":["open-vocabulary 3D scene understanding","sparse voxels","sparse voxel rasterization","Vision Language Models","Multi-modal Large Language Models","open-vocabulary segmentation","referring expression segmentation","training-free algorithm"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-01-14T10:45:57.000Z","title":"OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding","summary":"We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Mf0UQtcxycDL3jlgJpk6d.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09575.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.09028","authors":[{"_id":"69686b880ac10a06522f6aae","name":"Fengran Mo","hidden":false},{"_id":"69686b880ac10a06522f6aaf","name":"Zhan Su","hidden":false},{"_id":"69686b880ac10a06522f6ab0","user":{"_id":"63164fe6550b00d37db957b6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63164fe6550b00d37db957b6/D_SH8nQ2wCOcN-i7tLZ9R.jpeg","isPro":false,"fullname":"Yuchen Hui","user":"Meranti","type":"user"},"name":"Yuchen Hui","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:41.214Z","hidden":false},{"_id":"69686b880ac10a06522f6ab1","name":"Jinghan Zhang","hidden":false},{"_id":"69686b880ac10a06522f6ab2","name":"Jia Ao Sun","hidden":false},{"_id":"69686b880ac10a06522f6ab3","name":"Zheyuan Liu","hidden":false},{"_id":"69686b880ac10a06522f6ab4","name":"Chao Zhang","hidden":false},{"_id":"69686b880ac10a06522f6ab5","name":"Tetsuya Sakai","hidden":false},{"_id":"69686b880ac10a06522f6ab6","name":"Jian-Yun Nie","hidden":false}],"publishedAt":"2026-01-13T23:26:30.000Z","submittedOnDailyAt":"2026-01-15T18:11:42.429Z","title":"OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG","submittedOnDailyBy":{"_id":"6570cc04c4993b8fb975a2e3","avatarUrl":"/avatars/64d8e0580ffa8d704933f94985bf7d5c.svg","isPro":false,"fullname":"Fengran Mo","user":"fengran","type":"user"},"summary":"The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.","upvotes":17,"discussionId":"69686b890ac10a06522f6ab7","ai_summary":"OpenDecoder enhances retrieval-augmented generation by explicitly evaluating retrieved information quality through relevance, ranking, and query performance prediction scores, improving robustness to noisy context.","ai_keywords":["large language models","retrieval-augmented generation","QPP","query performance prediction","relevance score","ranking score","post-training"],"organization":{"_id":"6886fa1c363ab9ef0bb63d0a","name":"UniversitedeMontreal","fullname":"Universit de Montral","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6886f890bea3309b70e5950c/OZiMpTqoDfbwxXr10n8B4.png"}},"publishedAt":"2026-01-13T18:26:30.000Z","title":"OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG","summary":"The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09028.png","numComments":1,"submittedBy":{"_id":"6570cc04c4993b8fb975a2e3","avatarUrl":"/avatars/64d8e0580ffa8d704933f94985bf7d5c.svg","fullname":"Fengran Mo","name":"fengran","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6886fa1c363ab9ef0bb63d0a","name":"UniversitedeMontreal","fullname":"Universit de Montral","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6886f890bea3309b70e5950c/OZiMpTqoDfbwxXr10n8B4.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.08605","authors":[{"_id":"69670413c5e371f6b235d0ea","name":"Wenyuan Zhang","hidden":false},{"_id":"69670413c5e371f6b235d0eb","name":"Xinghua Zhang","hidden":false},{"_id":"69670413c5e371f6b235d0ec","name":"Haiyang Yu","hidden":false},{"_id":"69670413c5e371f6b235d0ed","name":"Shuaiyi Nie","hidden":false},{"_id":"69670413c5e371f6b235d0ee","name":"Bingli Wu","hidden":false},{"_id":"69670413c5e371f6b235d0ef","name":"Juwei Yue","hidden":false},{"_id":"69670413c5e371f6b235d0f0","name":"Tingwen Liu","hidden":false},{"_id":"69670413c5e371f6b235d0f1","name":"Yongbin Li","hidden":false}],"publishedAt":"2026-01-13T14:48:34.000Z","submittedOnDailyAt":"2026-01-15T02:32:45.902Z","title":"ExpSeek: Self-Triggered Experience Seeking for Web Agents","submittedOnDailyBy":{"_id":"6617c98901ad3a0642a2a08f","avatarUrl":"/avatars/cf52fb511f2f31de7940f9c13d19b8e7.svg","isPro":false,"fullname":"Wenyuan Zhang","user":"WYRipple","type":"user"},"summary":"Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.","upvotes":15,"discussionId":"69670413c5e371f6b235d0f2","ai_summary":"ExpSeek enables web agents to proactively seek experience during interaction by using entropy-based timing and tailored content, achieving significant performance improvements across multiple benchmarks.","ai_keywords":["experience intervention","web agents","step-level proactive seeking","entropy thresholds","intrinsic signals","experience content design","Qwen3-8B","Qwen3-32B","web agent benchmarks"],"organization":{"_id":"67d15cca6e2cf0e062dbfb54","name":"AlibabaTongyiLab","fullname":"TongyiLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}},"publishedAt":"2026-01-13T09:48:34.000Z","title":"ExpSeek: Self-Triggered Experience Seeking for Web Agents","summary":"Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08605.png","numComments":1,"submittedBy":{"_id":"6617c98901ad3a0642a2a08f","avatarUrl":"/avatars/cf52fb511f2f31de7940f9c13d19b8e7.svg","fullname":"Wenyuan Zhang","name":"WYRipple","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67d15cca6e2cf0e062dbfb54","name":"AlibabaTongyiLab","fullname":"TongyiLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.03928","authors":[{"_id":"69681d640ac10a06522f6988","name":"Mingyu Ouyang","hidden":false},{"_id":"69681d640ac10a06522f6989","user":{"_id":"64440be5af034cdfd69ca3a7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg","isPro":false,"fullname":"Qinghong (Kevin) Lin","user":"KevinQHLin","type":"user"},"name":"Kevin Qinghong Lin","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:03:36.762Z","hidden":false},{"_id":"69681d640ac10a06522f698a","name":"Mike Zheng Shou","hidden":false},{"_id":"69681d640ac10a06522f698b","name":"Hwee Tou Ng","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/634e2217c1ce28f1de921708/AWCt2drJyoFbFk0FEwlwd.mp4"],"publishedAt":"2026-01-07T13:48:12.000Z","submittedOnDailyAt":"2026-01-15T01:14:34.735Z","title":"FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection","submittedOnDailyBy":{"_id":"634e2217c1ce28f1de921708","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/634e2217c1ce28f1de921708/XTMB6alYUM0KAUptM98kP.jpeg","isPro":false,"fullname":"Yang","user":"yyyang","type":"user"},"summary":"Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.","upvotes":12,"discussionId":"69681d640ac10a06522f698c","ai_summary":"FocusUI is an efficient UI grounding framework that reduces computational overhead by selecting relevant visual tokens while preserving positional continuity through a novel PosPad strategy.","ai_keywords":["Vision-Language Models","UI grounding","visual tokens","patch-level supervision","instruction-conditioned score","UI-graph score","visual token pruning","positional continuity","PosPad strategy","ScreenSpot-Pro benchmark"],"organization":{"_id":"63a553c4ce5763e06f78669c","name":"showlab","fullname":"Show Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"}},"publishedAt":"2026-01-07T08:48:12.000Z","title":"FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection","summary":"Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/634e2217c1ce28f1de921708/AWCt2drJyoFbFk0FEwlwd.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03928.png","numComments":1,"submittedBy":{"_id":"634e2217c1ce28f1de921708","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/634e2217c1ce28f1de921708/XTMB6alYUM0KAUptM98kP.jpeg","fullname":"Yang","name":"yyyang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"63a553c4ce5763e06f78669c","name":"showlab","fullname":"Show Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.09465","authors":[{"_id":"696867a90ac10a06522f6a82","user":{"_id":"65562edfb7bad186e877c724","avatarUrl":"/avatars/bb91f42b102e113208bbe3238916a015.svg","isPro":false,"fullname":"zhangshuo","user":"mcflurryshuoz","type":"user"},"name":"Shuo Zhang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:43.605Z","hidden":false},{"_id":"696867a90ac10a06522f6a83","name":"Chaofa Yuan","hidden":false},{"_id":"696867a90ac10a06522f6a84","name":"Ryan Guo","hidden":false},{"_id":"696867a90ac10a06522f6a85","user":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","isPro":false,"fullname":"Yu_xm","user":"Yu2020","type":"user"},"name":"Xiaomin Yu","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:02:52.674Z","hidden":false},{"_id":"696867a90ac10a06522f6a86","name":"Rui Xu","hidden":false},{"_id":"696867a90ac10a06522f6a87","name":"Zhangquan Chen","hidden":false},{"_id":"696867a90ac10a06522f6a88","name":"Zinuo Li","hidden":false},{"_id":"696867a90ac10a06522f6a89","user":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","isPro":false,"fullname":"Zhi Yang","user":"yangzhi1","type":"user"},"name":"Zhi Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:45.582Z","hidden":false},{"_id":"696867a90ac10a06522f6a8a","name":"Shuhao Guan","hidden":false},{"_id":"696867a90ac10a06522f6a8b","name":"Zhenheng Tang","hidden":false},{"_id":"696867a90ac10a06522f6a8c","name":"Sen Hu","hidden":false},{"_id":"696867a90ac10a06522f6a8d","name":"Liwen Zhang","hidden":false},{"_id":"696867a90ac10a06522f6a8e","name":"Ronghao Chen","hidden":false},{"_id":"696867a90ac10a06522f6a8f","name":"Huacan Wang","hidden":false}],"publishedAt":"2026-01-14T13:19:13.000Z","submittedOnDailyAt":"2026-01-15T01:36:08.471Z","title":"EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.","upvotes":11,"discussionId":"696867a90ac10a06522f6a90","ai_summary":"EvoFSM is a structured self-evolving framework for LLM agents that uses finite state machines to improve adaptability while maintaining control through constrained optimization and memory mechanisms.","ai_keywords":["LLM-based agents","self-evolution","finite state machine","workflow adaptation","constrained optimization","critic mechanism","self-evolving memory","multi-hop QA","DeepSearch benchmark","interactive decision-making"],"organization":{"_id":"68b33ab6a9ed99140481cf44","name":"QuantaAlpha","fullname":"QuantaAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}},"publishedAt":"2026-01-14T08:19:13.000Z","title":"EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines","summary":"While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09465.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"organization":{"_id":"68b33ab6a9ed99140481cf44","name":"QuantaAlpha","fullname":"QuantaAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.06596","authors":[{"_id":"69685b660ac10a06522f6a09","name":"Hongjun An","hidden":false},{"_id":"69685b660ac10a06522f6a0a","name":"Yiliang Song","hidden":false},{"_id":"69685b660ac10a06522f6a0b","name":"Jiangan Chen","hidden":false},{"_id":"69685b660ac10a06522f6a0c","name":"Jiawei Shao","hidden":false},{"_id":"69685b660ac10a06522f6a0d","name":"Chi Zhang","hidden":false},{"_id":"69685b660ac10a06522f6a0e","name":"Xuelong Li","hidden":false}],"publishedAt":"2026-01-10T15:16:23.000Z","submittedOnDailyAt":"2026-01-15T00:58:08.622Z","title":"Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity","submittedOnDailyBy":{"_id":"65bc71766db364adbb0ee817","avatarUrl":"/avatars/4f889e82fed3aaff058bde7299b8d585.svg","isPro":false,"fullname":"Hongjun An","user":"Coder-AN","type":"user"},"summary":"Large Language Model (LLM) training often optimizes for preference alignment, rewarding outputs that are perceived as helpful and interaction-friendly. However, this preference-oriented objective can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from truth-oriented correction. In this work, we investigate whether aligned models are vulnerable to Preference-Undermining Attacks (PUA), a class of manipulative prompting strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a factorial evaluation framework to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled 2 times 2^4 design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for post-training processes like RLHF, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of preference alignment risks and the impact of manipulative prompts.","upvotes":11,"discussionId":"69685b670ac10a06522f6a0f","ai_summary":"Research examines how large language models can be manipulated through preference-undermining attacks that exploit alignment objectives, revealing model vulnerabilities and proposing a factorial evaluation method for diagnosing alignment risks.","ai_keywords":["large language model","preference alignment","preference-oriented objective","Preference-Undermining Attacks","RLHF","factorial evaluation framework","manipulative prompting","truth-oriented correction","post-training processes"]},"publishedAt":"2026-01-10T10:16:23.000Z","title":"Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity","summary":"Large Language Model (LLM) training often optimizes for preference alignment, rewarding outputs that are perceived as helpful and interaction-friendly. However, this preference-oriented objective can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from truth-oriented correction. In this work, we investigate whether aligned models are vulnerable to Preference-Undermining Attacks (PUA), a class of manipulative prompting strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a factorial evaluation framework to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled 2 times 2^4 design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for post-training processes like RLHF, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of preference alignment risks and the impact of manipulative prompts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06596.png","numComments":3,"submittedBy":{"_id":"65bc71766db364adbb0ee817","avatarUrl":"/avatars/4f889e82fed3aaff058bde7299b8d585.svg","fullname":"Hongjun An","name":"Coder-AN","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.09012","authors":[{"_id":"696869da0ac10a06522f6a98","name":"Mara Finkelstein","hidden":false},{"_id":"696869da0ac10a06522f6a99","name":"Isaac Caswell","hidden":false},{"_id":"696869da0ac10a06522f6a9a","name":"Tobias Domhan","hidden":false},{"_id":"696869da0ac10a06522f6a9b","name":"Jan-Thorsten Peter","hidden":false},{"_id":"696869da0ac10a06522f6a9c","name":"Juraj Juraska","hidden":false},{"_id":"696869da0ac10a06522f6a9d","name":"Parker Riley","hidden":false},{"_id":"696869da0ac10a06522f6a9e","name":"Daniel Deutsch","hidden":false},{"_id":"696869da0ac10a06522f6a9f","name":"Cole Dilanni","hidden":false},{"_id":"696869da0ac10a06522f6aa0","name":"Colin Cherry","hidden":false},{"_id":"696869da0ac10a06522f6aa1","name":"Eleftheria Briakou","hidden":false},{"_id":"696869da0ac10a06522f6aa2","name":"Elizabeth Nielsen","hidden":false},{"_id":"696869da0ac10a06522f6aa3","name":"Jiaming Luo","hidden":false},{"_id":"696869da0ac10a06522f6aa4","name":"Kat Black","hidden":false},{"_id":"696869da0ac10a06522f6aa5","user":{"_id":"65a402a53522df7a27125823","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a402a53522df7a27125823/tVR6lZlKmjPqi7dTZBGoS.jpeg","isPro":false,"fullname":"Ryan Mullins","user":"RyanMullins","type":"user"},"name":"Ryan Mullins","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:02:50.659Z","hidden":false},{"_id":"696869da0ac10a06522f6aa6","name":"Sweta Agrawal","hidden":false},{"_id":"696869da0ac10a06522f6aa7","name":"Wenda Xu","hidden":false},{"_id":"696869da0ac10a06522f6aa8","name":"Erin Kats","hidden":false},{"_id":"696869da0ac10a06522f6aa9","name":"Stephane Jaskiewicz","hidden":false},{"_id":"696869da0ac10a06522f6aaa","name":"Markus Freitag","hidden":false},{"_id":"696869da0ac10a06522f6aab","name":"David Vilar","hidden":false}],"publishedAt":"2026-01-13T22:23:24.000Z","submittedOnDailyAt":"2026-01-15T01:45:34.692Z","title":"TranslateGemma Technical Report","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.","upvotes":10,"discussionId":"696869da0ac10a06522f6aac","ai_summary":"TranslateGemma enhances Gemma 3's multilingual capabilities through two-stage fine-tuning with synthetic and human-translated data, achieving superior translation quality with improved efficiency.","ai_keywords":["machine translation","Gemma 3","two-stage fine-tuning","supervised fine-tuning","reinforcement learning","reward models","MetricX-QE","AutoMQM","WMT25","WMT24++","Vistra"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-01-13T17:23:24.000Z","title":"TranslateGemma Technical Report","summary":"We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09012.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.08955","authors":[{"_id":"696877d40ac10a06522f6abf","name":"Youwei Liu","hidden":false},{"_id":"696877d40ac10a06522f6ac0","user":{"_id":"63a4117984a6a25c65bc2fff","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63a4117984a6a25c65bc2fff/PxgTP_U5a5egCC8xvSh0s.png","isPro":false,"fullname":"Jian Wang","user":"jwanglvy","type":"user"},"name":"Jian Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:36.254Z","hidden":false},{"_id":"696877d40ac10a06522f6ac1","name":"Hanlin Wang","hidden":false},{"_id":"696877d40ac10a06522f6ac2","name":"Beichen Guo","hidden":false},{"_id":"696877d40ac10a06522f6ac3","name":"Wenjie Li","hidden":false}],"publishedAt":"2026-01-13T19:49:58.000Z","submittedOnDailyAt":"2026-01-15T03:31:06.215Z","title":"Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models","submittedOnDailyBy":{"_id":"63a4117984a6a25c65bc2fff","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63a4117984a6a25c65bc2fff/PxgTP_U5a5egCC8xvSh0s.png","isPro":false,"fullname":"Jian Wang","user":"jwanglvy","type":"user"},"summary":"Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially observable and imaginable Markov decision process to guide policy learning. We instantiate ITP with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.","upvotes":9,"discussionId":"696877d50ac10a06522f6ac4","ai_summary":"Imagine-then-Plan framework enables agent learning through adaptive lookahead imagination, combining imagined trajectories with current observations to guide policy learning in complex task scenarios.","ai_keywords":["world models","lookahead imagination","multi-step trajectories","adaptive lookahead mechanism","partially observable Markov decision process","policy learning","reinforcement training","agent benchmarks"]},"publishedAt":"2026-01-13T14:49:58.000Z","title":"Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models","summary":"Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially observable and imaginable Markov decision process to guide policy learning. We instantiate ITP with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08955.png","numComments":1,"submittedBy":{"_id":"63a4117984a6a25c65bc2fff","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63a4117984a6a25c65bc2fff/PxgTP_U5a5egCC8xvSh0s.png","fullname":"Jian Wang","name":"jwanglvy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.09697","authors":[{"_id":"6968654c0ac10a06522f6a56","name":"Jieying Chen","hidden":false},{"_id":"6968654c0ac10a06522f6a57","name":"Jeffrey Hu","hidden":false},{"_id":"6968654c0ac10a06522f6a58","name":"Joan Lasenby","hidden":false},{"_id":"6968654c0ac10a06522f6a59","name":"Ayush Tewari","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/eOKN4AvNtQiQsBxP-sroP.mp4"],"publishedAt":"2026-01-14T18:50:06.000Z","submittedOnDailyAt":"2026-01-15T01:26:04.275Z","title":"Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.","upvotes":5,"discussionId":"6968654c0ac10a06522f6a5a","ai_summary":"Diffusion-based video generation is made more efficient through keyframe-based 3D reconstruction and rendering, enabling faster synthesis with maintained visual quality.","ai_keywords":["diffusion models","video generation","keyframes","3D reconstruction","3D rendering","camera trajectory","geometric consistency","temporal stability"]},"publishedAt":"2026-01-14T13:50:06.000Z","title":"Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering","summary":"Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/eOKN4AvNtQiQsBxP-sroP.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09697.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.09173","authors":[{"_id":"696871160ac10a06522f6ab9","user":{"_id":"68acd2cd13c9b6d63b82d13d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68acd2cd13c9b6d63b82d13d/cSGk_84RSRAMbQWTUpJ--.png","isPro":false,"fullname":"Prashant Raju","user":"pcr2120","type":"user"},"name":"Prashant C. Raju","status":"claimed_verified","statusLastChangedAt":"2026-01-15T09:33:38.939Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/68acd2cd13c9b6d63b82d13d/YL7pMR4R3JLG58CRu6Rma.png","https://cdn-uploads.huggingface.co/production/uploads/68acd2cd13c9b6d63b82d13d/E2olX4I2S267g_RmfX8pS.jpeg","https://cdn-uploads.huggingface.co/production/uploads/68acd2cd13c9b6d63b82d13d/8DKSClALcQIMjjy41IBLy.jpeg"],"publishedAt":"2026-01-14T05:15:22.000Z","submittedOnDailyAt":"2026-01-15T09:19:28.899Z","title":"Geometric Stability: The Missing Axis of Representations","submittedOnDailyBy":{"_id":"68acd2cd13c9b6d63b82d13d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68acd2cd13c9b6d63b82d13d/cSGk_84RSRAMbQWTUpJ--.png","isPro":false,"fullname":"Prashant Raju","user":"pcr2120","type":"user"},"summary":"Analysis of learned representations has a blind spot: it focuses on similarity, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce geometric stability, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present Shesha, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated (approx 0.01) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2times more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability (= 0.89-0.96); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying how reliably systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.","upvotes":4,"discussionId":"696871160ac10a06522f6aba","githubRepo":"https://github.com/prashantcraju/geometric-stability","githubRepoAddedBy":"user","ai_summary":"Geometric stability measures representational robustness under perturbation, offering complementary insights to similarity metrics in analyzing learned representations across diverse domains.","ai_keywords":["learned representations","geometric stability","perturbation","representational geometry","Shesha","principal components","manifold structure","CKA","linear steerability","transferability","CRISPR perturbation","neural-behavioral coupling"],"githubStars":0},"publishedAt":"2026-01-14T00:15:22.000Z","title":"Geometric Stability: The Missing Axis of Representations","summary":"Analysis of learned representations has a blind spot: it focuses on similarity, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce geometric stability, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present Shesha, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated (approx 0.01) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2times more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability (= 0.89-0.96); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying how reliably systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/68acd2cd13c9b6d63b82d13d/YL7pMR4R3JLG58CRu6Rma.png","https://cdn-uploads.huggingface.co/production/uploads/68acd2cd13c9b6d63b82d13d/E2olX4I2S267g_RmfX8pS.jpeg","https://cdn-uploads.huggingface.co/production/uploads/68acd2cd13c9b6d63b82d13d/8DKSClALcQIMjjy41IBLy.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09173.png","numComments":1,"submittedBy":{"_id":"68acd2cd13c9b6d63b82d13d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68acd2cd13c9b6d63b82d13d/cSGk_84RSRAMbQWTUpJ--.png","fullname":"Prashant Raju","name":"pcr2120","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.09113","authors":[{"_id":"696867330ac10a06522f6a71","name":"Zixia Jia","hidden":false},{"_id":"696867330ac10a06522f6a72","name":"Jiaqi Li","hidden":false},{"_id":"696867330ac10a06522f6a73","name":"Yipeng Kang","hidden":false},{"_id":"696867330ac10a06522f6a74","name":"Yuxuan Wang","hidden":false},{"_id":"696867330ac10a06522f6a75","name":"Tong Wu","hidden":false},{"_id":"696867330ac10a06522f6a76","name":"Quansen Wang","hidden":false},{"_id":"696867330ac10a06522f6a77","name":"Xiaobo Wang","hidden":false},{"_id":"696867330ac10a06522f6a78","name":"Shuyi Zhang","hidden":false},{"_id":"696867330ac10a06522f6a79","name":"Junzhe Shen","hidden":false},{"_id":"696867330ac10a06522f6a7a","name":"Qing Li","hidden":false},{"_id":"696867330ac10a06522f6a7b","name":"Siyuan Qi","hidden":false},{"_id":"696867330ac10a06522f6a7c","name":"Yitao Liang","hidden":false},{"_id":"696867330ac10a06522f6a7d","name":"Di He","hidden":false},{"_id":"696867330ac10a06522f6a7e","name":"Zilong Zheng","hidden":false},{"_id":"696867330ac10a06522f6a7f","name":"Song-Chun Zhu","hidden":false}],"publishedAt":"2026-01-14T03:24:08.000Z","submittedOnDailyAt":"2026-01-15T01:34:09.547Z","title":"The AI Hippocampus: How Far are We From Human Memory?","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.","upvotes":4,"discussionId":"696867330ac10a06522f6a80","ai_summary":"Memory mechanisms in large language models and multi-modal language models are categorized into implicit, explicit, and agentic paradigms, supporting enhanced reasoning, adaptability, and contextual fidelity through internal parameters, external knowledge storage, and persistent agent memory structures.","ai_keywords":["Large Language Models","Multi-Modal LLMs","memory mechanisms","implicit memory","explicit memory","agentic memory","transformers","memorization","associative retrieval","contextual reasoning","external storage","dynamic knowledge representations","persistent memory structures","long-term planning","self-consistency","multi-agent systems","embodied AI","interactive AI","memory capacity","factual consistency","cross-system interoperability"]},"publishedAt":"2026-01-13T22:24:08.000Z","title":"The AI Hippocampus: How Far are We From Human Memory?","summary":"Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09113.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":209,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.01075","authors":[{"_id":"69693de932f0333869ff9357","name":"Hansen Jin Lillemark","hidden":false},{"_id":"69693de932f0333869ff9358","name":"Benhao Huang","hidden":false},{"_id":"69693de932f0333869ff9359","name":"Fangneng Zhan","hidden":false},{"_id":"69693de932f0333869ff935a","name":"Yilun Du","hidden":false},{"_id":"69693de932f0333869ff935b","name":"Thomas Anderson Keller","hidden":false}],"publishedAt":"2026-01-03T05:22:27.000Z","submittedOnDailyAt":"2026-01-15T16:54:28.180Z","title":"Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments","submittedOnDailyBy":{"_id":"64b22e6b0a54158d66f18688","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b22e6b0a54158d66f18688/Cbl3oMMMANbnCMoSUYenI.png","isPro":true,"fullname":"Benhao Huang","user":"HuskyDoge","type":"user"},"summary":"Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.","upvotes":3,"discussionId":"69693de932f0333869ff935c","projectPage":"https://flowequivariantworldmodels.github.io/","githubRepo":"https://github.com/hlillemark/flowm","githubRepoAddedBy":"user","ai_summary":"Flow Equivariant World Models unify self-motion and external object motion as one-parameter Lie group flows to create stable, symmetry-guided representations for embodied intelligence.","ai_keywords":["world models","Lie group flows","group equivariance","embodied systems","sensory input","time-parameterized symmetries","latent world representation","diffusion-based models","memory-augmented architectures","long rollouts","data efficiency"],"githubStars":5},"publishedAt":"2026-01-03T00:22:27.000Z","title":"Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments","summary":"Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01075.png","numComments":1,"submittedBy":{"_id":"64b22e6b0a54158d66f18688","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b22e6b0a54158d66f18688/Cbl3oMMMANbnCMoSUYenI.png","fullname":"Benhao Huang","name":"HuskyDoge","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.09609","authors":[{"_id":"696913db8854afb90e2fc9d7","name":"Qian Cao","hidden":false},{"_id":"696913db8854afb90e2fc9d8","name":"Yahui Liu","hidden":false},{"_id":"696913db8854afb90e2fc9d9","name":"Wei Bi","hidden":false},{"_id":"696913db8854afb90e2fc9da","name":"Yi Zhao","hidden":false},{"_id":"696913db8854afb90e2fc9db","name":"Ruihua Song","hidden":false},{"_id":"696913db8854afb90e2fc9dc","name":"Xiting Wang","hidden":false},{"_id":"696913db8854afb90e2fc9dd","name":"Ruiming Tang","hidden":false},{"_id":"696913db8854afb90e2fc9de","name":"Guorui Zhou","hidden":false},{"_id":"696913db8854afb90e2fc9df","name":"Han Li","hidden":false}],"publishedAt":"2026-01-14T16:30:20.000Z","submittedOnDailyAt":"2026-01-15T13:51:24.934Z","title":"DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing","submittedOnDailyBy":{"_id":"5fbdf878485ef14d9a960f4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg","isPro":false,"fullname":"Qian Cao","user":"Aman","type":"user"},"summary":"Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.","upvotes":1,"discussionId":"696913db8854afb90e2fc9e0","ai_summary":"A reinforcement learning framework with diverse planning branching and group-aware rewards enhances large language model output diversity in creative writing tasks.","ai_keywords":["reinforcement learning","large language models","Chain-of-Thought","diverse planning branching","group-aware diversity reward","creative writing benchmarks"]},"publishedAt":"2026-01-14T11:30:20.000Z","title":"DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing","summary":"Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09609.png","numComments":1,"submittedBy":{"_id":"5fbdf878485ef14d9a960f4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg","fullname":"Qian Cao","name":"Aman","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.09536","authors":[{"_id":"6968787c0ac10a06522f6aca","name":"Dongjie Cheng","hidden":false},{"_id":"6968787c0ac10a06522f6acb","name":"Yongqi Li","hidden":false},{"_id":"6968787c0ac10a06522f6acc","name":"Zhixin Ma","hidden":false},{"_id":"6968787c0ac10a06522f6acd","name":"Hongru Cai","hidden":false},{"_id":"6968787c0ac10a06522f6ace","name":"Yupeng Hu","hidden":false},{"_id":"6968787c0ac10a06522f6acf","name":"Wenjie Wang","hidden":false},{"_id":"6968787c0ac10a06522f6ad0","name":"Liqiang Nie","hidden":false},{"_id":"6968787c0ac10a06522f6ad1","name":"Wenjie Li","hidden":false}],"publishedAt":"2026-01-14T14:57:33.000Z","submittedOnDailyAt":"2026-01-15T09:47:29.628Z","title":"Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning","submittedOnDailyBy":{"_id":"64ac01f8ac13891808807e01","avatarUrl":"/avatars/25a2e2ad943581521ad488d00bf37738.svg","isPro":true,"fullname":"charlie","user":"charlesdj","type":"user"},"summary":"Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.","upvotes":1,"discussionId":"6968787c0ac10a06522f6ad2","ai_summary":"Unified generative multimodal reasoning approach enables diverse reasoning skills through intermediate image generation, with a two-stage SFT+RL framework and a text-only bootstrapping variant.","ai_keywords":["Multimodal Large Language Models","multimodal reasoning","generative multimodal reasoning","perception alignment loss","perception reward","SFT+RL framework","intermediate image generation","Omni-R1","Omni-R1-Zero"],"organization":{"_id":"69396d0f6ef210a3d45ac4b7","name":"ModalityDance","fullname":"Modality Dance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/653a111eee5888edef9182cf/7BPn5_PnfH27PkAaLQnxW.png"}},"publishedAt":"2026-01-14T09:57:33.000Z","title":"Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning","summary":"Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09536.png","numComments":1,"submittedBy":{"_id":"64ac01f8ac13891808807e01","avatarUrl":"/avatars/25a2e2ad943581521ad488d00bf37738.svg","fullname":"charlie","name":"charlesdj","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"69396d0f6ef210a3d45ac4b7","name":"ModalityDance","fullname":"Modality Dance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/653a111eee5888edef9182cf/7BPn5_PnfH27PkAaLQnxW.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.07287","authors":[{"_id":"69671bdac5e371f6b235d1c4","name":"Yuanyang Yin","hidden":false},{"_id":"69671bdac5e371f6b235d1c5","user":{"_id":"68fce03ed1d0efce7ca87075","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png","isPro":false,"fullname":"yfdeng","user":"yfdeng10","type":"user"},"name":"Yufan Deng","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:51:08.659Z","hidden":false},{"_id":"69671bdac5e371f6b235d1c6","name":"Shenghai Yuan","hidden":false},{"_id":"69671bdac5e371f6b235d1c7","name":"Kaipeng Zhang","hidden":false},{"_id":"69671bdac5e371f6b235d1c8","name":"Xiao Yang","hidden":false},{"_id":"69671bdac5e371f6b235d1c9","name":"Feng Zhao","hidden":false}],"publishedAt":"2026-01-12T07:48:26.000Z","submittedOnDailyAt":"2026-01-15T03:35:54.897Z","title":"Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models","submittedOnDailyBy":{"_id":"68fce03ed1d0efce7ca87075","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png","isPro":false,"fullname":"yfdeng","user":"yfdeng10","type":"user"},"summary":"The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\\%).","upvotes":1,"discussionId":"69671bdac5e371f6b235d1ca","ai_summary":"Diffusion Transformer-based image-to-video models suffer from condition isolation where visual attention becomes detached from text guidance; focal guidance addresses this through fine-grained semantic guidance and attention cache mechanisms.","ai_keywords":["diffusion models","diffusion transformer","image-to-video generation","text prompt","denoising process","semantic responses","condition isolation","attention mechanisms","clip","fine-grained semantic guidance","attention cache"]},"publishedAt":"2026-01-12T02:48:26.000Z","title":"Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models","summary":"The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\\%).","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07287.png","numComments":1,"submittedBy":{"_id":"68fce03ed1d0efce7ca87075","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png","fullname":"yfdeng","name":"yfdeng10","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.06794","authors":[{"_id":"696918718854afb90e2fc9e2","name":"Zhicong Li","hidden":false},{"_id":"696918718854afb90e2fc9e3","name":"Lingjie Jiang","hidden":false},{"_id":"696918718854afb90e2fc9e4","name":"Yulan Hu","hidden":false},{"_id":"696918718854afb90e2fc9e5","name":"Xingchen Zeng","hidden":false},{"_id":"696918718854afb90e2fc9e6","name":"Yixia Li","hidden":false},{"_id":"696918718854afb90e2fc9e7","name":"Xiangwen Zhang","hidden":false},{"_id":"696918718854afb90e2fc9e8","name":"Guanhua Chen","hidden":false},{"_id":"696918718854afb90e2fc9e9","name":"Zheng Pan","hidden":false},{"_id":"696918718854afb90e2fc9ea","name":"Xin Li","hidden":false},{"_id":"696918718854afb90e2fc9eb","name":"Yong Liu","hidden":false}],"publishedAt":"2026-01-11T07:29:08.000Z","submittedOnDailyAt":"2026-01-15T14:10:44.196Z","title":"No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning","submittedOnDailyBy":{"_id":"66ab80e9bfb7d73a56bc293c","avatarUrl":"/avatars/9644266304c832c74ef572b5eb2d9468.svg","isPro":false,"fullname":"Jack","user":"lingjie23","type":"user"},"summary":"Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.","upvotes":1,"discussionId":"696918728854afb90e2fc9ec","ai_summary":"ECHO is a reinforcement learning framework that jointly optimizes policy and critic through co-evolutionary loops, addressing staleness issues in critique-guided training of language model agents.","ai_keywords":["reinforcement learning","LLM agents","sparse outcome rewards","natural-language feedback","stationary critics","on-policy RL","policy evolution","critic adaptation","co-evolutionary loop","cascaded rollout mechanism","group-structured advantage estimation","saturation-aware gain shaping","dual-track GRPO updates"]},"publishedAt":"2026-01-11T02:29:08.000Z","title":"No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning","summary":"Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06794.png","numComments":1,"submittedBy":{"_id":"66ab80e9bfb7d73a56bc293c","avatarUrl":"/avatars/9644266304c832c74ef572b5eb2d9468.svg","fullname":"Jack","name":"lingjie23","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.04809","authors":[{"_id":"696864120ac10a06522f6a43","user":{"_id":"677a4c4de0db74115dece89a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/677a4c4de0db74115dece89a/MEUOgReiC6dW4TEH2aJAA.jpeg","isPro":false,"fullname":"Caijun Xu","user":"SII-Molu","type":"user"},"name":"Caijun Xu","status":"admin_assigned","statusLastChangedAt":"2026-01-15T19:08:52.927Z","hidden":false},{"_id":"696864120ac10a06522f6a44","name":"Changyi Xiao","hidden":false},{"_id":"696864120ac10a06522f6a45","name":"Zhongyuan Peng","hidden":false},{"_id":"696864120ac10a06522f6a46","name":"Xinrun Wang","hidden":false},{"_id":"696864120ac10a06522f6a47","name":"Yixin Cao","hidden":false}],"publishedAt":"2026-01-08T10:42:04.000Z","submittedOnDailyAt":"2026-01-15T21:27:34.242Z","title":"SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning","submittedOnDailyBy":{"_id":"677a4c4de0db74115dece89a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/677a4c4de0db74115dece89a/MEUOgReiC6dW4TEH2aJAA.jpeg","isPro":false,"fullname":"Caijun Xu","user":"SII-Molu","type":"user"},"summary":"Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.","upvotes":1,"discussionId":"696864130ac10a06522f6a48","projectPage":"https://github.com/molumolua/SCALER","githubRepo":"https://github.com/molumolua/SCALER","githubRepoAddedBy":"user","ai_summary":"SCALER is a reinforcement learning framework that maintains effective training signals for language models through adaptive environment design and multi-environment strategies, enabling sustained performance improvements in reasoning tasks.","ai_keywords":["reinforcement learning","large language models","reasoning capabilities","training signals","synthetic environment","adaptive environment design","scalable synthesis pipeline","verifiable reasoning environments","controllable difficulty","instance generation","adaptive multi-environment RL","capability frontier","distributional diversity","reward sparsity","overfitting","long-horizon training"],"githubStars":6},"publishedAt":"2026-01-08T05:42:04.000Z","title":"SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning","summary":"Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04809.png","numComments":1,"submittedBy":{"_id":"677a4c4de0db74115dece89a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/677a4c4de0db74115dece89a/MEUOgReiC6dW4TEH2aJAA.jpeg","fullname":"Caijun Xu","name":"SII-Molu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.09282","authors":[{"_id":"6968de528bd7789bd70cc493","user":{"_id":"68c90f3b622d9314451ff9ff","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68c90f3b622d9314451ff9ff/Xm00KSMoQQ2bef3c2Gbt7.jpeg","isPro":false,"fullname":"Leszek Sliwko","user":"lsliwko","type":"user"},"name":"Leszek Sliwko","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:02:33.478Z","hidden":false},{"_id":"6968de528bd7789bd70cc494","name":"Jolanta Mizeria-Pietraszko","hidden":false}],"publishedAt":"2026-01-14T08:36:21.000Z","submittedOnDailyAt":"2026-01-15T12:40:40.003Z","title":"Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing","submittedOnDailyBy":{"_id":"68c90f3b622d9314451ff9ff","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68c90f3b622d9314451ff9ff/Xm00KSMoQQ2bef3c2Gbt7.jpeg","isPro":false,"fullname":"Leszek Sliwko","user":"lsliwko","type":"user"},"summary":"Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.","upvotes":0,"discussionId":"6968de528bd7789bd70cc495","ai_summary":"A semantic, intent-driven scheduling approach uses large language models to interpret natural language hints for workload allocation in cluster systems, achieving high accuracy and improved placement compared to traditional methods.","ai_keywords":["Large Language Model","Kubernetes scheduler extender","natural language processing","intent-driven scheduling","semantic soft affinity","cluster workload allocation"]},"publishedAt":"2026-01-14T03:36:21.000Z","title":"Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing","summary":"Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09282.png","numComments":1,"submittedBy":{"_id":"68c90f3b622d9314451ff9ff","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68c90f3b622d9314451ff9ff/Xm00KSMoQQ2bef3c2Gbt7.jpeg","fullname":"Leszek Sliwko","name":"lsliwko","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.08472","authors":[{"_id":"6967548ec5e371f6b235d22e","name":"Benedikt Droste","hidden":false},{"_id":"6967548ec5e371f6b235d22f","name":"Jan Philipp Harries","hidden":false},{"_id":"6967548ec5e371f6b235d230","name":"Maximilian Idahl","hidden":false},{"_id":"6967548ec5e371f6b235d231","name":"Bjrn Plster","hidden":false}],"publishedAt":"2026-01-13T11:59:15.000Z","submittedOnDailyAt":"2026-01-15T17:22:07.226Z","title":"sui-1: Grounded and Verifiable Long-Form Summarization","submittedOnDailyBy":{"_id":"6448e79fe988635a3d6ad05b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6448e79fe988635a3d6ad05b/IgYgFxakZCP0bFqojYa53.jpeg","isPro":true,"fullname":"Benedikt","user":"benediktnlp","type":"user"},"summary":"Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available.","upvotes":0,"discussionId":"6967548fc5e371f6b235d232","ai_summary":"A 24 billion parameter model generates abstractive summaries with inline citations through synthetic data training, outperforming larger models in accuracy and verifiability.","ai_keywords":["large language models","abstractive summaries","inline citations","chain-of-thought prompting","multi-stage verification","synthetic data pipeline","task-specific training"],"organization":{"_id":"64cbf83c8256a8efea5ea4d5","name":"ellamind","fullname":"ellamind","avatar":"https://cdn-uploads.huggingface.co/production/uploads/647ef81ce9c81260ff84fdd7/G2Ftt38LatXWWu3CX9izs.png"}},"publishedAt":"2026-01-13T06:59:15.000Z","title":"sui-1: Grounded and Verifiable Long-Form Summarization","summary":"Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08472.png","numComments":1,"submittedBy":{"_id":"6448e79fe988635a3d6ad05b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6448e79fe988635a3d6ad05b/IgYgFxakZCP0bFqojYa53.jpeg","fullname":"Benedikt","name":"benediktnlp","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"64cbf83c8256a8efea5ea4d5","name":"ellamind","fullname":"ellamind","avatar":"https://cdn-uploads.huggingface.co/production/uploads/647ef81ce9c81260ff84fdd7/G2Ftt38LatXWWu3CX9izs.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.04469","authors":[{"_id":"696908c58854afb90e2fc9b5","name":"Iaroslav Chelombitko","hidden":false},{"_id":"696908c58854afb90e2fc9b6","name":"Ekaterina Chelombitko","hidden":false},{"_id":"696908c58854afb90e2fc9b7","name":"Aleksey Komissarov","hidden":false}],"publishedAt":"2026-01-08T01:05:51.000Z","submittedOnDailyAt":"2026-01-15T13:10:43.934Z","title":"SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers","submittedOnDailyBy":{"_id":"65030ef13e61bc0192252999","avatarUrl":"/avatars/c1f46bbffc36765ad1bba4bb1cbe3612.svg","isPro":false,"fullname":"Iaroslav","user":"Aragoner","type":"user"},"summary":"The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.\n  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.\n  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the \"elbow points\" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP","upvotes":0,"discussionId":"696908c58854afb90e2fc9b8","ai_summary":"A corpus-free toolkit for morphological lexicon creation using MDL-inspired scoring enables systematic evaluation of BPE tokenizers for morphologically rich Uralic languages, establishing optimal vocabulary sizes through integrated performance metrics.","ai_keywords":["morphological lexicon creation","MDL-inspired Self-Referential Atomicity Scoring","BPE tokenizers","Integrated Performance Score","elbow points","morpheme coverage","over-splitting","agglutinative languages"]},"publishedAt":"2026-01-07T20:05:51.000Z","title":"SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers","summary":"The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.\n  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.\n  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the \"elbow points\" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04469.png","numComments":1,"submittedBy":{"_id":"65030ef13e61bc0192252999","avatarUrl":"/avatars/c1f46bbffc36765ad1bba4bb1cbe3612.svg","fullname":"Iaroslav","name":"Aragoner","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false}]
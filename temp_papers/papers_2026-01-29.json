[{"paper":{"id":"2601.20614","authors":[{"_id":"697ac91bdf3e800774f13c12","name":"Yanqi Dai","hidden":false},{"_id":"697ac91bdf3e800774f13c13","name":"Yuxiang Ji","hidden":false},{"_id":"697ac91bdf3e800774f13c14","name":"Xiao Zhang","hidden":false},{"_id":"697ac91bdf3e800774f13c15","name":"Yong Wang","hidden":false},{"_id":"697ac91bdf3e800774f13c16","name":"Xiangxiang Chu","hidden":false},{"_id":"697ac91bdf3e800774f13c17","name":"Zhiwu Lu","hidden":false}],"publishedAt":"2026-01-28T13:49:23.000Z","submittedOnDailyAt":"2026-01-29T00:15:35.371Z","title":"Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation","submittedOnDailyBy":{"_id":"66cde57cb1fe4c78fe3ab770","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg","isPro":false,"fullname":"Yanqi Dai","user":"YanqiDai","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.","upvotes":93,"discussionId":"697ac91bdf3e800774f13c18","githubRepo":"https://github.com/AMAP-ML/MathForge","githubRepoAddedBy":"user","ai_summary":"MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","Group Relative Policy Optimization","Difficulty-Aware Group Policy Optimization","Multi-Aspect Question Reformulation","mathematical reasoning","policy updates","group advantage estimation","question-level weighting","data augmentation"],"githubStars":84,"organization":{"_id":"67d11771890254196d3174e5","name":"GD-ML","fullname":"AMAP-ML","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}},"publishedAt":"2026-01-28T08:49:23.000Z","title":"Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20614.png","numComments":12,"submittedBy":{"_id":"66cde57cb1fe4c78fe3ab770","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg","fullname":"Yanqi Dai","name":"YanqiDai","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"67d11771890254196d3174e5","name":"GD-ML","fullname":"AMAP-ML","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20540","authors":[{"_id":"697ac48cdf3e800774f13bc1","name":"Robbyant Team","hidden":false},{"_id":"697ac48cdf3e800774f13bc2","name":"Zelin Gao","hidden":false},{"_id":"697ac48cdf3e800774f13bc3","user":{"_id":"64981bea09cea550852652af","avatarUrl":"/avatars/df528e9008972c8e5ae4d278e617476c.svg","isPro":false,"fullname":"Qiuyu Wang","user":"qiuyuu","type":"user"},"name":"Qiuyu Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-29T09:16:29.190Z","hidden":false},{"_id":"697ac48cdf3e800774f13bc4","name":"Yanhong Zeng","hidden":false},{"_id":"697ac48cdf3e800774f13bc5","name":"Jiapeng Zhu","hidden":false},{"_id":"697ac48cdf3e800774f13bc6","user":{"_id":"64acd2ec39fcfebff8c79c00","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64acd2ec39fcfebff8c79c00/Avq66l5hO-aggNtk4Y1ss.png","isPro":false,"fullname":"Ka Leong Cheng","user":"felixcheng97","type":"user"},"name":"Ka Leong Cheng","status":"claimed_verified","statusLastChangedAt":"2026-01-29T13:56:36.041Z","hidden":false},{"_id":"697ac48cdf3e800774f13bc7","name":"Yixuan Li","hidden":false},{"_id":"697ac48cdf3e800774f13bc8","user":{"_id":"665f059a8947302aa2c63afe","avatarUrl":"/avatars/50f560285946532321a0bd526494148d.svg","isPro":false,"fullname":"hanlin wang","user":"hlwang06","type":"user"},"name":"Hanlin Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-29T09:18:10.952Z","hidden":false},{"_id":"697ac48cdf3e800774f13bc9","name":"Yinghao Xu","hidden":false},{"_id":"697ac48cdf3e800774f13bca","name":"Shuailei Ma","hidden":false},{"_id":"697ac48cdf3e800774f13bcb","name":"Yihang Chen","hidden":false},{"_id":"697ac48cdf3e800774f13bcc","name":"Jie Liu","hidden":false},{"_id":"697ac48cdf3e800774f13bcd","name":"Yansong Cheng","hidden":false},{"_id":"697ac48cdf3e800774f13bce","name":"Yao Yao","hidden":false},{"_id":"697ac48cdf3e800774f13bcf","name":"Jiayi Zhu","hidden":false},{"_id":"697ac48cdf3e800774f13bd0","user":{"_id":"656084f44e8918182d4f07c8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/akAvCUCi7eR31PWOXrVPw.jpeg","isPro":false,"fullname":"Yihao Meng","user":"Yhmeng1106","type":"user"},"name":"Yihao Meng","status":"claimed_verified","statusLastChangedAt":"2026-01-29T13:56:37.840Z","hidden":false},{"_id":"697ac48cdf3e800774f13bd1","name":"Kecheng Zheng","hidden":false},{"_id":"697ac48cdf3e800774f13bd2","user":{"_id":"63f0baf66309c84d5f4a2226","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg","isPro":true,"fullname":"Qingyan","user":"QingyanBai","type":"user"},"name":"Qingyan Bai","status":"claimed_verified","statusLastChangedAt":"2026-01-29T09:16:24.535Z","hidden":false},{"_id":"697ac48cdf3e800774f13bd3","user":{"_id":"6478a982256b62e219917d67","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PUJ-N2cQxgEmDGfyjajyA.jpeg","isPro":false,"fullname":"JingyeChen22","user":"JingyeChen22","type":"user"},"name":"Jingye Chen","status":"claimed_verified","statusLastChangedAt":"2026-01-29T09:17:37.828Z","hidden":false},{"_id":"697ac48cdf3e800774f13bd4","name":"Zehong Shen","hidden":false},{"_id":"697ac48cdf3e800774f13bd5","user":{"_id":"662128ec9ca2cd4e6db2fb44","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/662128ec9ca2cd4e6db2fb44/uUg1V-pVfxT3mLuFgJuAN.jpeg","isPro":false,"fullname":"Bruce Yu","user":"bruceyyu","type":"user"},"name":"Yue Yu","status":"claimed_verified","statusLastChangedAt":"2026-01-29T09:16:27.115Z","hidden":false},{"_id":"697ac48cdf3e800774f13bd6","name":"Xing Zhu","hidden":false},{"_id":"697ac48cdf3e800774f13bd7","name":"Yujun Shen","hidden":false},{"_id":"697ac48cdf3e800774f13bd8","name":"Hao Ouyang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"],"publishedAt":"2026-01-28T12:37:01.000Z","submittedOnDailyAt":"2026-01-29T00:09:39.166Z","title":"Advancing Open-source World Models","submittedOnDailyBy":{"_id":"64981bea09cea550852652af","avatarUrl":"/avatars/df528e9008972c8e5ae4d278e617476c.svg","isPro":false,"fullname":"Qiuyu Wang","user":"qiuyuu","type":"user"},"summary":"We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.","upvotes":66,"discussionId":"697ac48cdf3e800774f13bd9","projectPage":"https://technology.robbyant.com/lingbot-world","githubRepo":"https://github.com/Robbyant/lingbot-world/","githubRepoAddedBy":"user","ai_summary":"LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.","ai_keywords":["world simulator","video generation","world model","long-term memory","real-time interactivity"],"githubStars":756,"organization":{"_id":"69709f892cd08371c1011a2e","name":"robbyant","fullname":"Robbyant","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}},"publishedAt":"2026-01-28T07:37:01.000Z","title":"Advancing Open-source World Models","summary":"We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20540.png","numComments":1,"submittedBy":{"_id":"64981bea09cea550852652af","avatarUrl":"/avatars/df528e9008972c8e5ae4d278e617476c.svg","fullname":"Qiuyu Wang","name":"qiuyuu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"69709f892cd08371c1011a2e","name":"robbyant","fullname":"Robbyant","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.19325","authors":[{"_id":"69798298df44b75fa47e47a9","name":"Zichen Wen","hidden":false},{"_id":"69798298df44b75fa47e47aa","user":{"_id":"688c72c011ef3399b561dee7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/688c72c011ef3399b561dee7/puhgnTOAfZYetsC46hqGm.jpeg","isPro":false,"fullname":"BoxueYang","user":"Boxue","type":"user"},"name":"Boxue Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-29T09:17:12.982Z","hidden":false},{"_id":"69798298df44b75fa47e47ab","name":"Shuang Chen","hidden":false},{"_id":"69798298df44b75fa47e47ac","name":"Yaojie Zhang","hidden":false},{"_id":"69798298df44b75fa47e47ad","name":"Yuhang Han","hidden":false},{"_id":"69798298df44b75fa47e47ae","name":"Junlong Ke","hidden":false},{"_id":"69798298df44b75fa47e47af","name":"Cong Wang","hidden":false},{"_id":"69798298df44b75fa47e47b0","name":"Yicheng Fu","hidden":false},{"_id":"69798298df44b75fa47e47b1","name":"Jiawang Zhao","hidden":false},{"_id":"69798298df44b75fa47e47b2","name":"Jiangchao Yao","hidden":false},{"_id":"69798298df44b75fa47e47b3","name":"Xi Fang","hidden":false},{"_id":"69798298df44b75fa47e47b4","name":"Zhen Wang","hidden":false},{"_id":"69798298df44b75fa47e47b5","name":"Henxing Cai","hidden":false},{"_id":"69798298df44b75fa47e47b6","name":"Lin Yao","hidden":false},{"_id":"69798298df44b75fa47e47b7","name":"Zhifeng Gao","hidden":false},{"_id":"69798298df44b75fa47e47b8","name":"Yanhui Hong","hidden":false},{"_id":"69798298df44b75fa47e47b9","name":"Nang Yuan","hidden":false},{"_id":"69798298df44b75fa47e47ba","name":"Yixuan Li","hidden":false},{"_id":"69798298df44b75fa47e47bb","name":"Guojiang Zhao","hidden":false},{"_id":"69798298df44b75fa47e47bc","name":"Haoyi Tao","hidden":false},{"_id":"69798298df44b75fa47e47bd","name":"Nan Wang","hidden":false},{"_id":"69798298df44b75fa47e47be","name":"Han Lyu","hidden":false},{"_id":"69798298df44b75fa47e47bf","name":"Guolin Ke","hidden":false},{"_id":"69798298df44b75fa47e47c0","name":"Ning Liao","hidden":false},{"_id":"69798298df44b75fa47e47c1","name":"Xiaoxing Wang","hidden":false},{"_id":"69798298df44b75fa47e47c2","name":"Kai Chen","hidden":false},{"_id":"69798298df44b75fa47e47c3","name":"Zhiyu Li","hidden":false},{"_id":"69798298df44b75fa47e47c4","name":"Feiyu Xiong","hidden":false},{"_id":"69798298df44b75fa47e47c5","name":"Sihan Hu","hidden":false},{"_id":"69798298df44b75fa47e47c6","name":"Kun Chen","hidden":false},{"_id":"69798298df44b75fa47e47c7","name":"Yanfeng Wang","hidden":false},{"_id":"69798298df44b75fa47e47c8","name":"Weinan E","hidden":false},{"_id":"69798298df44b75fa47e47c9","name":"Linfeng Zhang","hidden":false},{"_id":"69798298df44b75fa47e47ca","name":"Linfeng Zhang","hidden":false}],"publishedAt":"2026-01-27T08:12:18.000Z","submittedOnDailyAt":"2026-01-29T01:20:58.570Z","title":"Innovator-VL: A Multimodal Large Language Model for Scientific Discovery","submittedOnDailyBy":{"_id":"653b8c3e97a4d71d950e2f20","avatarUrl":"/avatars/b68880022e14556d0be58c69615db3be.svg","isPro":false,"fullname":"Zichen Wen","user":"zichenwen","type":"user"},"summary":"We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.","upvotes":53,"discussionId":"69798298df44b75fa47e47cb","projectPage":"https://innovatorlm.github.io/Innovator-VL","githubRepo":"https://github.com/InnovatorLM/Innovator-VL","githubRepoAddedBy":"user","ai_summary":"Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.","ai_keywords":["multimodal large language model","scientific multimodal large language model","end-to-end reproducible training pipeline","supervised fine-tuning","reinforcement learning","scientific reasoning","data efficiency","principled data selection","generalization","scientific alignment"],"githubStars":70,"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}},"publishedAt":"2026-01-27T03:12:18.000Z","title":"Innovator-VL: A Multimodal Large Language Model for Scientific Discovery","summary":"We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19325.png","numComments":1,"submittedBy":{"_id":"653b8c3e97a4d71d950e2f20","avatarUrl":"/avatars/b68880022e14556d0be58c69615db3be.svg","fullname":"Zichen Wen","name":"zichenwen","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":13,"isUserFollowing":false},"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20552","authors":[{"_id":"697acf36df3e800774f13c41","name":"Haoran Wei","hidden":false},{"_id":"697acf36df3e800774f13c42","name":"Yaofeng Sun","hidden":false},{"_id":"697acf36df3e800774f13c43","name":"Yukun Li","hidden":false}],"publishedAt":"2026-01-28T12:46:07.000Z","submittedOnDailyAt":"2026-01-29T00:38:50.729Z","title":"DeepSeek-OCR 2: Visual Causal Flow","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.","upvotes":26,"discussionId":"697acf37df3e800774f13c44","githubRepo":"https://github.com/deepseek-ai/DeepSeek-OCR-2","githubRepoAddedBy":"user","ai_summary":"DeepSeek-OCR 2 introduces DeepEncoder V2 that dynamically reorders visual tokens based on semantic content, enabling more human-like causal reasoning in 2D image understanding through cascaded 1D causal structures.","ai_keywords":["encoder-DeepEncoder V2","visual tokens","vision-language models","raster-scan order","positional encoding","causal reasoning","2D image understanding","cascaded 1D causal reasoning"],"githubStars":1558,"organization":{"_id":"652faff917096ceb6bf53f3f","name":"deepseek-ai","fullname":"DeepSeek","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}},"publishedAt":"2026-01-28T07:46:07.000Z","title":"DeepSeek-OCR 2: Visual Causal Flow","summary":"We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20552.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":219,"isUserFollowing":false},"organization":{"_id":"652faff917096ceb6bf53f3f","name":"deepseek-ai","fullname":"DeepSeek","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20209","authors":[{"_id":"697acf8ddf3e800774f13c46","user":{"_id":"6747de57f8cab58c22ec94a2","avatarUrl":"/avatars/5bae0341862fac24564781c0fa32aac5.svg","isPro":false,"fullname":"Jinyang Wu","user":"Jinyang23","type":"user"},"name":"Jinyang Wu","status":"claimed_verified","statusLastChangedAt":"2026-01-29T09:16:13.804Z","hidden":false},{"_id":"697acf8ddf3e800774f13c47","name":"Shuo Yang","hidden":false},{"_id":"697acf8ddf3e800774f13c48","name":"Changpeng Yang","hidden":false},{"_id":"697acf8ddf3e800774f13c49","name":"Yuhao Shen","hidden":false},{"_id":"697acf8ddf3e800774f13c4a","name":"Shuai Zhang","hidden":false},{"_id":"697acf8ddf3e800774f13c4b","name":"Zhengqi Wen","hidden":false},{"_id":"697acf8ddf3e800774f13c4c","name":"Jianhua Tao","hidden":false}],"publishedAt":"2026-01-28T03:15:34.000Z","submittedOnDailyAt":"2026-01-29T00:45:22.548Z","title":"Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning","submittedOnDailyBy":{"_id":"6747de57f8cab58c22ec94a2","avatarUrl":"/avatars/5bae0341862fac24564781c0fa32aac5.svg","isPro":false,"fullname":"Jinyang Wu","user":"Jinyang23","type":"user"},"summary":"Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose Spark (Strategic Policy-Aware exploRation via Key-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that Spark achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.","upvotes":12,"discussionId":"697acf8edf3e800774f13c4d","ai_summary":"Spark is a reinforcement learning framework that strategically allocates computational resources by branching at critical decision states, improving sample efficiency and generalization for long-horizon tasks.","ai_keywords":["reinforcement learning","large language models","long-horizon tasks","trajectory scarcity","rollout size","computational resource allocation","adaptive branching exploration","decision-making signals","sample efficiency","generalization"]},"publishedAt":"2026-01-27T22:15:34.000Z","title":"Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning","summary":"Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose Spark (Strategic Policy-Aware exploRation via Key-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that Spark achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20209.png","numComments":1,"submittedBy":{"_id":"6747de57f8cab58c22ec94a2","avatarUrl":"/avatars/5bae0341862fac24564781c0fa32aac5.svg","fullname":"Jinyang Wu","name":"Jinyang23","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.20834","authors":[{"_id":"697ad00edf3e800774f13c4f","name":"Andrew Kyle Lampinen","hidden":false},{"_id":"697ad00edf3e800774f13c50","name":"Yuxuan Li","hidden":false},{"_id":"697ad00edf3e800774f13c51","name":"Eghbal Hosseini","hidden":false},{"_id":"697ad00edf3e800774f13c52","name":"Sangnie Bhardwaj","hidden":false},{"_id":"697ad00edf3e800774f13c53","name":"Murray Shanahan","hidden":false}],"publishedAt":"2026-01-28T18:33:17.000Z","submittedOnDailyAt":"2026-01-29T00:42:23.006Z","title":"Linear representations in language models can change dramatically over a conversation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.","upvotes":8,"discussionId":"697ad00edf3e800774f13c54","ai_summary":"Linear representation directions in language models dynamically shift during conversations, affecting how factual information is encoded while preserving generic content, with implications for interpretability and context-adaptive model behavior.","ai_keywords":["linear directions","language model representations","conversation dynamics","factual representation","representational drift","context adaptation","model steering","interpretability"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-01-28T13:33:17.000Z","title":"Linear representations in language models can change dramatically over a conversation","summary":"Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20834.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":219,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20802","authors":[{"_id":"697b378a870173bd9177734d","user":{"_id":"6645dbab80ee4eb5ddc8aed8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6645dbab80ee4eb5ddc8aed8/-IckaooWMXeACKew9THv_.jpeg","isPro":false,"fullname":"Jonas Hübotter","user":"jonhue","type":"user"},"name":"Jonas Hübotter","status":"claimed_verified","statusLastChangedAt":"2026-01-29T13:56:34.106Z","hidden":false},{"_id":"697b378a870173bd9177734e","name":"Frederike Lübeck","hidden":false},{"_id":"697b378a870173bd9177734f","name":"Lejs Behric","hidden":false},{"_id":"697b378a870173bd91777350","name":"Anton Baumann","hidden":false},{"_id":"697b378a870173bd91777351","name":"Marco Bagatella","hidden":false},{"_id":"697b378a870173bd91777352","name":"Daniel Marta","hidden":false},{"_id":"697b378a870173bd91777353","name":"Ido Hakimi","hidden":false},{"_id":"697b378a870173bd91777354","name":"Idan Shenfeld","hidden":false},{"_id":"697b378a870173bd91777355","name":"Thomas Kleine Buening","hidden":false},{"_id":"697b378a870173bd91777356","name":"Carlos Guestrin","hidden":false},{"_id":"697b378a870173bd91777357","name":"Andreas Krause","hidden":false}],"publishedAt":"2026-01-28T17:45:12.000Z","submittedOnDailyAt":"2026-01-29T09:40:07.498Z","title":"Reinforcement Learning via Self-Distillation","submittedOnDailyBy":{"_id":"6645dbab80ee4eb5ddc8aed8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6645dbab80ee4eb5ddc8aed8/-IckaooWMXeACKew9THv_.jpeg","isPro":false,"fullname":"Jonas Hübotter","user":"jonhue","type":"user"},"summary":"Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.","upvotes":6,"discussionId":"697b378a870173bd91777358","projectPage":"https://self-distillation.github.io/SDPO","githubRepo":"https://github.com/lasgroup/SDPO","githubRepoAddedBy":"user","ai_summary":"Self-Distillation Policy Optimization (SDPO) enhances reinforcement learning with verifiable rewards by utilizing rich textual feedback to improve sample efficiency and accuracy in language model training.","ai_keywords":["reinforcement learning","verifiable rewards","rich feedback","Self-Distillation Policy Optimization","SDPO","tokenized feedback","dense learning signal","reward model","policy distillation","in-context learning","sample efficiency","accuracy"],"githubStars":1,"organization":{"_id":"68923f153ceba9f21ce33320","name":"lasgroup","fullname":"LAS @ ETH Zurich","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6645dbab80ee4eb5ddc8aed8/-wp0yB2cNzO2B0wte9nzF.png"}},"publishedAt":"2026-01-28T12:45:12.000Z","title":"Reinforcement Learning via Self-Distillation","summary":"Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20802.png","numComments":2,"submittedBy":{"_id":"6645dbab80ee4eb5ddc8aed8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6645dbab80ee4eb5ddc8aed8/-IckaooWMXeACKew9THv_.jpeg","fullname":"Jonas Hübotter","name":"jonhue","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"68923f153ceba9f21ce33320","name":"lasgroup","fullname":"LAS @ ETH Zurich","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6645dbab80ee4eb5ddc8aed8/-wp0yB2cNzO2B0wte9nzF.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.20055","authors":[{"_id":"697baf7ea67238fac88cbf0d","user":{"_id":"6702352c547acbd64cddf31e","avatarUrl":"/avatars/37fbacb299f217505b2a03549128f10d.svg","isPro":false,"fullname":"Vikash Singh","user":"optimusPrimeBee","type":"user"},"name":"Vikash Singh","status":"claimed_verified","statusLastChangedAt":"2026-01-29T21:15:21.775Z","hidden":false},{"_id":"697baf7ea67238fac88cbf0e","name":"Darion Cassel","hidden":false},{"_id":"697baf7ea67238fac88cbf0f","name":"Nathaniel Weir","hidden":false},{"_id":"697baf7ea67238fac88cbf10","name":"Nick Feng","hidden":false},{"_id":"697baf7ea67238fac88cbf11","name":"Sam Bayless","hidden":false}],"publishedAt":"2026-01-27T20:59:11.000Z","submittedOnDailyAt":"2026-01-29T16:37:00.504Z","title":"VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning","submittedOnDailyBy":{"_id":"6702352c547acbd64cddf31e","avatarUrl":"/avatars/37fbacb299f217505b2a03549128f10d.svg","isPro":false,"fullname":"Vikash Singh","user":"optimusPrimeBee","type":"user"},"summary":"Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.","upvotes":5,"discussionId":"697baf7ea67238fac88cbf12","ai_summary":"A neurosymbolic framework integrates large language models with SMT solvers to enhance logical correctness through verification-guided iterative refinement, utilizing formal semantic equivalence checking, semantic routing, and minimal correction subsets for precise error localization and consensus verification.","ai_keywords":["Large Language Models","SMT solvers","verification-guided answers","iterative refinement","atomic claims","first-order logic","automated theorem proving","formal semantic equivalence checking","semantic routing","symbolic solvers","LLM ensembles","commonsense reasoning","Minimal Correction Subsets","logical error localization","structured feedback","convergence","formal guarantees","consensus verification"],"organization":{"_id":"65424e525bb41e82fba69276","name":"amazonwebservices","fullname":"Amazon Web Services (AWS)","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65424dd6afa1020a9c31759e/wt7-MpCfO2O8pVG8mISJS.png"}},"publishedAt":"2026-01-27T15:59:11.000Z","title":"VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning","summary":"Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20055.png","numComments":1,"submittedBy":{"_id":"6702352c547acbd64cddf31e","avatarUrl":"/avatars/37fbacb299f217505b2a03549128f10d.svg","fullname":"Vikash Singh","name":"optimusPrimeBee","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"65424e525bb41e82fba69276","name":"amazonwebservices","fullname":"Amazon Web Services (AWS)","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65424dd6afa1020a9c31759e/wt7-MpCfO2O8pVG8mISJS.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.20789","authors":[{"_id":"697ad0a3df3e800774f13c56","name":"Ethan Shen","hidden":false},{"_id":"697ad0a3df3e800774f13c57","name":"Danny Tormoen","hidden":false},{"_id":"697ad0a3df3e800774f13c58","name":"Saurabh Shah","hidden":false},{"_id":"697ad0a3df3e800774f13c59","name":"Ali Farhadi","hidden":false},{"_id":"697ad0a3df3e800774f13c5a","name":"Tim Dettmers","hidden":false}],"publishedAt":"2026-01-28T17:27:08.000Z","submittedOnDailyAt":"2026-01-29T00:44:55.441Z","title":"SERA: Soft-Verified Efficient Repository Agents","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.","upvotes":4,"discussionId":"697ad0a4df3e800774f13c5b","githubRepo":"https://github.com/allenai/SERA","githubRepoAddedBy":"user","ai_summary":"Soft-Verified Efficient Repository Agents (SERA) enables cost-effective training of coding agents through supervised fine-tuning, achieving state-of-the-art performance while enabling specialization to private codebases at a fraction of the cost of previous methods.","ai_keywords":["supervised fine-tuning","coding agents","private codebases","Soft-Verified Generation","synthetic trajectories","scaling laws","ablations","reinforcement learning"],"githubStars":75,"organization":{"_id":"65e6310cc7738c6b88970c23","name":"ai21labs","fullname":"AI21","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67baf6e5489cb4dc98a4bff4/9Rkvk1VGhK1woxWvhqDyb.png"}},"publishedAt":"2026-01-28T12:27:08.000Z","title":"SERA: Soft-Verified Efficient Repository Agents","summary":"Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20789.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":219,"isUserFollowing":false},"organization":{"_id":"65e6310cc7738c6b88970c23","name":"ai21labs","fullname":"AI21","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67baf6e5489cb4dc98a4bff4/9Rkvk1VGhK1woxWvhqDyb.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20380","authors":[{"_id":"697acd54df3e800774f13c30","name":"Le Zhang","hidden":false},{"_id":"697acd54df3e800774f13c31","name":"Yixiong Xiao","hidden":false},{"_id":"697acd54df3e800774f13c32","name":"Xinjiang Lu","hidden":false},{"_id":"697acd54df3e800774f13c33","name":"Jingjia Cao","hidden":false},{"_id":"697acd54df3e800774f13c34","name":"Yusai Zhao","hidden":false},{"_id":"697acd54df3e800774f13c35","name":"Jingbo Zhou","hidden":false},{"_id":"697acd54df3e800774f13c36","name":"Lang An","hidden":false},{"_id":"697acd54df3e800774f13c37","name":"Zikan Feng","hidden":false},{"_id":"697acd54df3e800774f13c38","name":"Wanxiang Sha","hidden":false},{"_id":"697acd54df3e800774f13c39","name":"Yu Shi","hidden":false},{"_id":"697acd54df3e800774f13c3a","name":"Congxi Xiao","hidden":false},{"_id":"697acd54df3e800774f13c3b","name":"Jian Xiong","hidden":false},{"_id":"697acd54df3e800774f13c3c","name":"Yankai Zhang","hidden":false},{"_id":"697acd54df3e800774f13c3d","name":"Hua Wu","hidden":false},{"_id":"697acd54df3e800774f13c3e","name":"Haifeng Wang","hidden":false}],"publishedAt":"2026-01-28T08:45:17.000Z","submittedOnDailyAt":"2026-01-29T00:30:40.013Z","title":"OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.","upvotes":4,"discussionId":"697acd55df3e800774f13c3f","ai_summary":"OmegaUse is a general-purpose GUI agent model that achieves state-of-the-art performance on mobile and desktop platforms through a combination of high-quality data construction, decoupled training methods, and a Mixture-of-Experts architecture.","ai_keywords":["GUI agents","foundation models","autonomous task execution","data-construction pipeline","decoupled training paradigm","automated synthesis framework","bottom-up autonomous exploration","top-down taxonomy-guided generation","supervised fine-tuning","group relative policy optimization","mixture-of-experts","screenspot-v2","androidcontrol","os-nav","chim-nav","ubu-nav"]},"publishedAt":"2026-01-28T03:45:17.000Z","title":"OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution","summary":"Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20380.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":219,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.19949","authors":[{"_id":"697ac3e5df3e800774f13bbe","user":{"_id":"67d43aeb6ffb8add49ea6712","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg","isPro":false,"fullname":"Mandip Goswami","user":"mandipgoswami","type":"user"},"name":"Mandip Goswami","status":"claimed_verified","statusLastChangedAt":"2026-01-29T09:16:31.429Z","hidden":false}],"publishedAt":"2026-01-25T22:17:18.000Z","submittedOnDailyAt":"2026-01-29T00:17:20.409Z","title":"RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation","submittedOnDailyBy":{"_id":"67d43aeb6ffb8add49ea6712","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg","isPro":false,"fullname":"Mandip Goswami","user":"mandipgoswami","type":"user"},"summary":"Despite decades of research on reverberant speech, comparing methods remains difficult because most corpora lack per-file acoustic annotations or provide limited documentation for reproduction. We present RIR-Mega-Speech, a corpus of approximately 117.5 hours created by convolving LibriSpeech utterances with roughly 5,000 simulated room impulse responses from the RIR-Mega collection. Every file includes RT60, direct-to-reverberant ratio (DRR), and clarity index (C_{50}) computed from the source RIR using clearly defined, reproducible procedures. We also provide scripts to rebuild the dataset and reproduce all evaluation results.\n  Using Whisper small on 1,500 paired utterances, we measure 5.20% WER (95% CI: 4.69--5.78) on clean speech and 7.70% (7.04--8.35) on reverberant versions, corresponding to a paired increase of 2.50 percentage points (2.06--2.98). This represents a 48% relative degradation. WER increases monotonically with RT60 and decreases with DRR, consistent with prior perceptual studies. While the core finding that reverberation harms recognition is well established, we aim to provide the community with a standardized resource where acoustic conditions are transparent and results can be verified independently. The repository includes one-command rebuild instructions for both Windows and Linux environments.","upvotes":3,"discussionId":"697ac3e5df3e800774f13bbf","projectPage":"https://huggingface.co/datasets/mandipgoswami/rir-mega-speech","githubRepo":"https://github.com/mandip42/rir-mega-speech","githubRepoAddedBy":"user","ai_summary":"A large-scale reverberant speech corpus with detailed acoustic annotations is introduced to facilitate standardized comparison and reproduction of speech processing research.","ai_keywords":["RIR-Mega-Speech","LibriSpeech","room impulse responses","RT60","direct-to-reverberant ratio","clarity index","C50","Whisper","WER"],"githubStars":1},"publishedAt":"2026-01-25T17:17:18.000Z","title":"RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation","summary":"Despite decades of research on reverberant speech, comparing methods remains difficult because most corpora lack per-file acoustic annotations or provide limited documentation for reproduction. We present RIR-Mega-Speech, a corpus of approximately 117.5 hours created by convolving LibriSpeech utterances with roughly 5,000 simulated room impulse responses from the RIR-Mega collection. Every file includes RT60, direct-to-reverberant ratio (DRR), and clarity index (C_{50}) computed from the source RIR using clearly defined, reproducible procedures. We also provide scripts to rebuild the dataset and reproduce all evaluation results.\n  Using Whisper small on 1,500 paired utterances, we measure 5.20% WER (95% CI: 4.69--5.78) on clean speech and 7.70% (7.04--8.35) on reverberant versions, corresponding to a paired increase of 2.50 percentage points (2.06--2.98). This represents a 48% relative degradation. WER increases monotonically with RT60 and decreases with DRR, consistent with prior perceptual studies. While the core finding that reverberation harms recognition is well established, we aim to provide the community with a standardized resource where acoustic conditions are transparent and results can be verified independently. The repository includes one-command rebuild instructions for both Windows and Linux environments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19949.png","numComments":1,"submittedBy":{"_id":"67d43aeb6ffb8add49ea6712","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg","fullname":"Mandip Goswami","name":"mandipgoswami","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.17950","authors":[{"_id":"697822a5026bdf0473116d94","name":"Matthew Walmer","hidden":false},{"_id":"697822a5026bdf0473116d95","name":"Saksham Suri","hidden":false},{"_id":"697822a5026bdf0473116d96","name":"Anirud Aggarwal","hidden":false},{"_id":"697822a5026bdf0473116d97","name":"Abhinav Shrivastava","hidden":false}],"publishedAt":"2026-01-25T18:59:45.000Z","submittedOnDailyAt":"2026-01-29T06:56:30.253Z","title":"UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders","submittedOnDailyBy":{"_id":"5f1158120c833276f61f1a84","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg","isPro":false,"fullname":"Niels Rogge","user":"nielsr","type":"user"},"summary":"The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.","upvotes":3,"discussionId":"697822a5026bdf0473116d98","projectPage":"https://www.cs.umd.edu/~mwalmer/uplift/","githubRepo":"https://github.com/mwalmer-umd/UPLiFT/","githubRepoAddedBy":"user","ai_summary":"Iterative upsampling methods can match cross-attention-based approaches while achieving lower inference costs through the proposed UPLiFT architecture with Local Attender operator for dense feature generation.","ai_keywords":["feature upsampling","visual backbones","cross-attention","iterative upsampling","pixel-dense feature upsamplers","UPLiFT","Local Attender","Coupled Flow Matching","VAE feature upsampling"],"githubStars":20,"organization":{"_id":"69717e671a15e8e2ea2e7dd4","name":"UPLiFT-upsampler","fullname":"UPLiFT - University of Maryland, College Park","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62aa3f2e0efa8121f4562a26/E6IDSX8c8OnXIcfkzLr91.png"}},"publishedAt":"2026-01-25T13:59:45.000Z","title":"UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders","summary":"The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17950.png","numComments":1,"submittedBy":{"_id":"5f1158120c833276f61f1a84","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg","fullname":"Niels Rogge","name":"nielsr","type":"user","isPro":false,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":1084,"isUserFollowing":false},"organization":{"_id":"69717e671a15e8e2ea2e7dd4","name":"UPLiFT-upsampler","fullname":"UPLiFT - University of Maryland, College Park","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62aa3f2e0efa8121f4562a26/E6IDSX8c8OnXIcfkzLr91.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20829","authors":[{"_id":"697b7b4601afa04094ca0864","name":"Minwu Kim","hidden":false},{"_id":"697b7b4601afa04094ca0865","name":"Safal Shrestha","hidden":false},{"_id":"697b7b4601afa04094ca0866","name":"Keith Ross","hidden":false}],"publishedAt":"2026-01-28T18:29:21.000Z","submittedOnDailyAt":"2026-01-29T12:54:19.365Z","title":"Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning","submittedOnDailyBy":{"_id":"64e77b47d96966317b45eeb3","avatarUrl":"/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg","isPro":false,"fullname":"Minwu Kim","user":"guactastesgood","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.","upvotes":2,"discussionId":"697b7b4601afa04094ca0867","githubRepo":"https://github.com/minwukim/training-on-saturated-problems","githubRepoAddedBy":"user","ai_summary":"Failure-prefix conditioning enables more effective reinforcement learning from saturated problems by focusing exploration on informative failure trajectories, maintaining token efficiency while improving robustness.","ai_keywords":["reinforcement learning","large language models","reasoning abilities","training stagnation","informative failures","failure-prefix conditioning","exploration","token efficiency","robustness","iterative approach","saturated problems"],"githubStars":0,"organization":{"_id":"691d8e884bbe8df0d99462e2","name":"newyorkuniversity","fullname":"New York University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/orNHmPzOQf2_F5UgXPsu5.png"}},"publishedAt":"2026-01-28T13:29:21.000Z","title":"Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20829.png","numComments":1,"submittedBy":{"_id":"64e77b47d96966317b45eeb3","avatarUrl":"/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg","fullname":"Minwu Kim","name":"guactastesgood","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"691d8e884bbe8df0d99462e2","name":"newyorkuniversity","fullname":"New York University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/orNHmPzOQf2_F5UgXPsu5.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20618","authors":[{"_id":"697ac77edf3e800774f13bfa","name":"Shuguang Zhang","hidden":false},{"_id":"697ac77edf3e800774f13bfb","name":"Junhong Lian","hidden":false},{"_id":"697ac77edf3e800774f13bfc","name":"Guoxin Yu","hidden":false},{"_id":"697ac77edf3e800774f13bfd","name":"Baoxun Xu","hidden":false},{"_id":"697ac77edf3e800774f13bfe","name":"Xiang Ao","hidden":false}],"publishedAt":"2026-01-28T13:51:34.000Z","submittedOnDailyAt":"2026-01-29T00:08:16.102Z","title":"GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection","submittedOnDailyBy":{"_id":"63a15c583c8841cfe2dbc5c6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1671519286473-noauth.jpeg","isPro":false,"fullname":"Lian Junhong","user":"THEATLAS","type":"user"},"summary":"Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.","upvotes":2,"discussionId":"697ac77fdf3e800774f13bff","ai_summary":"A multimodal sarcasm detection approach uses generative models to create stable semantic anchors and measures cross-modal discrepancies for improved accuracy and robustness.","ai_keywords":["Multimodal sarcasm detection","cross-modal embedding misalignment","large language models","Multimodal LLMs","Generative Discrepancy Comparison Network","semantic incongruities","cross-modal conflicts","sentiment discrepancies","visual-textual fidelity","gated module"],"organization":{"_id":"68ef0ab704f0f03d81964936","name":"ict-cas","fullname":"Institute of Computing Technology, Chinese Academy of Sciences","avatar":"https://cdn-uploads.huggingface.co/production/uploads/656ad93853703dd78f3de7b8/MR5EsF33Ev3IdnMOHv1be.jpeg"}},"publishedAt":"2026-01-28T08:51:34.000Z","title":"GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection","summary":"Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20618.png","numComments":1,"submittedBy":{"_id":"63a15c583c8841cfe2dbc5c6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1671519286473-noauth.jpeg","fullname":"Lian Junhong","name":"THEATLAS","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"68ef0ab704f0f03d81964936","name":"ict-cas","fullname":"Institute of Computing Technology, Chinese Academy of Sciences","avatar":"https://cdn-uploads.huggingface.co/production/uploads/656ad93853703dd78f3de7b8/MR5EsF33Ev3IdnMOHv1be.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.19280","authors":[{"_id":"697bbd09a67238fac88cbfd7","user":{"_id":"68266b5261ed4d89177c3612","avatarUrl":"/avatars/e41ef277047334174eca408aba2a63db.svg","isPro":false,"fullname":"Kishan Panaganti","user":"kishanpb","type":"user"},"name":"Kishan Panaganti","status":"claimed_verified","statusLastChangedAt":"2026-01-29T21:15:19.705Z","hidden":false},{"_id":"697bbd09a67238fac88cbfd8","name":"Zhenwen Liang","hidden":false},{"_id":"697bbd09a67238fac88cbfd9","name":"Wenhao Yu","hidden":false},{"_id":"697bbd09a67238fac88cbfda","name":"Haitao Mi","hidden":false},{"_id":"697bbd09a67238fac88cbfdb","name":"Dong Yu","hidden":false}],"publishedAt":"2026-01-27T07:10:41.000Z","submittedOnDailyAt":"2026-01-29T17:39:08.045Z","title":"Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning","submittedOnDailyBy":{"_id":"68266b5261ed4d89177c3612","avatarUrl":"/avatars/e41ef277047334174eca408aba2a63db.svg","isPro":false,"fullname":"Kishan Panaganti","user":"kishanpb","type":"user"},"summary":"Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.\n  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model's performance.","upvotes":2,"discussionId":"697bbd0aa67238fac88cbfdc","ai_summary":"A novel optimization framework dynamically adapts training distributions for large language models by classifying prompt difficulty and reallocating computational resources to improve reasoning performance.","ai_keywords":["Large Language Model","post-training","Reinforcement Learning","GRPO","Group Distributionally Robust Optimization","Online Difficulty Classifier","EMA-debiased multiplicative-weights bandit sampler","shadow-price controller","pass@k","variance reduction","no-regret guarantees","emergent curriculum"],"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-01-27T02:10:41.000Z","title":"Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning","summary":"Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.\n  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model's performance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19280.png","numComments":1,"submittedBy":{"_id":"68266b5261ed4d89177c3612","avatarUrl":"/avatars/e41ef277047334174eca408aba2a63db.svg","fullname":"Kishan Panaganti","name":"kishanpb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.19194","authors":[{"_id":"697a36e7df3e800774f13a2b","name":"Alexander Polok","hidden":false},{"_id":"697a36e7df3e800774f13a2c","name":"Dominik Klement","hidden":false},{"_id":"697a36e7df3e800774f13a2d","name":"Samuele Cornell","hidden":false},{"_id":"697a36e7df3e800774f13a2e","name":"Matthew Wiesner","hidden":false},{"_id":"697a36e7df3e800774f13a2f","name":"Jan Černocký","hidden":false},{"_id":"697a36e7df3e800774f13a30","name":"Sanjeev Khudanpur","hidden":false},{"_id":"697a36e7df3e800774f13a31","name":"Lukáš Burget","hidden":false}],"publishedAt":"2026-01-27T04:51:07.000Z","submittedOnDailyAt":"2026-01-29T01:49:03.042Z","title":"SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper","submittedOnDailyBy":{"_id":"638d2b765e14c2f38677987b","avatarUrl":"/avatars/622c183e897a99e33717f4a92305fbd3.svg","isPro":false,"fullname":"Alexander Polok","user":"Lakoc","type":"user"},"summary":"Speaker-attributed automatic speech recognition (ASR) in multi-speaker environments remains a major challenge. While some approaches achieve strong performance when fine-tuned on specific domains, few systems generalize well across out-of-domain datasets. Our prior work, Diarization-Conditioned Whisper (DiCoW), leverages speaker diarization outputs as conditioning information and, with minimal fine-tuning, demonstrated strong multilingual and multi-domain performance. In this paper, we address a key limitation of DiCoW: ambiguity in Silence-Target-Non-target-Overlap (STNO) masks, where two or more fully overlapping speakers may have nearly identical conditioning despite differing transcriptions. We introduce SE-DiCoW (Self-Enrolled Diarization-Conditioned Whisper), which uses diarization output to locate an enrollment segment anywhere in the conversation where the target speaker is most active. This enrollment segment is used as fixed conditioning via cross-attention at each encoder layer. We further refine DiCoW with improved data segmentation, model initialization, and augmentation. Together, these advances yield substantial gains: SE-DiCoW reduces macro-averaged tcpWER by 52.4% relative to the original DiCoW on the EMMA MT-ASR benchmark.","upvotes":2,"discussionId":"697a36e7df3e800774f13a32","ai_summary":"SE-DiCoW improves speaker-attributed ASR performance by using diarization output to identify enrollment segments for fixed conditioning in cross-attention layers, achieving significant reductions in transcription error rates.","ai_keywords":["speaker diarization","cross-attention","encoder layers","tcpWER","EMMA MT-ASR benchmark","STNO masks","automatic speech recognition"],"organization":{"_id":"6536246dbadc497807a8b54a","name":"BUT-FIT","fullname":"Brno University of Technology, Faculty of Information Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60dd973c5cf30e35111dbb02/ddxou0Vv6AT3lyPKgCnNH.png"}},"publishedAt":"2026-01-26T23:51:07.000Z","title":"SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper","summary":"Speaker-attributed automatic speech recognition (ASR) in multi-speaker environments remains a major challenge. While some approaches achieve strong performance when fine-tuned on specific domains, few systems generalize well across out-of-domain datasets. Our prior work, Diarization-Conditioned Whisper (DiCoW), leverages speaker diarization outputs as conditioning information and, with minimal fine-tuning, demonstrated strong multilingual and multi-domain performance. In this paper, we address a key limitation of DiCoW: ambiguity in Silence-Target-Non-target-Overlap (STNO) masks, where two or more fully overlapping speakers may have nearly identical conditioning despite differing transcriptions. We introduce SE-DiCoW (Self-Enrolled Diarization-Conditioned Whisper), which uses diarization output to locate an enrollment segment anywhere in the conversation where the target speaker is most active. This enrollment segment is used as fixed conditioning via cross-attention at each encoder layer. We further refine DiCoW with improved data segmentation, model initialization, and augmentation. Together, these advances yield substantial gains: SE-DiCoW reduces macro-averaged tcpWER by 52.4% relative to the original DiCoW on the EMMA MT-ASR benchmark.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19194.png","numComments":1,"submittedBy":{"_id":"638d2b765e14c2f38677987b","avatarUrl":"/avatars/622c183e897a99e33717f4a92305fbd3.svg","fullname":"Alexander Polok","name":"Lakoc","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":12,"isUserFollowing":false},"organization":{"_id":"6536246dbadc497807a8b54a","name":"BUT-FIT","fullname":"Brno University of Technology, Faculty of Information Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60dd973c5cf30e35111dbb02/ddxou0Vv6AT3lyPKgCnNH.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20757","authors":[{"_id":"697bee82a67238fac88cc00a","name":"Jing Yang","hidden":false},{"_id":"697bee82a67238fac88cc00b","name":"Moritz Hechtbauer","hidden":false},{"_id":"697bee82a67238fac88cc00c","name":"Elisabeth Khalilov","hidden":false},{"_id":"697bee82a67238fac88cc00d","name":"Evelyn Luise Brinkmann","hidden":false},{"_id":"697bee82a67238fac88cc00e","name":"Vera Schmitt","hidden":false},{"_id":"697bee82a67238fac88cc00f","name":"Nils Feldhus","hidden":false}],"publishedAt":"2026-01-28T16:41:17.000Z","submittedOnDailyAt":"2026-01-29T21:06:13.597Z","title":"Persona Prompting as a Lens on LLM Social Reasoning","submittedOnDailyBy":{"_id":"60914d0799d335a567823838","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1620135161366-noauth.jpeg","isPro":false,"fullname":"Nils Feldhus","user":"nfel","type":"user"},"summary":"For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.","upvotes":1,"discussionId":"697bee82a67238fac88cc010","githubRepo":"https://github.com/jingyng/PP-social-reasoning","githubRepoAddedBy":"user","ai_summary":"Persona prompting affects large language model rationales in socially sensitive tasks by improving classification accuracy but reducing explanation quality while failing to eliminate demographic biases.","ai_keywords":["Large Language Models","Persona prompting","hate speech detection","word-level rationales","demographic personas","model bias","human alignment","classification accuracy","rationale quality"],"githubStars":1,"organization":{"_id":"67dbeb621cbd4c7c0fdc3200","name":"xplainlp","fullname":"XplaiNLP Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/636529118b6bd0d9f658dc23/6-SS8CzilC3_MKLYUzKu4.png"}},"publishedAt":"2026-01-28T11:41:17.000Z","title":"Persona Prompting as a Lens on LLM Social Reasoning","summary":"For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20757.png","numComments":1,"submittedBy":{"_id":"60914d0799d335a567823838","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1620135161366-noauth.jpeg","fullname":"Nils Feldhus","name":"nfel","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":10,"isUserFollowing":false},"organization":{"_id":"67dbeb621cbd4c7c0fdc3200","name":"xplainlp","fullname":"XplaiNLP Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/636529118b6bd0d9f658dc23/6-SS8CzilC3_MKLYUzKu4.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20622","authors":[{"_id":"697accc0df3e800774f13c2a","name":"Boyu Li","hidden":false},{"_id":"697accc0df3e800774f13c2b","name":"Lin-Ping Yuan","hidden":false},{"_id":"697accc0df3e800774f13c2c","name":"Zeyu Wang","hidden":false},{"_id":"697accc0df3e800774f13c2d","name":"Hongbo Fu","hidden":false}],"publishedAt":"2026-01-28T13:55:36.000Z","submittedOnDailyAt":"2026-01-29T00:28:12.151Z","title":"SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Sketching provides an intuitive way to convey dynamic intent in animation authoring (i.e., how elements change over time and space), making it a natural medium for automatic content creation. Yet existing approaches often constrain sketches to fixed command tokens or predefined visual forms, overlooking their freeform nature and the central role of humans in shaping intention. To address this, we introduce an interaction paradigm where users convey dynamic intent to a vision-language model via free-form sketching, instantiated here in a sketch storyboard to motion graphics workflow. We implement an interface and improve it through a three-stage study with 24 participants. The study shows how sketches convey motion with minimal input, how their inherent ambiguity requires users to be involved for clarification, and how sketches can visually guide video refinement. Our findings reveal the potential of sketch and AI interaction to bridge the gap between intention and outcome, and demonstrate its applicability to 3D animation and video generation.","upvotes":1,"discussionId":"697accc0df3e800774f13c2e","ai_summary":"Free-form sketching enables intuitive dynamic intent communication for automated content creation, bridging human intention and digital output in animation workflows.","ai_keywords":["vision-language model","sketch storyboard","motion graphics","3D animation","video generation"]},"publishedAt":"2026-01-28T08:55:36.000Z","title":"SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation","summary":"Sketching provides an intuitive way to convey dynamic intent in animation authoring (i.e., how elements change over time and space), making it a natural medium for automatic content creation. Yet existing approaches often constrain sketches to fixed command tokens or predefined visual forms, overlooking their freeform nature and the central role of humans in shaping intention. To address this, we introduce an interaction paradigm where users convey dynamic intent to a vision-language model via free-form sketching, instantiated here in a sketch storyboard to motion graphics workflow. We implement an interface and improve it through a three-stage study with 24 participants. The study shows how sketches convey motion with minimal input, how their inherent ambiguity requires users to be involved for clarification, and how sketches can visually guide video refinement. Our findings reveal the potential of sketch and AI interaction to bridge the gap between intention and outcome, and demonstrate its applicability to 3D animation and video generation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20622.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":219,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.20262","authors":[{"_id":"697ac611df3e800774f13bdb","user":{"_id":"653b9b677c2fb22dda9020da","avatarUrl":"/avatars/3cfe55566205f66ec1919479f60f3017.svg","isPro":false,"fullname":"Boseong Jeon","user":"bf-jeon","type":"user"},"name":"Boseong Jeon","status":"claimed_verified","statusLastChangedAt":"2026-01-29T09:16:20.722Z","hidden":false},{"_id":"697ac611df3e800774f13bdc","name":"Yunho Choi","hidden":false},{"_id":"697ac611df3e800774f13bdd","name":"Taehan Kim","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/653b9b677c2fb22dda9020da/T0LnMfaultPAr-0igGlDl.mp4"],"publishedAt":"2026-01-28T05:16:26.000Z","submittedOnDailyAt":"2026-01-29T09:23:57.010Z","title":"Shallow-π: Knowledge Distillation for Flow-based VLAs","submittedOnDailyBy":{"_id":"653b9b677c2fb22dda9020da","avatarUrl":"/avatars/3cfe55566205f66ec1919479f60f3017.svg","isPro":false,"fullname":"Boseong Jeon","user":"bf-jeon","type":"user"},"summary":"The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.","upvotes":1,"discussionId":"697ac611df3e800774f13bde","projectPage":"https://icsl-jeon.github.io/shallow-pi/","githubRepo":"https://github.com/icsl-Jeon/openpi","githubRepoAddedBy":"user","ai_summary":"A knowledge distillation framework called Shallow-pi is presented that reduces transformer depth in vision-language-action models, achieving faster inference with minimal performance loss in real-world robotic applications.","ai_keywords":["knowledge distillation","transformer layer reduction","vision-language-action models","VLA models","token level","visual token pruning","flow-based VLA models","model compression","real-time robotic deployment","inference speed","success rate","manipulation benchmarks","Jetson Orin","Jetson Thor","robot platforms","humanoid systems","dynamic manipulation scenarios"],"githubStars":1,"organization":{"_id":"686df54910a52f2c2cf03c06","name":"SamsungResearch","fullname":"Samsung Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"}},"publishedAt":"2026-01-28T00:16:26.000Z","title":"Shallow-π: Knowledge Distillation for Flow-based VLAs","summary":"The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/653b9b677c2fb22dda9020da/T0LnMfaultPAr-0igGlDl.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20262.png","numComments":1,"submittedBy":{"_id":"653b9b677c2fb22dda9020da","avatarUrl":"/avatars/3cfe55566205f66ec1919479f60f3017.svg","fullname":"Boseong Jeon","name":"bf-jeon","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"686df54910a52f2c2cf03c06","name":"SamsungResearch","fullname":"Samsung Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"},"isAuthorParticipating":true}]
[{"paper":{"id":"2601.20833","authors":[{"_id":"697b9192a67238fac88cbee8","name":"Tengyue Xu","hidden":false},{"_id":"697b9192a67238fac88cbee9","name":"Zhuoyang Qian","hidden":false},{"_id":"697b9192a67238fac88cbeea","name":"Gaoge Liu","hidden":false},{"_id":"697b9192a67238fac88cbeeb","name":"Li Ling","hidden":false},{"_id":"697b9192a67238fac88cbeec","name":"Zhentao Zhang","hidden":false},{"_id":"697b9192a67238fac88cbeed","name":"Biao Wu","hidden":false},{"_id":"697b9192a67238fac88cbeee","name":"Shuo Zhang","hidden":false},{"_id":"697b9192a67238fac88cbeef","name":"Ke Lu","hidden":false},{"_id":"697b9192a67238fac88cbef0","name":"Wei Shi","hidden":false},{"_id":"697b9192a67238fac88cbef1","name":"Ziqi Wang","hidden":false},{"_id":"697b9192a67238fac88cbef2","name":"Zheng Feng","hidden":false},{"_id":"697b9192a67238fac88cbef3","name":"Yan Luo","hidden":false},{"_id":"697b9192a67238fac88cbef4","name":"Shu Xu","hidden":false},{"_id":"697b9192a67238fac88cbef5","name":"Yongjin Chen","hidden":false},{"_id":"697b9192a67238fac88cbef6","name":"Zhibo Feng","hidden":false},{"_id":"697b9192a67238fac88cbef7","name":"Zhuo Chen","hidden":false},{"_id":"697b9192a67238fac88cbef8","name":"Bruce Yuan","hidden":false},{"_id":"697b9192a67238fac88cbef9","name":"Harry Wang","hidden":false},{"_id":"697b9192a67238fac88cbefa","name":"Kris Chen","hidden":false}],"publishedAt":"2026-01-28T18:31:54.000Z","submittedOnDailyAt":"2026-01-30T03:32:00.106Z","title":"Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives","submittedOnDailyBy":{"_id":"62baa0d6dd02fbf607ce97be","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg","isPro":false,"fullname":"Wendy","user":"Wendy-Fly","type":"user"},"summary":"Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.","upvotes":113,"discussionId":"697b9192a67238fac88cbefb","githubRepo":"https://github.com/AgentAlphaAGI/Idea2Paper","githubRepoAddedBy":"user","ai_summary":"Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.","ai_keywords":["large language model","autonomous scientific discovery","runtime-centric execution","context window limitations","hallucination","pre-computation-driven framework","peer-reviewed papers","research patterns","methodological knowledge graph","end-to-end research workflows"],"githubStars":54,"organization":{"_id":"69542731e1200d74c1c053d1","name":"AgentAlphaAGI","fullname":"AgentAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}},"publishedAt":"2026-01-28T13:31:54.000Z","title":"Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives","summary":"Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20833.png","numComments":1,"submittedBy":{"_id":"62baa0d6dd02fbf607ce97be","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg","fullname":"Wendy","name":"Wendy-Fly","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"69542731e1200d74c1c053d1","name":"AgentAlphaAGI","fullname":"AgentAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.20354","authors":[{"_id":"697c1857a67238fac88cc06e","user":{"_id":"656c9cfef7be0986b49934ea","avatarUrl":"/avatars/2030e77c28fb4c518b692cd9a20de665.svg","isPro":false,"fullname":"MuMing","user":"ZengbinWang","type":"user"},"name":"Zengbin Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:36:35.067Z","hidden":false},{"_id":"697c1857a67238fac88cc06f","name":"Xuecai Hu","hidden":false},{"_id":"697c1857a67238fac88cc070","name":"Yong Wang","hidden":false},{"_id":"697c1857a67238fac88cc071","name":"Feng Xiong","hidden":false},{"_id":"697c1857a67238fac88cc072","name":"Man Zhang","hidden":false},{"_id":"697c1857a67238fac88cc073","name":"Xiangxiang Chu","hidden":false}],"publishedAt":"2026-01-28T08:15:00.000Z","submittedOnDailyAt":"2026-01-30T05:01:51.614Z","title":"Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models","submittedOnDailyBy":{"_id":"66d255e3947594430c723ff6","avatarUrl":"/avatars/c56e4792332a01bf34085a75ee64916e.svg","isPro":false,"fullname":"xiaochonglinghu","user":"xiaochonglinghu","type":"user"},"summary":"Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.","upvotes":96,"discussionId":"697c1857a67238fac88cc074","githubRepo":"https://github.com/AMAP-ML/SpatialGenEval","githubRepoAddedBy":"user","ai_summary":"A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.","ai_keywords":["text-to-image models","spatial intelligence","benchmark","long prompts","information-dense prompts","spatial reasoning","Stable Diffusion-XL","Uniworld-V1","OmniGen2","fine-tuning"],"githubStars":93,"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}},"publishedAt":"2026-01-28T03:15:00.000Z","title":"Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models","summary":"Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20354.png","numComments":2,"submittedBy":{"_id":"66d255e3947594430c723ff6","avatarUrl":"/avatars/c56e4792332a01bf34085a75ee64916e.svg","fullname":"xiaochonglinghu","name":"xiaochonglinghu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.21204","authors":[{"_id":"697c3801a67238fac88cc1b1","name":"Hong Liu","hidden":false},{"_id":"697c3801a67238fac88cc1b2","name":"Jiaqi Zhang","hidden":false},{"_id":"697c3801a67238fac88cc1b3","name":"Chao Wang","hidden":false},{"_id":"697c3801a67238fac88cc1b4","name":"Xing Hu","hidden":false},{"_id":"697c3801a67238fac88cc1b5","name":"Linkun Lyu","hidden":false},{"_id":"697c3801a67238fac88cc1b6","name":"Jiaqi Sun","hidden":false},{"_id":"697c3801a67238fac88cc1b7","name":"Xurui Yang","hidden":false},{"_id":"697c3801a67238fac88cc1b8","name":"Bo Wang","hidden":false},{"_id":"697c3801a67238fac88cc1b9","name":"Fengcun Li","hidden":false},{"_id":"697c3801a67238fac88cc1ba","name":"Yulei Qian","hidden":false},{"_id":"697c3801a67238fac88cc1bb","name":"Lingtong Si","hidden":false},{"_id":"697c3801a67238fac88cc1bc","name":"Yerui Sun","hidden":false},{"_id":"697c3801a67238fac88cc1bd","name":"Rumei Li","hidden":false},{"_id":"697c3801a67238fac88cc1be","name":"Peng Pei","hidden":false},{"_id":"697c3801a67238fac88cc1bf","name":"Yuchen Xie","hidden":false},{"_id":"697c3801a67238fac88cc1c0","name":"Xunliang Cai","hidden":false}],"publishedAt":"2026-01-29T03:11:19.000Z","submittedOnDailyAt":"2026-01-30T02:18:11.112Z","title":"Scaling Embeddings Outperforms Scaling Experts in Language Models","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.","upvotes":76,"discussionId":"697c3801a67238fac88cc1c1","ai_summary":"Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.","ai_keywords":["Mixture-of-Experts","sparsity scaling","embedding scaling","Pareto frontier","parameter budgeting","model width","model depth","system optimizations","speculative decoding","LongCat-Flash-Lite"],"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}},"publishedAt":"2026-01-28T22:11:19.000Z","title":"Scaling Embeddings Outperforms Scaling Experts in Language Models","summary":"While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21204.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":221,"isUserFollowing":false},"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.22153","authors":[{"_id":"697c2899a67238fac88cc115","user":{"_id":"63f47b5321eb234ab739e91a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg","isPro":false,"fullname":"Haozhe Xie","user":"hzxie","type":"user"},"name":"Haozhe Xie","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:48.996Z","hidden":false},{"_id":"697c2899a67238fac88cc116","user":{"_id":"672392c4a4c4381cefc06416","avatarUrl":"/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg","isPro":false,"fullname":"Wen Beichen","user":"wenbc21","type":"user"},"name":"Beichen Wen","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:52.487Z","hidden":false},{"_id":"697c2899a67238fac88cc117","user":{"_id":"6899ff3f4c5ca50a326bb456","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Nuqof2ofdaQUD5b07cDnG.png","isPro":false,"fullname":"Zheng Jiarui","user":"zghtyarecrenj","type":"user"},"name":"Jiarui Zheng","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:46.030Z","hidden":false},{"_id":"697c2899a67238fac88cc118","name":"Zhaoxi Chen","hidden":false},{"_id":"697c2899a67238fac88cc119","name":"Fangzhou Hong","hidden":false},{"_id":"697c2899a67238fac88cc11a","name":"Haiwen Diao","hidden":false},{"_id":"697c2899a67238fac88cc11b","name":"Ziwei Liu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"],"publishedAt":"2026-01-29T18:59:51.000Z","submittedOnDailyAt":"2026-01-30T01:46:39.673Z","title":"DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation","submittedOnDailyBy":{"_id":"63f47b5321eb234ab739e91a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg","isPro":false,"fullname":"Haozhe Xie","user":"hzxie","type":"user"},"summary":"Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.","upvotes":45,"discussionId":"697c2899a67238fac88cc11c","projectPage":"https://haozhexie.com/project/dynamic-vla","githubRepo":"https://github.com/hzxie/DynamicVLA","githubRepoAddedBy":"user","ai_summary":"DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.","ai_keywords":["Vision-Language-Action models","temporal reasoning","closed-loop adaptation","convolutional vision encoder","multimodal inference","Continuous Inference","Latent-aware Action Streaming","Dynamic Object Manipulation benchmark","synthetic episodes","real-world episodes"],"githubStars":48,"organization":{"_id":"62d55f243bf5e059f7ca25ba","name":"mmlab-ntu","fullname":"MMLab@NTU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}},"publishedAt":"2026-01-29T13:59:51.000Z","title":"DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation","summary":"Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22153.png","numComments":2,"submittedBy":{"_id":"63f47b5321eb234ab739e91a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg","fullname":"Haozhe Xie","name":"hzxie","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":19,"isUserFollowing":false},"organization":{"_id":"62d55f243bf5e059f7ca25ba","name":"mmlab-ntu","fullname":"MMLab@NTU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.21639","authors":[{"_id":"697c5270a67238fac88cc226","user":{"_id":"693f91d7ed7d40c019934508","avatarUrl":"/avatars/0d73f098627c9ebd2ae7d90e693a34f6.svg","isPro":false,"fullname":"Yufeng Zhong","user":"Albert-Zhong","type":"user"},"name":"Yufeng Zhong","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:35:23.719Z","hidden":false},{"_id":"697c5270a67238fac88cc227","name":"Lei Chen","hidden":false},{"_id":"697c5270a67238fac88cc228","name":"Xuanle Zhao","hidden":false},{"_id":"697c5270a67238fac88cc229","name":"Wenkang Han","hidden":false},{"_id":"697c5270a67238fac88cc22a","name":"Liming Zheng","hidden":false},{"_id":"697c5270a67238fac88cc22b","name":"Jing Huang","hidden":false},{"_id":"697c5270a67238fac88cc22c","name":"Deyang Jiang","hidden":false},{"_id":"697c5270a67238fac88cc22d","name":"Yilin Cao","hidden":false},{"_id":"697c5270a67238fac88cc22e","name":"Lin Ma","hidden":false},{"_id":"697c5270a67238fac88cc22f","name":"Zhixiong Zeng","hidden":false}],"publishedAt":"2026-01-29T12:43:02.000Z","submittedOnDailyAt":"2026-01-30T04:33:06.261Z","title":"OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models","submittedOnDailyBy":{"_id":"6572cbc42bb242937c0a1101","avatarUrl":"/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg","isPro":false,"fullname":"Xuanle Zhao","user":"xxxllz","type":"user"},"summary":"The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.","upvotes":41,"discussionId":"697c5270a67238fac88cc230","githubRepo":"https://github.com/DocTron-hub/OCRVerse","githubRepoAddedBy":"user","ai_summary":"OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.","ai_keywords":["OCR","vision-centric OCR","text-centric OCR","end-to-end OCR","data engineering","SFT-RL training","cross-domain training","reward strategies","domain-specific customization","cross-domain fusion"],"githubStars":13},"publishedAt":"2026-01-29T07:43:02.000Z","title":"OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models","summary":"The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21639.png","numComments":2,"submittedBy":{"_id":"6572cbc42bb242937c0a1101","avatarUrl":"/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg","fullname":"Xuanle Zhao","name":"xxxllz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.21821","authors":[{"_id":"697c2a0ca67238fac88cc11e","name":"Honglin Lin","hidden":false},{"_id":"697c2a0ca67238fac88cc11f","user":{"_id":"6625ef13605f46d05c1d0031","avatarUrl":"/avatars/22f201dca35e43013cb593884516e96c.svg","isPro":false,"fullname":"Zheng Liu","user":"starriver030515","type":"user"},"name":"Zheng Liu","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:36:23.860Z","hidden":false},{"_id":"697c2a0ca67238fac88cc120","name":"Yun Zhu","hidden":false},{"_id":"697c2a0ca67238fac88cc121","user":{"_id":"67b30bb2c2e25cfcdeda4a2f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67b30bb2c2e25cfcdeda4a2f/K5ePD5uWNkwlpkI_43Oe1.jpeg","isPro":false,"fullname":"Qin, Chonghan","user":"J017athan","type":"user"},"name":"Chonghan Qin","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:36:20.559Z","hidden":false},{"_id":"697c2a0ca67238fac88cc122","name":"Juekai Lin","hidden":false},{"_id":"697c2a0ca67238fac88cc123","name":"Xiaoran Shang","hidden":false},{"_id":"697c2a0ca67238fac88cc124","name":"Conghui He","hidden":false},{"_id":"697c2a0ca67238fac88cc125","name":"Wentao Zhang","hidden":false},{"_id":"697c2a0ca67238fac88cc126","name":"Lijun Wu","hidden":false}],"publishedAt":"2026-01-29T15:07:28.000Z","submittedOnDailyAt":"2026-01-30T01:36:56.361Z","title":"MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods","submittedOnDailyBy":{"_id":"640d99628512ec51d7ef71c7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg","isPro":false,"fullname":"Honglin Lin","user":"LHL3341","type":"user"},"summary":"Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.","upvotes":25,"discussionId":"697c2a0da67238fac88cc127","projectPage":"https://mmfinereason.github.io/","ai_summary":"A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.","ai_keywords":["Vision Language Models","Chain-of-Thought","multimodal reasoning","Qwen3-VL","CoT rationale generation","reasoning quality","difficulty awareness","parameter efficiency","general capabilities"],"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}},"publishedAt":"2026-01-29T10:07:28.000Z","title":"MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods","summary":"Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21821.png","numComments":2,"submittedBy":{"_id":"640d99628512ec51d7ef71c7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg","fullname":"Honglin Lin","name":"LHL3341","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21420","authors":[{"_id":"697c36f4a67238fac88cc1a4","user":{"_id":"65a62085576772f531e13856","avatarUrl":"/avatars/72c67a60422e333ea4e323f7480ae0b7.svg","isPro":false,"fullname":"Huang Zihao","user":"FetchFortune","type":"user"},"name":"Zihao Huang","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:35:39.380Z","hidden":false},{"_id":"697c36f4a67238fac88cc1a5","name":"Jundong Zhou","hidden":false},{"_id":"697c36f4a67238fac88cc1a6","name":"Xingwei Qu","hidden":false},{"_id":"697c36f4a67238fac88cc1a7","name":"Qiyang Min","hidden":false},{"_id":"697c36f4a67238fac88cc1a8","user":{"_id":"638efcf4c67af472d316d424","avatarUrl":"/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg","isPro":false,"fullname":"Ge Zhang","user":"zhangysk","type":"user"},"name":"Ge Zhang","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:44.281Z","hidden":false}],"publishedAt":"2026-01-29T08:58:22.000Z","submittedOnDailyAt":"2026-01-30T02:24:05.556Z","title":"ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation","submittedOnDailyBy":{"_id":"65a62085576772f531e13856","avatarUrl":"/avatars/72c67a60422e333ea4e323f7480ae0b7.svg","isPro":false,"fullname":"Huang Zihao","user":"FetchFortune","type":"user"},"summary":"Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio R before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to R^2times and KV cache by Rtimes. At R=2, empirical measurements show prefill speedups reaching 175\\% and decoding speedups up to 117\\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.","upvotes":22,"discussionId":"697c36f4a67238fac88cc1a9","githubRepo":"https://github.com/ZihaoHuang-notabot/ConceptMoE","githubRepoAddedBy":"user","ai_summary":"ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.","ai_keywords":["ConceptMoE","token-level compute allocation","semantically similar tokens","concept representations","learnable chunk module","inter-token similarity","MoE architecture","attention computation","KV cache","layer looping","prefill speedups","decoding speedups"],"githubStars":6,"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}},"publishedAt":"2026-01-29T03:58:22.000Z","title":"ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation","summary":"Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio R before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to R^2times and KV cache by Rtimes. At R=2, empirical measurements show prefill speedups reaching 175\\% and decoding speedups up to 117\\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21420.png","numComments":2,"submittedBy":{"_id":"65a62085576772f531e13856","avatarUrl":"/avatars/72c67a60422e333ea4e323f7480ae0b7.svg","fullname":"Huang Zihao","name":"FetchFortune","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.22046","authors":[{"_id":"697c65a1a67238fac88cc24b","name":"Changjian Jiang","hidden":false},{"_id":"697c65a1a67238fac88cc24c","name":"Kerui Ren","hidden":false},{"_id":"697c65a1a67238fac88cc24d","name":"Xudong Li","hidden":false},{"_id":"697c65a1a67238fac88cc24e","name":"Kaiwen Song","hidden":false},{"_id":"697c65a1a67238fac88cc24f","name":"Linning Xu","hidden":false},{"_id":"697c65a1a67238fac88cc250","name":"Tao Lu","hidden":false},{"_id":"697c65a1a67238fac88cc251","name":"Junting Dong","hidden":false},{"_id":"697c65a1a67238fac88cc252","name":"Yu Zhang","hidden":false},{"_id":"697c65a1a67238fac88cc253","name":"Bo Dai","hidden":false},{"_id":"697c65a1a67238fac88cc254","name":"Mulin Yu","hidden":false}],"publishedAt":"2026-01-29T17:47:26.000Z","submittedOnDailyAt":"2026-01-30T05:34:08.168Z","title":"PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction","submittedOnDailyBy":{"_id":"656e9e26bbec423d88b603e8","avatarUrl":"/avatars/10d8cb945a60e0401bfa4f74137cb203.svg","isPro":false,"fullname":"MulinYu","user":"UML","type":"user"},"summary":"Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .","upvotes":19,"discussionId":"697c65a1a67238fac88cc255","projectPage":"https://city-super.github.io/PLANING/","ai_summary":"PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.","ai_keywords":["monocular image sequences","hybrid representation","explicit geometric primitives","neural Gaussians","decoupled manner","online initialization","optimization strategy","streaming reconstruction","dense mesh Chamfer-L2","PSNR","ScanNetV2","2D Gaussian Splatting"],"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "}},"publishedAt":"2026-01-29T12:47:26.000Z","title":"PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction","summary":"Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22046.png","numComments":2,"submittedBy":{"_id":"656e9e26bbec423d88b603e8","avatarUrl":"/avatars/10d8cb945a60e0401bfa4f74137cb203.svg","fullname":"MulinYu","name":"UML","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "},"isAuthorParticipating":false},{"paper":{"id":"2601.21337","authors":[{"_id":"697c1ebea67238fac88cc0ae","name":"Xian Shi","hidden":false},{"_id":"697c1ebea67238fac88cc0af","name":"Xiong Wang","hidden":false},{"_id":"697c1ebea67238fac88cc0b0","name":"Zhifang Guo","hidden":false},{"_id":"697c1ebea67238fac88cc0b1","name":"Yongqi Wang","hidden":false},{"_id":"697c1ebea67238fac88cc0b2","name":"Pei Zhang","hidden":false},{"_id":"697c1ebea67238fac88cc0b3","name":"Xinyu Zhang","hidden":false},{"_id":"697c1ebea67238fac88cc0b4","name":"Zishan Guo","hidden":false},{"_id":"697c1ebea67238fac88cc0b5","name":"Hongkun Hao","hidden":false},{"_id":"697c1ebea67238fac88cc0b6","name":"Yu Xi","hidden":false},{"_id":"697c1ebea67238fac88cc0b7","name":"Baosong Yang","hidden":false},{"_id":"697c1ebea67238fac88cc0b8","name":"Jin Xu","hidden":false},{"_id":"697c1ebea67238fac88cc0b9","name":"Jingren Zhou","hidden":false},{"_id":"697c1ebea67238fac88cc0ba","name":"Junyang Lin","hidden":false}],"publishedAt":"2026-01-29T06:58:13.000Z","submittedOnDailyAt":"2026-01-30T02:17:20.204Z","title":"Qwen3-ASR Technical Report","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.","upvotes":18,"discussionId":"697c1ebea67238fac88cc0bb","ai_summary":"The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.","ai_keywords":["speech recognition models","language identification","non-autoregressive models","forced alignment","timestamp prediction","audio understanding","large-scale speech training data","foundation model","TTFT","concurrency"],"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}},"publishedAt":"2026-01-29T01:58:13.000Z","title":"Qwen3-ASR Technical Report","summary":"In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21337.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":221,"isUserFollowing":false},"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.20730","authors":[{"_id":"697b161fdf3e800774f13d05","name":"Shicheng Fang","hidden":false},{"_id":"697b161fdf3e800774f13d06","name":"Yuxin Wang","hidden":false},{"_id":"697b161fdf3e800774f13d07","name":"XiaoRan Liu","hidden":false},{"_id":"697b161fdf3e800774f13d08","name":"Jiahao Lu","hidden":false},{"_id":"697b161fdf3e800774f13d09","name":"Chuanyuan Tan","hidden":false},{"_id":"697b161fdf3e800774f13d0a","name":"Xinchi Chen","hidden":false},{"_id":"697b161fdf3e800774f13d0b","name":"Yining Zheng. Xuanjing Huang","hidden":false},{"_id":"697b161fdf3e800774f13d0c","name":"Xipeng Qiu","hidden":false}],"publishedAt":"2026-01-28T16:05:44.000Z","submittedOnDailyAt":"2026-01-30T06:14:19.038Z","title":"AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts","submittedOnDailyBy":{"_id":"64f033ef82c6eea604c4da8b","avatarUrl":"/avatars/51b93fea7fd68b4274ee03701245dcca.svg","isPro":false,"fullname":"Xiaoran Liu (SII)","user":"SII-xrliu","type":"user"},"summary":"The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.","upvotes":17,"discussionId":"697b1620df3e800774f13d0d","ai_summary":"AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.","ai_keywords":["Large Language Models","autonomous agents","dynamic contexts","AgentLongBench","Lateral Thinking Puzzles","environment rollouts","knowledge-intensive scenarios","knowledge-free scenarios","information density","tool responses","memory fragmentation","long-turn dialogues"],"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}},"publishedAt":"2026-01-28T11:05:44.000Z","title":"AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts","summary":"The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20730.png","numComments":2,"submittedBy":{"_id":"64f033ef82c6eea604c4da8b","avatarUrl":"/avatars/51b93fea7fd68b4274ee03701245dcca.svg","fullname":"Xiaoran Liu (SII)","name":"SII-xrliu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":12,"isUserFollowing":false},"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.22154","authors":[{"_id":"697c2a85a67238fac88cc129","user":{"_id":"6656b9a6ca7f02c3d4817744","avatarUrl":"/avatars/01d32967cad6054389c61b5bb9a528b1.svg","isPro":false,"fullname":"kxbunny","user":"bunny127","type":"user"},"name":"Kaixuan Fan","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:36:18.527Z","hidden":false},{"_id":"697c2a85a67238fac88cc12a","name":"Kaituo Feng","hidden":false},{"_id":"697c2a85a67238fac88cc12b","name":"Manyuan Zhang","hidden":false},{"_id":"697c2a85a67238fac88cc12c","name":"Tianshuo Peng","hidden":false},{"_id":"697c2a85a67238fac88cc12d","name":"Zhixun Li","hidden":false},{"_id":"697c2a85a67238fac88cc12e","name":"Yilei Jiang","hidden":false},{"_id":"697c2a85a67238fac88cc12f","name":"Shuang Chen","hidden":false},{"_id":"697c2a85a67238fac88cc130","name":"Peng Pei","hidden":false},{"_id":"697c2a85a67238fac88cc131","name":"Xunliang Cai","hidden":false},{"_id":"697c2a85a67238fac88cc132","name":"Xiangyu Yue","hidden":false}],"publishedAt":"2026-01-29T18:59:52.000Z","submittedOnDailyAt":"2026-01-30T01:21:23.896Z","title":"Exploring Reasoning Reward Model for Agents","submittedOnDailyBy":{"_id":"6656b9a6ca7f02c3d4817744","avatarUrl":"/avatars/01d32967cad6054389c61b5bb9a528b1.svg","isPro":false,"fullname":"kxbunny","user":"bunny127","type":"user"},"summary":"Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.","upvotes":16,"discussionId":"697c2a85a67238fac88cc133","githubRepo":"https://github.com/kxfan2002/Reagent","githubRepoAddedBy":"user","ai_summary":"Agent-RRM, a multi-faceted reward model, provides structured feedback for agentic trajectories through reasoning traces, critiques, and performance scores, with unified feedback integration showing superior performance across diverse benchmarks.","ai_keywords":["Agentic Reinforcement Learning","reward model","reasoning trace","critique","overall score","Reagent-C","Reagent-R","Reagent-U","GAIA","WebWalkerQA"],"githubStars":14},"publishedAt":"2026-01-29T13:59:52.000Z","title":"Exploring Reasoning Reward Model for Agents","summary":"Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22154.png","numComments":2,"submittedBy":{"_id":"6656b9a6ca7f02c3d4817744","avatarUrl":"/avatars/01d32967cad6054389c61b5bb9a528b1.svg","fullname":"kxbunny","name":"bunny127","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.16914","authors":[{"_id":"697cf81a6676f93322706163","name":"Justin Cui","hidden":false},{"_id":"697cf81a6676f93322706164","name":"Jie Wu","hidden":false},{"_id":"697cf81a6676f93322706165","name":"Ming Li","hidden":false},{"_id":"697cf81a6676f93322706166","name":"Tao Yang","hidden":false},{"_id":"697cf81a6676f93322706167","name":"Xiaojie Li","hidden":false},{"_id":"697cf81a6676f93322706168","name":"Rui Wang","hidden":false},{"_id":"697cf81a6676f93322706169","name":"Andrew Bai","hidden":false},{"_id":"697cf81a6676f9332270616a","name":"Yuanhao Ban","hidden":false},{"_id":"697cf81a6676f9332270616b","name":"Cho-Jui Hsieh","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/SCPS4FUiRSnG90gd3-jMg.mp4"],"publishedAt":"2026-01-23T17:21:35.000Z","submittedOnDailyAt":"2026-01-30T16:00:50.046Z","title":"LoL: Longer than Longer, Scaling Video Generation to Hour","submittedOnDailyBy":{"_id":"65862671e878be571bf9fc52","avatarUrl":"/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg","isPro":false,"fullname":"cuijiaxing","user":"cuijiaxing","type":"user"},"summary":"Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.","upvotes":15,"discussionId":"697cf81a6676f9332270616c","ai_summary":"Researchers developed a method to overcome sink-collapse in autoregressive video generation by addressing the conflict between Rotary Position Embedding and multi-head attention mechanisms, enabling real-time streaming of videos up to 12 hours long.","ai_keywords":["Rotary Position Embedding","multi-head attention","autoregressive models","sink-collapse","attention sink frames","RoPE jitter","video generation","streaming video generation"]},"publishedAt":"2026-01-23T12:21:35.000Z","title":"LoL: Longer than Longer, Scaling Video Generation to Hour","summary":"Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/SCPS4FUiRSnG90gd3-jMg.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16914.png","numComments":1,"submittedBy":{"_id":"65862671e878be571bf9fc52","avatarUrl":"/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg","fullname":"cuijiaxing","name":"cuijiaxing","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.21754","authors":[{"_id":"697c1b0da67238fac88cc08d","user":{"_id":"645503e102912fad3f2202fb","avatarUrl":"/avatars/87d8c80f4ff7920d990be3c3b94dfc64.svg","isPro":false,"fullname":"Haoyu Wang","user":"Harryis","type":"user"},"name":"Haoyu Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:36:27.856Z","hidden":false},{"_id":"697c1b0da67238fac88cc08e","name":"Guozheng Ma","hidden":false},{"_id":"697c1b0da67238fac88cc08f","name":"Shugang Cui","hidden":false},{"_id":"697c1b0da67238fac88cc090","name":"Yilun Kong","hidden":false},{"_id":"697c1b0da67238fac88cc091","name":"Haotian Luo","hidden":false},{"_id":"697c1b0da67238fac88cc092","name":"Li Shen","hidden":false},{"_id":"697c1b0da67238fac88cc093","name":"Mengya Gao","hidden":false},{"_id":"697c1b0da67238fac88cc094","name":"Yichao Wu","hidden":false},{"_id":"697c1b0da67238fac88cc095","name":"Xiaogang Wang","hidden":false},{"_id":"697c1b0da67238fac88cc096","name":"Dacheng Tao","hidden":false}],"publishedAt":"2026-01-29T14:08:41.000Z","submittedOnDailyAt":"2026-01-30T00:16:13.782Z","title":"Language-based Trial and Error Falls Behind in the Era of Experience","submittedOnDailyBy":{"_id":"632ab8f5a968c34257da5c52","avatarUrl":"/avatars/59df09e6c9e1e633170514d950ad7981.svg","isPro":false,"fullname":"Haotian Luo","user":"LordNoah","type":"user"},"summary":"While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.","upvotes":13,"discussionId":"697c1b0da67238fac88cc097","projectPage":"https://scout-cs.github.io/","githubRepo":"https://github.com/Harry-mic/SCOUT","githubRepoAddedBy":"user","ai_summary":"A novel framework called SCOUT is introduced that uses lightweight scouts to reduce exploration costs for large language models in nonlinguistic environments, enabling improved performance through supervised fine-tuning and reinforcement learning.","ai_keywords":["Large Language Models","agentic tasks","pretraining distribution","testing distribution","exploration cost","trial-and-error","parameter-heavy LLMs","high dimensional semantic space","lightweight scouts","MLPs","environmental dynamics","Supervised Fine-Tuning","reinforcement learning","Qwen2.5-3B-Instruct","Gemini-2.5-Pro"],"githubStars":7,"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"}},"publishedAt":"2026-01-29T09:08:41.000Z","title":"Language-based Trial and Error Falls Behind in the Era of Experience","summary":"While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21754.png","numComments":2,"submittedBy":{"_id":"632ab8f5a968c34257da5c52","avatarUrl":"/avatars/59df09e6c9e1e633170514d950ad7981.svg","fullname":"Haotian Luo","name":"LordNoah","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.22157","authors":[{"_id":"697c5bb6a67238fac88cc23d","name":"Jonathan Kahana","hidden":false},{"_id":"697c5bb6a67238fac88cc23e","name":"Eliahu Horwitz","hidden":false},{"_id":"697c5bb6a67238fac88cc23f","name":"Yedid Hoshen","hidden":false}],"publishedAt":"2026-01-29T18:59:55.000Z","submittedOnDailyAt":"2026-01-30T07:53:18.197Z","title":"Discovering Hidden Gems in Model Repositories","submittedOnDailyBy":{"_id":"6465fd33dac127ac80f0b334","avatarUrl":"/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg","isPro":false,"fullname":"Jonathan Kahana","user":"jonkahana","type":"user"},"summary":"Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.","upvotes":11,"discussionId":"697c5bb6a67238fac88cc240","ai_summary":"Hidden superior models exist in public repositories but are overlooked due to inefficient discovery methods; a multi-armed bandit approach using shared query sets and aggressive elimination significantly accelerates identification of top-performing models.","ai_keywords":["multi-armed bandit","sequential halving","model discovery","shared query sets","aggressive elimination"]},"publishedAt":"2026-01-29T13:59:55.000Z","title":"Discovering Hidden Gems in Model Repositories","summary":"Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22157.png","numComments":3,"submittedBy":{"_id":"6465fd33dac127ac80f0b334","avatarUrl":"/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg","fullname":"Jonathan Kahana","name":"jonkahana","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.22083","authors":[{"_id":"697cfaa26676f9332270616e","name":"Enyi Jiang","hidden":false},{"_id":"697cfaa26676f9332270616f","name":"Yibo Jacky Zhang","hidden":false},{"_id":"697cfaa26676f93322706170","name":"Yinglun Xu","hidden":false},{"_id":"697cfaa26676f93322706171","name":"Andreas Haupt","hidden":false},{"_id":"697cfaa26676f93322706172","name":"Nancy Amato","hidden":false},{"_id":"697cfaa26676f93322706173","name":"Sanmi Koyejo","hidden":false}],"publishedAt":"2026-01-29T18:21:57.000Z","submittedOnDailyAt":"2026-01-30T16:10:48.655Z","title":"Latent Adversarial Regularization for Offline Preference Optimization","submittedOnDailyBy":{"_id":"6058351db2c84d5386b3afe5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6058351db2c84d5386b3afe5/mS1Nu9i5VYgAnHSKXz-b4.jpeg","isPro":false,"fullname":"Enyi (Olivia) Jiang","user":"EnyiJiang","type":"user"},"summary":"Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.","upvotes":10,"discussionId":"697cfaa36676f93322706174","githubRepo":"https://github.com/enyijiang/GANPO","githubRepoAddedBy":"user","ai_summary":"GANPO uses latent-space regularization through adversarial divergence minimization to improve language model preference optimization, offering more robust structural feedback than token-level methods.","ai_keywords":["preference optimization","language models","latent-space regularization","token-level regularization","policy model","reference model","adversarial approach","GANs","latent-space divergence","offline preference optimization","distributional shift","computational overhead"],"githubStars":0,"organization":{"_id":"672c672dcf09d152f4da04c4","name":"StanfordUniversity","fullname":"Stanford University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"}},"publishedAt":"2026-01-29T13:21:57.000Z","title":"Latent Adversarial Regularization for Offline Preference Optimization","summary":"Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22083.png","numComments":1,"submittedBy":{"_id":"6058351db2c84d5386b3afe5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6058351db2c84d5386b3afe5/mS1Nu9i5VYgAnHSKXz-b4.jpeg","fullname":"Enyi (Olivia) Jiang","name":"EnyiJiang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"672c672dcf09d152f4da04c4","name":"StanfordUniversity","fullname":"Stanford University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.21590","authors":[{"_id":"697c85936676f93322706019","user":{"_id":"667b6e28a0e1bc140664e725","avatarUrl":"/avatars/2e4a69a0f98ce6de3f044b2831609ea4.svg","isPro":false,"fullname":"xiaotong","user":"xtongji","type":"user"},"name":"Xiaotong Ji","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:30.083Z","hidden":false},{"_id":"697c85936676f9332270601a","name":"Rasul Tutunov","hidden":false},{"_id":"697c85936676f9332270601b","name":"Matthieu Zimmer","hidden":false},{"_id":"697c85936676f9332270601c","name":"Haitham Bou Ammar","hidden":false}],"publishedAt":"2026-01-29T12:01:53.000Z","submittedOnDailyAt":"2026-01-30T07:59:11.002Z","title":"Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening","submittedOnDailyBy":{"_id":"631c375768f7da9ad2496bf6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg","isPro":true,"fullname":"Haitham Bou Ammar","user":"hba123","type":"user"},"summary":"Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.","upvotes":10,"discussionId":"697c85936676f9332270601d","ai_summary":"A theoretically grounded method for improving large language model reasoning performance through distribution sharpening without iterative sampling or external rewards, achieving comparable results to reinforcement learning post-training with significantly reduced computational costs.","ai_keywords":["reinforcement learning","large language models","distribution sharpening","Markov chain Monte Carlo","power distribution","low-temperature sampling","autoregressive generation","GRPO","inference latency"]},"publishedAt":"2026-01-29T07:01:53.000Z","title":"Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening","summary":"Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21590.png","numComments":7,"submittedBy":{"_id":"631c375768f7da9ad2496bf6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg","fullname":"Haitham Bou Ammar","name":"hba123","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":110,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.21571","authors":[{"_id":"697caa116676f933227060d6","user":{"_id":"66452e93ac4e2c877e9d8990","avatarUrl":"/avatars/9fdbe17d458d1cde67973f9a4dfd43f6.svg","isPro":true,"fullname":"neil rathi","user":"neilrathi","type":"user"},"name":"Neil Rathi","status":"admin_assigned","statusLastChangedAt":"2026-01-30T16:49:03.380Z","hidden":false},{"_id":"697caa116676f933227060d7","user":{"_id":"67e8489c88acefd725236410","avatarUrl":"/avatars/691552eea2f4352c7d5e948f0aff0a52.svg","isPro":false,"fullname":"Alec Radford","user":"AlecRadford","type":"user"},"name":"Alec Radford","status":"admin_assigned","statusLastChangedAt":"2026-01-30T16:48:53.758Z","hidden":false}],"publishedAt":"2026-01-29T11:34:01.000Z","submittedOnDailyAt":"2026-01-30T14:19:59.651Z","title":"Shaping capabilities with token-level data filtering","submittedOnDailyBy":{"_id":"5dd96eb166059660ed1ee413","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg","isPro":true,"fullname":"Julien Chaumond","user":"julien-c","type":"user"},"summary":"Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.","upvotes":8,"discussionId":"697caa116676f933227060d8","githubRepo":"https://github.com/neilrathi/token-filtering","githubRepoAddedBy":"admin","ai_summary":"Token filtering during pretraining effectively reduces unwanted language model capabilities while maintaining alignment, becoming more effective at larger scales and tolerating noisy labels with sufficient compute.","ai_keywords":["language models","pretraining","token filtering","data attribution","sparse autoencoders","classifier distillation","model scaling","computational efficiency","adversarial robustness"],"githubStars":10,"organization":{"_id":"63924ab637e424786530c90e","name":"Anthropic","fullname":"Anthropic","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670531762351-6200d0a443eb0913fa2df7cc.png"}},"publishedAt":"2026-01-29T06:34:01.000Z","title":"Shaping capabilities with token-level data filtering","summary":"Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21571.png","numComments":2,"submittedBy":{"_id":"5dd96eb166059660ed1ee413","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg","fullname":"Julien Chaumond","name":"julien-c","type":"user","isPro":true,"isHf":true,"isHfAdmin":true,"isMod":false,"followerCount":3886,"isUserFollowing":false},"organization":{"_id":"63924ab637e424786530c90e","name":"Anthropic","fullname":"Anthropic","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670531762351-6200d0a443eb0913fa2df7cc.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21051","authors":[{"_id":"697c1898a67238fac88cc076","name":"Zhuoran Yang","hidden":false},{"_id":"697c1898a67238fac88cc077","name":"Ed Li","hidden":false},{"_id":"697c1898a67238fac88cc078","name":"Jianliang He","hidden":false},{"_id":"697c1898a67238fac88cc079","user":{"_id":"620042b28c2eb991da50d34e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg","isPro":true,"fullname":"Aman Priyanshu","user":"AmanPriyanshu","type":"user"},"name":"Aman Priyanshu","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:54.307Z","hidden":false},{"_id":"697c1898a67238fac88cc07a","name":"Baturay Saglam","hidden":false},{"_id":"697c1898a67238fac88cc07b","name":"Paul Kassianik","hidden":false},{"_id":"697c1898a67238fac88cc07c","name":"Sajana Weerawardhena","hidden":false},{"_id":"697c1898a67238fac88cc07d","name":"Anu Vellore","hidden":false},{"_id":"697c1898a67238fac88cc07e","name":"Blaine Nelson","hidden":false},{"_id":"697c1898a67238fac88cc07f","name":"Neusha Javidnia","hidden":false},{"_id":"697c1898a67238fac88cc080","name":"Arthur Goldblatt","hidden":false},{"_id":"697c1898a67238fac88cc081","name":"Fraser Burch","hidden":false},{"_id":"697c1898a67238fac88cc082","name":"Avi Zohary","hidden":false},{"_id":"697c1898a67238fac88cc083","name":"Assaf Eisenman","hidden":false},{"_id":"697c1898a67238fac88cc084","name":"Mahdi Sabbaghi","hidden":false},{"_id":"697c1898a67238fac88cc085","user":{"_id":"624e9a2994078e6b99e87b93","avatarUrl":"/avatars/f59f7f593b8b1d3258786351803dc1bd.svg","isPro":true,"fullname":"Supriti Vijay","user":"SupritiVijay","type":"user"},"name":"Supriti Vijay","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:56.159Z","hidden":false},{"_id":"697c1898a67238fac88cc086","name":"Rahim Dharssi","hidden":false},{"_id":"697c1898a67238fac88cc087","name":"Dhruv Kedia","hidden":false},{"_id":"697c1898a67238fac88cc088","name":"Kojin Oshiba","hidden":false},{"_id":"697c1898a67238fac88cc089","name":"Yaron Singer","hidden":false},{"_id":"697c1898a67238fac88cc08a","name":"Amin Karbasi","hidden":false}],"publishedAt":"2026-01-28T21:15:24.000Z","submittedOnDailyAt":"2026-01-30T00:04:43.826Z","title":"Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report","submittedOnDailyBy":{"_id":"646d769cda8e99940b71928e","avatarUrl":"/avatars/acee495a23362aa39b3d3e75c9afd967.svg","isPro":false,"fullname":"Zhuoran Yang","user":"zhuoranyang","type":"user"},"summary":"We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.","upvotes":8,"discussionId":"697c1899a67238fac88cc08b","projectPage":"https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning","ai_summary":"A two-stage trained cybersecurity reasoning model achieves competitive performance on specialized tasks while maintaining general capabilities through supervised fine-tuning and reinforcement learning from verifiable rewards.","ai_keywords":["supervised fine-tuning","reinforcement learning from verifiable rewards","cybersecurity analysis","multi-hop reasoning","safety performance"],"organization":{"_id":"67cb6bcf560c3dcbb1a9c8b6","name":"fdtn-ai","fullname":"Cisco Foundation AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6573a9fe769f3ee9bdf4d9c7/MfBxEGubvNKGKnWcmR_Cu.png"}},"publishedAt":"2026-01-28T16:15:24.000Z","title":"Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report","summary":"We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21051.png","numComments":2,"submittedBy":{"_id":"646d769cda8e99940b71928e","avatarUrl":"/avatars/acee495a23362aa39b3d3e75c9afd967.svg","fullname":"Zhuoran Yang","name":"zhuoranyang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67cb6bcf560c3dcbb1a9c8b6","name":"fdtn-ai","fullname":"Cisco Foundation AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6573a9fe769f3ee9bdf4d9c7/MfBxEGubvNKGKnWcmR_Cu.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.18129","authors":[{"_id":"6979f992df3e800774f139a0","name":"Kunat Pipatanakul","hidden":false},{"_id":"6979f992df3e800774f139a1","user":{"_id":"615313b0793ef66b3324da1f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg","isPro":false,"fullname":"Pittawat Taveekitworachai","user":"pittawat","type":"user"},"name":"Pittawat Taveekitworachai","status":"claimed_verified","statusLastChangedAt":"2026-01-28T14:41:40.587Z","hidden":false}],"publishedAt":"2026-01-26T04:20:59.000Z","submittedOnDailyAt":"2026-01-30T00:34:54.063Z","title":"Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models","submittedOnDailyBy":{"_id":"62d192c2d50433c35eb1b48e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62d192c2d50433c35eb1b48e/VjmDu8GOIuLuQNBQdQLLS.png","isPro":false,"fullname":"Kunat Pipatanakul","user":"kunato","type":"user"},"summary":"Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.","upvotes":8,"discussionId":"6979f993df3e800774f139a2","projectPage":"https://opentyphoon.ai/model/typhoon-s","githubRepo":"https://github.com/scb-10x/typhoon-s","githubRepoAddedBy":"user","ai_summary":"A minimal post-training approach using supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning enables the development of high-quality sovereign language models with reduced resource requirements.","ai_keywords":["supervised fine-tuning","on-policy distillation","reinforcement fine-tuning","GRPO","InK-GRPO","instruction tuning","sovereign language models","minimal post-training recipe"],"githubStars":5,"organization":{"_id":"63e9cdf9dd2c4effdd6d39c0","name":"typhoon-ai","fullname":"Typhoon","avatar":"https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"}},"publishedAt":"2026-01-25T23:20:59.000Z","title":"Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models","summary":"Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18129.png","numComments":3,"submittedBy":{"_id":"62d192c2d50433c35eb1b48e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62d192c2d50433c35eb1b48e/VjmDu8GOIuLuQNBQdQLLS.png","fullname":"Kunat Pipatanakul","name":"kunato","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":15,"isUserFollowing":false},"organization":{"_id":"63e9cdf9dd2c4effdd6d39c0","name":"typhoon-ai","fullname":"Typhoon","avatar":"https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.22069","authors":[{"_id":"697c258aa67238fac88cc0f3","name":"Yibo Wang","hidden":false},{"_id":"697c258aa67238fac88cc0f4","name":"Yongcheng Jing","hidden":false},{"_id":"697c258aa67238fac88cc0f5","name":"Shunyu Liu","hidden":false},{"_id":"697c258aa67238fac88cc0f6","name":"Hao Guan","hidden":false},{"_id":"697c258aa67238fac88cc0f7","name":"Rong-cheng Tu","hidden":false},{"_id":"697c258aa67238fac88cc0f8","name":"Chengyu Wang","hidden":false},{"_id":"697c258aa67238fac88cc0f9","name":"Jun Huang","hidden":false},{"_id":"697c258aa67238fac88cc0fa","name":"Dacheng Tao","hidden":false}],"publishedAt":"2026-01-29T18:07:39.000Z","submittedOnDailyAt":"2026-01-30T01:03:53.017Z","title":"VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning","submittedOnDailyBy":{"_id":"66f16d166f7038039a1e2770","avatarUrl":"/avatars/0a30d3e9af3b109ce4b82396b0e8d685.svg","isPro":false,"fullname":"Yibo Wang","user":"yiboowang","type":"user"},"summary":"Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as \"optical memory.\" We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.","upvotes":7,"discussionId":"697c258aa67238fac88cc0fb","githubRepo":"https://github.com/w-yibo/VTC-R1","githubRepoAddedBy":"user","ai_summary":"VTC-R1 enables efficient long-context reasoning by compressing textual traces into compact images and iteratively feeding them back into vision-language models as optical memory, achieving significant speedup without sacrificing performance.","ai_keywords":["vision-language models","optical memory","token compression","long-context reasoning","vision-text compression","VLMs-Glyph","Qwen3-VL","OpenR1-Math-220K","MATH500","AIME25","AMC23","GPQA-D"],"githubStars":12,"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"}},"publishedAt":"2026-01-29T13:07:39.000Z","title":"VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning","summary":"Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as \"optical memory.\" We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22069.png","numComments":2,"submittedBy":{"_id":"66f16d166f7038039a1e2770","avatarUrl":"/avatars/0a30d3e9af3b109ce4b82396b0e8d685.svg","fullname":"Yibo Wang","name":"yiboowang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21181","authors":[{"_id":"697c3180a67238fac88cc175","user":{"_id":"65a4bf8e90b5e87bcdff41c7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a4bf8e90b5e87bcdff41c7/I6OSoZigV7Fl6OKRl_Yqy.jpeg","isPro":false,"fullname":"Sangyun Chung","user":"topyun","type":"user"},"name":"Sangyun Chung","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:36:12.512Z","hidden":false},{"_id":"697c3180a67238fac88cc176","name":"Se Yeon Kim","hidden":false},{"_id":"697c3180a67238fac88cc177","name":"Youngchae Chee","hidden":false},{"_id":"697c3180a67238fac88cc178","name":"Yong Man Ro","hidden":false}],"publishedAt":"2026-01-29T02:30:32.000Z","submittedOnDailyAt":"2026-01-30T01:51:57.494Z","title":"MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models","submittedOnDailyBy":{"_id":"65a4bf8e90b5e87bcdff41c7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a4bf8e90b5e87bcdff41c7/I6OSoZigV7Fl6OKRl_Yqy.jpeg","isPro":false,"fullname":"Sangyun Chung","user":"topyun","type":"user"},"summary":"Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\\% and 2.0\\% improvements for VideoLLaMA2-AV, 8.7\\% and 4.7\\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at https://github.com/top-yun/MAD{https://github.com/top-yun/MAD}","upvotes":7,"discussionId":"697c3180a67238fac88cc179","ai_summary":"Multimodal Large Language Models suffer from cross-modal hallucinations where one modality incorrectly influences generation from another, leading to fabricated outputs; this exposes a fundamental deficiency in modality-interaction control. To address this, a training-free method called Modality-Adaptive Decoding (MAD) is proposed that adaptively weights modality-specific decoding branches based on task requirements by leveraging the model's inherent ability to self-assess modality relevance. MAD uses extracted modality probabilities to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models, showing that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning.","ai_keywords":["Multimodal Large Language Models","cross-modal hallucinations","modality-interaction control","Modality-Adaptive Decoding","contrastive decoding","modality-specific decoding branches","self-assessment","modality probabilities"]},"publishedAt":"2026-01-28T21:30:32.000Z","title":"MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models","summary":"Multimodal Large Language Models (MLLMs) suffer from cross-modal hallucinations, where one modality inappropriately influences generation about another, leading to fabricated output. This exposes a more fundamental deficiency in modality-interaction control. To address this, we propose Modality-Adaptive Decoding (MAD), a training-free method that adaptively weights modality-specific decoding branches based on task requirements. MAD leverages the model's inherent ability to self-assess modality relevance by querying which modalities are needed for each task. The extracted modality probabilities are then used to adaptively weight contrastive decoding branches, enabling the model to focus on relevant information while suppressing cross-modal interference. Extensive experiments on CMM and AVHBench demonstrate that MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models (7.8\\% and 2.0\\% improvements for VideoLLaMA2-AV, 8.7\\% and 4.7\\% improvements for Qwen2.5-Omni). Our approach demonstrates that explicit modality awareness through self-assessment is crucial for robust multimodal reasoning, offering a principled extension to existing contrastive decoding methods. Our code is available at https://github.com/top-yun/MAD{https://github.com/top-yun/MAD}","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21181.png","numComments":2,"submittedBy":{"_id":"65a4bf8e90b5e87bcdff41c7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a4bf8e90b5e87bcdff41c7/I6OSoZigV7Fl6OKRl_Yqy.jpeg","fullname":"Sangyun Chung","name":"topyun","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.20975","authors":[{"_id":"697c2f91a67238fac88cc159","name":"Nikita Gupta","hidden":false},{"_id":"697c2f91a67238fac88cc15a","name":"Riju Chatterjee","hidden":false},{"_id":"697c2f91a67238fac88cc15b","name":"Lukas Haas","hidden":false},{"_id":"697c2f91a67238fac88cc15c","name":"Connie Tao","hidden":false},{"_id":"697c2f91a67238fac88cc15d","name":"Andrew Wang","hidden":false},{"_id":"697c2f91a67238fac88cc15e","name":"Chang Liu","hidden":false},{"_id":"697c2f91a67238fac88cc15f","name":"Hidekazu Oiwa","hidden":false},{"_id":"697c2f91a67238fac88cc160","name":"Elena Gribovskaya","hidden":false},{"_id":"697c2f91a67238fac88cc161","name":"Jan Ackermann","hidden":false},{"_id":"697c2f91a67238fac88cc162","name":"John Blitzer","hidden":false},{"_id":"697c2f91a67238fac88cc163","name":"Sasha Goldshtein","hidden":false},{"_id":"697c2f91a67238fac88cc164","name":"Dipanjan Das","hidden":false}],"publishedAt":"2026-01-28T19:20:47.000Z","submittedOnDailyAt":"2026-01-30T01:42:18.203Z","title":"DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.","upvotes":6,"discussionId":"697c2f91a67238fac88cc165","projectPage":"https://www.kaggle.com/benchmarks/google/dsqa/leaderboard","ai_summary":"DeepSearchQA presents a 900-prompt benchmark evaluating agents on complex multi-step information-seeking tasks requiring systematic information collation, deduplication, and reasoning about stopping criteria across 17 fields.","ai_keywords":["multi-step information-seeking tasks","causal chain","systematic collation","de-duplication","entity resolution","long-horizon planning","context retention","open-ended search space","agent architectures","recall","precision","premature stopping","hedging behaviors","deep-research capabilities"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-01-28T14:20:47.000Z","title":"DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents","summary":"We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20975.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":221,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21598","authors":[{"_id":"697c20f1a67238fac88cc0ce","user":{"_id":"67a1d21e33e92b4a1183f3bb","avatarUrl":"/avatars/43f9dd3fcb7d58ddc69562fd1fc12957.svg","isPro":false,"fullname":"Zhi Zheng","user":"zz1358m","type":"user"},"name":"Zhi Zheng","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:36:25.968Z","hidden":false},{"_id":"697c20f1a67238fac88cc0cf","name":"Wee Sun Lee","hidden":false}],"publishedAt":"2026-01-29T12:07:16.000Z","submittedOnDailyAt":"2026-01-30T00:39:38.307Z","title":"Beyond Imitation: Reinforcement Learning for Active Latent Planning","submittedOnDailyBy":{"_id":"67a1d21e33e92b4a1183f3bb","avatarUrl":"/avatars/43f9dd3fcb7d58ddc69562fd1fc12957.svg","isPro":false,"fullname":"Zhi Zheng","user":"zz1358m","type":"user"},"summary":"Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the Active Latent Planning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\\% accuracy and -3.3\\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.","upvotes":5,"discussionId":"697c20f2a67238fac88cc0d0","ai_summary":"Active latent planning method improves reasoning accuracy and efficiency by modeling latent token supervision as conditional VAE and using reinforcement learning with coherence rewards.","ai_keywords":["chain-of-thought reasoning","latent reasoning","large language models","latent tokens","conditional variational auto-encoder","reinforcement learning","coherence reward","dense latent space","active planning"]},"publishedAt":"2026-01-29T07:07:16.000Z","title":"Beyond Imitation: Reinforcement Learning for Active Latent Planning","summary":"Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the Active Latent Planning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\\% accuracy and -3.3\\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21598.png","numComments":2,"submittedBy":{"_id":"67a1d21e33e92b4a1183f3bb","avatarUrl":"/avatars/43f9dd3fcb7d58ddc69562fd1fc12957.svg","fullname":"Zhi Zheng","name":"zz1358m","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.22158","authors":[{"_id":"697c6ad6a67238fac88cc276","name":"Yiyang Lu","hidden":false},{"_id":"697c6ad6a67238fac88cc277","name":"Susie Lu","hidden":false},{"_id":"697c6ad6a67238fac88cc278","name":"Qiao Sun","hidden":false},{"_id":"697c6ad6a67238fac88cc279","name":"Hanhong Zhao","hidden":false},{"_id":"697c6ad6a67238fac88cc27a","name":"Zhicheng Jiang","hidden":false},{"_id":"697c6ad6a67238fac88cc27b","name":"Xianbang Wang","hidden":false},{"_id":"697c6ad6a67238fac88cc27c","name":"Tianhong Li","hidden":false},{"_id":"697c6ad6a67238fac88cc27d","name":"Zhengyang Geng","hidden":false},{"_id":"697c6ad6a67238fac88cc27e","name":"Kaiming He","hidden":false}],"publishedAt":"2026-01-29T18:59:56.000Z","submittedOnDailyAt":"2026-01-30T05:55:44.778Z","title":"One-step Latent-free Image Generation with Pixel Mean Flows","submittedOnDailyBy":{"_id":"661678b244425d16e37f2341","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/661678b244425d16e37f2341/VUSQ3S0IXZLoIxO4oX4cv.png","isPro":false,"fullname":"Yiyang Lu","user":"Lyy0725","type":"user"},"summary":"Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.","upvotes":4,"discussionId":"697c6ad6a67238fac88cc27f","ai_summary":"Pixel MeanFlow introduces a one-step latent-free image generation method by separating network output space from loss space, achieving strong performance on ImageNet at multiple resolutions.","ai_keywords":["diffusion models","flow-based models","multi-step sampling","latent space","one-step generation","image manifold","MeanFlow","velocity space","x-prediction"]},"publishedAt":"2026-01-29T13:59:56.000Z","title":"One-step Latent-free Image Generation with Pixel Mean Flows","summary":"Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22158.png","numComments":2,"submittedBy":{"_id":"661678b244425d16e37f2341","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/661678b244425d16e37f2341/VUSQ3S0IXZLoIxO4oX4cv.png","fullname":"Yiyang Lu","name":"Lyy0725","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.22156","authors":[{"_id":"697c6643a67238fac88cc257","user":{"_id":"6144e4667f2544bb450787b2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6144e4667f2544bb450787b2/5wSDDqJbI4TGtBLP9IvjY.png","isPro":false,"fullname":"Yingfa Chen","user":"chen-yingfa","type":"user"},"name":"Yingfa Chen","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:35:16.767Z","hidden":false},{"_id":"697c6643a67238fac88cc258","name":"Zhen Leng Thai","hidden":false},{"_id":"697c6643a67238fac88cc259","name":"Zihan Zhou","hidden":false},{"_id":"697c6643a67238fac88cc25a","name":"Zhu Zhang","hidden":false},{"_id":"697c6643a67238fac88cc25b","name":"Xingyu Shen","hidden":false},{"_id":"697c6643a67238fac88cc25c","name":"Shuo Wang","hidden":false},{"_id":"697c6643a67238fac88cc25d","name":"Chaojun Xiao","hidden":false},{"_id":"697c6643a67238fac88cc25e","name":"Xu Han","hidden":false},{"_id":"697c6643a67238fac88cc25f","name":"Zhiyuan Liu","hidden":false}],"publishedAt":"2026-01-29T18:59:53.000Z","submittedOnDailyAt":"2026-01-30T05:36:50.213Z","title":"Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts","submittedOnDailyBy":{"_id":"6144e4667f2544bb450787b2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6144e4667f2544bb450787b2/5wSDDqJbI4TGtBLP9IvjY.png","isPro":false,"fullname":"Yingfa Chen","user":"chen-yingfa","type":"user"},"summary":"Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data","upvotes":4,"discussionId":"697c6644a67238fac88cc260","githubRepo":"https://github.com/thunlp/hybrid-linear-attention","githubRepoAddedBy":"user","ai_summary":"HALO enables efficient conversion of Transformer models to RNN-attention hybrid architectures with improved long-context performance using minimal training data.","ai_keywords":["Hybrid Transformer architectures","softmax attention blocks","recurrent neural networks","parameter transfer","knowledge distillation","Transformer models","RNN-attention hybrid models","HALO","HypeNet","position encoding","HyPE","Qwen3 series"],"githubStars":3,"organization":{"_id":"633fe81429b5a95f6e16e34a","name":"openbmb","fullname":"OpenBMB","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"}},"publishedAt":"2026-01-29T13:59:53.000Z","title":"Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts","summary":"Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22156.png","numComments":3,"submittedBy":{"_id":"6144e4667f2544bb450787b2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6144e4667f2544bb450787b2/5wSDDqJbI4TGtBLP9IvjY.png","fullname":"Yingfa Chen","name":"chen-yingfa","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"633fe81429b5a95f6e16e34a","name":"openbmb","fullname":"OpenBMB","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.22146","authors":[{"_id":"697c1ec2a67238fac88cc0bd","name":"Ajay Patel","hidden":false},{"_id":"697c1ec2a67238fac88cc0be","name":"Colin Raffel","hidden":false},{"_id":"697c1ec2a67238fac88cc0bf","name":"Chris Callison-Burch","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/gjMgF1Ov8Nb1ol6DXci-x.png"],"publishedAt":"2026-01-29T18:58:47.000Z","submittedOnDailyAt":"2026-01-30T08:15:21.082Z","title":"FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale","submittedOnDailyBy":{"_id":"5e6a3d4ea9afd5125d9ec064","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg","isPro":true,"fullname":"Stefan Schweter","user":"stefan-it","type":"user"},"summary":"Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .","upvotes":4,"discussionId":"697c1ec2a67238fac88cc0c0","ai_summary":"Large language models can be pre-trained from scratch using synthetic instruction-response pairs generated from unstructured text corpora, outperforming traditional methods on benchmarks measuring response quality.","ai_keywords":["large language models","self-supervised learning","predict the next word","instruction-tuning","synthetic training data","instruction templates","unstructured text data","pre-training","downstream usage","response quality"],"organization":{"_id":"677fd99972d67fdcd1624163","name":"fineinstructions","fullname":"FineInstructions","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61c40eeb727d1257bf3cf5ba/PbZWCoc-IhhHGU4H7kitU.jpeg"}},"publishedAt":"2026-01-29T13:58:47.000Z","title":"FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale","summary":"Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/gjMgF1Ov8Nb1ol6DXci-x.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22146.png","numComments":4,"submittedBy":{"_id":"5e6a3d4ea9afd5125d9ec064","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg","fullname":"Stefan Schweter","name":"stefan-it","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3735,"isUserFollowing":false},"organization":{"_id":"677fd99972d67fdcd1624163","name":"fineinstructions","fullname":"FineInstructions","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61c40eeb727d1257bf3cf5ba/PbZWCoc-IhhHGU4H7kitU.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.21579","authors":[{"_id":"697c9eaa6676f933227060a3","user":{"_id":"67884550b5c0ef3754135924","avatarUrl":"/avatars/01bc97f18c4c2f35accfcf3b8a4fb5e0.svg","isPro":false,"fullname":"Wuyang Zhou","user":"WuyangZzzz","type":"user"},"name":"Wuyang Zhou","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:13.092Z","hidden":false},{"_id":"697c9eaa6676f933227060a4","name":"Yuxuan Gu","hidden":false},{"_id":"697c9eaa6676f933227060a5","name":"Giorgos Iacovides","hidden":false},{"_id":"697c9eaa6676f933227060a6","name":"Danilo Mandic","hidden":false}],"publishedAt":"2026-01-29T11:43:05.000Z","submittedOnDailyAt":"2026-01-30T11:18:11.150Z","title":"KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices","submittedOnDailyBy":{"_id":"67884550b5c0ef3754135924","avatarUrl":"/avatars/01bc97f18c4c2f35accfcf3b8a4fb5e0.svg","isPro":false,"fullname":"Wuyang Zhou","user":"WuyangZzzz","type":"user"},"summary":"The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive O(n^3C) parameter complexity with n as the width of the residual stream and C as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, O left( nC cdot n! right). To address both challenges, we propose KromHC, which uses the Kronecker products of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to O(n^2C). Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at https://github.com/wz1119/KromHC.","upvotes":4,"discussionId":"697c9eaa6676f933227060a7","githubRepo":"https://github.com/wz1119/KromHC","githubRepoAddedBy":"user","ai_summary":"KromHC addresses training instability and scalability issues in hyper-connections by using Kronecker products to parametrize residual matrices with reduced parameter complexity.","ai_keywords":["Hyper-Connections","Manifold-Constrained Hyper-Connections","Birkhoff polytope","Sinkhorn-Knopp algorithm","doubly stochastic matrices","Birkhoff-von-Neumann theorem","Kronecker products","tensorized residual stream","parameter complexity"],"githubStars":1},"publishedAt":"2026-01-29T06:43:05.000Z","title":"KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices","summary":"The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive O(n^3C) parameter complexity with n as the width of the residual stream and C as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, O left( nC cdot n! right). To address both challenges, we propose KromHC, which uses the Kronecker products of smaller doubly stochastic matrices to parametrize the residual matrix in mHC. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to O(n^2C). Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at https://github.com/wz1119/KromHC.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21579.png","numComments":3,"submittedBy":{"_id":"67884550b5c0ef3754135924","avatarUrl":"/avatars/01bc97f18c4c2f35accfcf3b8a4fb5e0.svg","fullname":"Wuyang Zhou","name":"WuyangZzzz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.21343","authors":[{"_id":"697c3816a67238fac88cc1c3","name":"Ellen Xiaoqing Tan","hidden":false},{"_id":"697c3816a67238fac88cc1c4","name":"Shehzaad Dhuliawala","hidden":false},{"_id":"697c3816a67238fac88cc1c5","name":"Jing Xu","hidden":false},{"_id":"697c3816a67238fac88cc1c6","name":"Ping Yu","hidden":false},{"_id":"697c3816a67238fac88cc1c7","name":"Sainbayar Sukhbaatar","hidden":false},{"_id":"697c3816a67238fac88cc1c8","name":"Jason Weston","hidden":false},{"_id":"697c3816a67238fac88cc1c9","name":"Olga Golovneva","hidden":false}],"publishedAt":"2026-01-29T07:09:30.000Z","submittedOnDailyAt":"2026-01-30T02:18:39.156Z","title":"Self-Improving Pretraining: using post-trained models to pretrain better models","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.","upvotes":4,"discussionId":"697c3817a67238fac88cc1ca","ai_summary":"A reinforcement learning-based pretraining method improves language model safety, factuality, and quality by evaluating generations through a combination of model rollouts, original suffixes, and rewritten suffixes.","ai_keywords":["reinforcement learning","pretraining","next K generated tokens","model rollouts","safety","factuality","language models"],"organization":{"_id":"66b54027408752ae16404b05","name":"metaresearch","fullname":"Meta Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"}},"publishedAt":"2026-01-29T02:09:30.000Z","title":"Self-Improving Pretraining: using post-trained models to pretrain better models","summary":"Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21343.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":221,"isUserFollowing":false},"organization":{"_id":"66b54027408752ae16404b05","name":"metaresearch","fullname":"Meta Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.22101","authors":[{"_id":"697c69e5a67238fac88cc270","user":{"_id":"6526b8ebba9a8279c139616b","avatarUrl":"/avatars/09f6b677603a03be128996a0765233e6.svg","isPro":false,"fullname":"Mahdi Nikdan","user":"mnikdan97","type":"user"},"name":"Mahdi Nikdan","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:32.261Z","hidden":false},{"_id":"697c69e5a67238fac88cc271","name":"Amir Zandieh","hidden":false},{"_id":"697c69e5a67238fac88cc272","name":"Dan Alistarh","hidden":false},{"_id":"697c69e5a67238fac88cc273","name":"Vahab Mirrokni","hidden":false}],"publishedAt":"2026-01-29T18:35:01.000Z","submittedOnDailyAt":"2026-01-30T06:34:59.557Z","title":"ECO: Quantized Training without Full-Precision Master Weights","submittedOnDailyBy":{"_id":"6526b8ebba9a8279c139616b","avatarUrl":"/avatars/09f6b677603a03be128996a0765233e6.svg","isPro":false,"fullname":"Mahdi Nikdan","user":"mnikdan97","type":"user"},"summary":"Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.","upvotes":3,"discussionId":"697c69e5a67238fac88cc274","ai_summary":"Error-compensating optimizer eliminates memory overhead from master weights in quantized LLM training while maintaining near-lossless accuracy.","ai_keywords":["quantization","Large Language Models","Sparse Mixture of Experts","master weights","gradient updates","error-compensating optimizer","error-feedback loop","convergence","Pareto frontier","FP8 quantization","INT4 precision"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-01-29T13:35:01.000Z","title":"ECO: Quantized Training without Full-Precision Master Weights","summary":"Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22101.png","numComments":2,"submittedBy":{"_id":"6526b8ebba9a8279c139616b","avatarUrl":"/avatars/09f6b677603a03be128996a0765233e6.svg","fullname":"Mahdi Nikdan","name":"mnikdan97","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.22054","authors":[{"_id":"697c3fd1a67238fac88cc1d6","name":"Baorui Ma","hidden":false},{"_id":"697c3fd1a67238fac88cc1d7","user":{"_id":"67053e91bacadba833e4ffff","avatarUrl":"/avatars/365025ea4330a2e46a2c47f65c3637c4.svg","isPro":false,"fullname":"jhyang","user":"yjh001","type":"user"},"name":"Jiahui Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:35:33.255Z","hidden":false},{"_id":"697c3fd1a67238fac88cc1d8","name":"Donglin Di","hidden":false},{"_id":"697c3fd1a67238fac88cc1d9","name":"Xuancheng Zhang","hidden":false},{"_id":"697c3fd1a67238fac88cc1da","name":"Jianxun Cui","hidden":false},{"_id":"697c3fd1a67238fac88cc1db","name":"Hao Li","hidden":false},{"_id":"697c3fd1a67238fac88cc1dc","name":"Yan Xie","hidden":false},{"_id":"697c3fd1a67238fac88cc1dd","name":"Wei Chen","hidden":false}],"publishedAt":"2026-01-29T17:52:41.000Z","submittedOnDailyAt":"2026-01-30T04:36:48.680Z","title":"MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources","submittedOnDailyBy":{"_id":"642e9e28ccdcf5da7f978c3e","avatarUrl":"/avatars/fb30b4ea96d5a9ad743d24dae090b7a0.svg","isPro":false,"fullname":"m","user":"bruiiii","type":"user"},"summary":"Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.","upvotes":3,"discussionId":"697c3fd2a67238fac88cc1de","projectPage":"https://metric-anything.github.io/metric-anything-io/","githubRepo":"https://github.com/metric-anything/metric-anything","githubRepoAddedBy":"user","ai_summary":"Metric Anything presents a scalable pretraining framework for metric depth estimation that leverages diverse 3D data and sparse metric prompts to achieve superior performance across multiple vision tasks.","ai_keywords":["vision foundation models","metric depth estimation","sparse metric prompt","pretraining framework","depth completion","super-resolution","Radar-camera fusion","monocular depth estimation","camera intrinsics recovery","3D reconstruction","VLA planning","visual encoder","Multimodal Large Language Model","spatial intelligence"],"githubStars":41},"publishedAt":"2026-01-29T12:52:41.000Z","title":"MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources","summary":"Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22054.png","numComments":2,"submittedBy":{"_id":"642e9e28ccdcf5da7f978c3e","avatarUrl":"/avatars/fb30b4ea96d5a9ad743d24dae090b7a0.svg","fullname":"m","name":"bruiiii","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.21996","authors":[{"_id":"697c7cc4a67238fac88cc2b7","name":"Jianhui Chen","hidden":false},{"_id":"697c7cc4a67238fac88cc2b8","name":"Yuzhang Luo","hidden":false},{"_id":"697c7cc4a67238fac88cc2b9","name":"Liangming Pan","hidden":false}],"publishedAt":"2026-01-29T17:06:54.000Z","submittedOnDailyAt":"2026-01-30T07:14:58.536Z","title":"Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units","submittedOnDailyBy":{"_id":"639c215c5b8c217e25ddda0b","avatarUrl":"/avatars/87b4847329309dca1a8eaec1f1a49618.svg","isPro":false,"fullname":"Jianhui Chen","user":"JianhuiChen","type":"user"},"summary":"While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.","upvotes":3,"discussionId":"697c7cc5a67238fac88cc2ba","githubRepo":"https://github.com/chenjianhuii/Mechanistic-Data-Attribution","githubRepoAddedBy":"user","ai_summary":"Mechnistic Data Attribution framework traces interpretable units to specific training samples using influence functions, demonstrating causal relationships between data structure and neural circuit formation in language models.","ai_keywords":["Mechanistic Interpretability","Influence Functions","interpretable units","training samples","Pythia family","induction head","in-context learning","mechanistic catalyst","data augmentation pipeline","circuit convergence"],"githubStars":1,"organization":{"_id":"61c2e4b131692679706c0716","name":"PKU","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61c2e44c39245e7bf62def6f/bGOsSh93qDIlsl2XWsEi2.png"}},"publishedAt":"2026-01-29T12:06:54.000Z","title":"Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units","summary":"While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21996.png","numComments":3,"submittedBy":{"_id":"639c215c5b8c217e25ddda0b","avatarUrl":"/avatars/87b4847329309dca1a8eaec1f1a49618.svg","fullname":"Jianhui Chen","name":"JianhuiChen","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"61c2e4b131692679706c0716","name":"PKU","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61c2e44c39245e7bf62def6f/bGOsSh93qDIlsl2XWsEi2.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21406","authors":[{"_id":"697c2082a67238fac88cc0c5","name":"Zihan Su","hidden":false},{"_id":"697c2082a67238fac88cc0c6","name":"Hongyang Wei","hidden":false},{"_id":"697c2082a67238fac88cc0c7","name":"Kangrui Cen","hidden":false},{"_id":"697c2082a67238fac88cc0c8","name":"Yong Wang","hidden":false},{"_id":"697c2082a67238fac88cc0c9","name":"Guanhua Chen","hidden":false},{"_id":"697c2082a67238fac88cc0ca","name":"Chun Yuan","hidden":false},{"_id":"697c2082a67238fac88cc0cb","name":"Xiangxiang Chu","hidden":false}],"publishedAt":"2026-01-29T08:42:25.000Z","submittedOnDailyAt":"2026-01-30T00:39:16.691Z","title":"Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation","submittedOnDailyBy":{"_id":"648dca31385b84261811505d","avatarUrl":"/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg","isPro":false,"fullname":"Zihan Su","user":"Sugewud","type":"user"},"summary":"Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.","upvotes":3,"discussionId":"697c2082a67238fac88cc0cc","projectPage":"https://sugewud.github.io/UniMRG-Project/","githubRepo":"https://github.com/Sugewud/UniMRG","githubRepoAddedBy":"user","ai_summary":"UniMRG enhances unified multimodal models by training them to generate multiple visual representations, improving both understanding and generation capabilities through complementary information capture.","ai_keywords":["Unified Multimodal Models","post-training methods","visual understanding","visual generation","auxiliary generation tasks","intrinsic representations","pixel reconstruction","depth estimation","segmentation","fine-grained perception","hallucination reduction","spatial understanding"],"githubStars":6},"publishedAt":"2026-01-29T03:42:25.000Z","title":"Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation","summary":"Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21406.png","numComments":3,"submittedBy":{"_id":"648dca31385b84261811505d","avatarUrl":"/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg","fullname":"Zihan Su","name":"Sugewud","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.20465","authors":[{"_id":"697ac025df3e800774f13bb7","user":{"_id":"6350c89759bfa9a85d434138","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666238674117-6350c89759bfa9a85d434138.jpeg","isPro":false,"fullname":"Yang Lee","user":"innovation64","type":"user"},"name":"Yang Li","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:37:18.269Z","hidden":false},{"_id":"697ac025df3e800774f13bb8","name":"Jiaxiang Liu","hidden":false},{"_id":"697ac025df3e800774f13bb9","name":"Yusong Wang","hidden":false},{"_id":"697ac025df3e800774f13bba","name":"Yujie Wu","hidden":false},{"_id":"697ac025df3e800774f13bbb","name":"Mingkun Xu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6350c89759bfa9a85d434138/eeW4yBhh0061BS4OgjrYq.png"],"publishedAt":"2026-01-28T10:36:03.000Z","submittedOnDailyAt":"2026-01-30T05:07:16.630Z","title":"BMAM: Brain-inspired Multi-Agent Memory Framework","submittedOnDailyBy":{"_id":"6350c89759bfa9a85d434138","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666238674117-6350c89759bfa9a85d434138.jpeg","isPro":false,"fullname":"Yang Lee","user":"innovation64","type":"user"},"summary":"Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.","upvotes":3,"discussionId":"697ac025df3e800774f13bbc","githubRepo":"https://github.com/innovation64/BMAM","githubRepoAddedBy":"user","ai_summary":"BMAM presents a brain-inspired multi-agent memory architecture that decomposes memory into specialized subsystems to address long-term reasoning challenges in language-model-based agents.","ai_keywords":["language-model-based agents","extended interaction horizons","temporally grounded information","behavioral consistency","soul erosion","Brain-inspired Multi-Agent Memory","memory architecture","episodic memory","semantic memory","salience-aware memory","control-oriented memory","long-horizon reasoning","LoCoMo benchmark","hippocampus-inspired episodic memory"],"githubStars":0},"publishedAt":"2026-01-28T05:36:03.000Z","title":"BMAM: Brain-inspired Multi-Agent Memory Framework","summary":"Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6350c89759bfa9a85d434138/eeW4yBhh0061BS4OgjrYq.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20465.png","numComments":1,"submittedBy":{"_id":"6350c89759bfa9a85d434138","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666238674117-6350c89759bfa9a85d434138.jpeg","fullname":"Yang Lee","name":"innovation64","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":20,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.22143","authors":[{"_id":"697cb1756676f933227060e8","name":"Anthony Chen","hidden":false},{"_id":"697cb1756676f933227060e9","name":"Naomi Ken Korem","hidden":false},{"_id":"697cb1756676f933227060ea","name":"Tavi Halperin","hidden":false},{"_id":"697cb1756676f933227060eb","name":"Matan Ben Yosef","hidden":false},{"_id":"697cb1756676f933227060ec","name":"Urska Jelercic","hidden":false},{"_id":"697cb1756676f933227060ed","name":"Ofir Bibi","hidden":false},{"_id":"697cb1756676f933227060ee","name":"Or Patashnik","hidden":false},{"_id":"697cb1756676f933227060ef","name":"Daniel Cohen-Or","hidden":false}],"publishedAt":"2026-01-29T18:57:13.000Z","submittedOnDailyAt":"2026-01-30T10:57:43.837Z","title":"JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion","submittedOnDailyBy":{"_id":"637745113a63a2983ffbde13","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg","isPro":false,"fullname":"Haofan Wang","user":"wanghaofan","type":"user"},"summary":"Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.","upvotes":2,"discussionId":"697cb1756676f933227060f0","ai_summary":"A lightweight LoRA adaptation of an audio-video diffusion model enables high-quality video dubbing with preserved speaker identity and improved lip synchronization through synthetic multilingual video training.","ai_keywords":["audio-video diffusion model","LoRA","video-to-video dubbing","generative model","multilingual videos","lip synchronization","speaker identity","audio-visual foundation models"]},"publishedAt":"2026-01-29T13:57:13.000Z","title":"JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion","summary":"Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22143.png","numComments":2,"submittedBy":{"_id":"637745113a63a2983ffbde13","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg","fullname":"Haofan Wang","name":"wanghaofan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":95,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.19001","authors":[{"_id":"697c6d70a67238fac88cc28e","name":"Haozheng Luo","hidden":false},{"_id":"697c6d70a67238fac88cc28f","name":"Zhuolin Jiang","hidden":false},{"_id":"697c6d70a67238fac88cc290","name":"Md Zahid Hasan","hidden":false},{"_id":"697c6d70a67238fac88cc291","name":"Yan Chen","hidden":false},{"_id":"697c6d70a67238fac88cc292","name":"Soumalya Sarkar","hidden":false}],"publishedAt":"2026-01-26T22:23:09.000Z","submittedOnDailyAt":"2026-01-30T06:07:02.787Z","title":"FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning","submittedOnDailyBy":{"_id":"63ef0af2bfe4ead22ca8f69a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676610243576-noauth.jpeg","isPro":false,"fullname":"Haozheng Luo","user":"robinzixuan","type":"user"},"summary":"We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST","upvotes":2,"discussionId":"697c6d70a67238fac88cc293","ai_summary":"FROST is an attention-aware method that improves reasoning efficiency by pruning uncritical paths and removing reasoning outliers, leading to reduced token usage and improved accuracy.","ai_keywords":["attention weights","reasoning paths","attention-based mechanism","reasoning outliers","attention outlier metrics","token usage","kurtosis","infinity norm"],"organization":{"_id":"680e1986c980bb2d024cdfaa","name":"northwestern-university","fullname":"Northwestern University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/680e15aacadcd7ce86cac9cf/0qOIptd897seA531DA6k2.png"}},"publishedAt":"2026-01-26T17:23:09.000Z","title":"FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning","summary":"We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19001.png","numComments":2,"submittedBy":{"_id":"63ef0af2bfe4ead22ca8f69a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676610243576-noauth.jpeg","fullname":"Haozheng Luo","name":"robinzixuan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"680e1986c980bb2d024cdfaa","name":"northwestern-university","fullname":"Northwestern University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/680e15aacadcd7ce86cac9cf/0qOIptd897seA531DA6k2.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21268","authors":[{"_id":"697d3a2e6676f933227061c7","name":"Micah Rentschler","hidden":false},{"_id":"697d3a2e6676f933227061c8","name":"Jesse Roberts","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66fffe1b3ec4cc293d40f2d5/RhXbEWYVLwkn2b2EjR0j-.png"],"publishedAt":"2026-01-29T05:02:08.000Z","submittedOnDailyAt":"2026-01-30T20:41:59.802Z","title":"Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels","submittedOnDailyBy":{"_id":"66fffe1b3ec4cc293d40f2d5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66fffe1b3ec4cc293d40f2d5/-6Ff7rSQ_fvqBmTQRTlB4.png","isPro":false,"fullname":"Micah Rentschler","user":"micahr234","type":"user"},"summary":"Most reinforcement learning (RL) methods for training large language models (LLMs) require ground-truth labels or task-specific verifiers, limiting scalability when correctness is ambiguous or expensive to obtain. We introduce Reinforcement Learning from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator's answers to natural-language meta-questions (e.g., \"Is the answer correct?\" or \"Is the reasoning logically consistent?\"). RLME treats the evaluator's probability of a positive judgment as a reward and updates the generator via group-relative policy optimization, enabling learning without labels. Across a suite of experiments, we show that RLME achieves accuracy and sample efficiency comparable to label-based training, enables controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns rather than post-hoc rationalization, and generalizes to open-domain settings where ground-truth labels are unavailable, broadening the domains in which LLMs may be trained with RL.","upvotes":1,"discussionId":"697d3a2f6676f933227061c9","ai_summary":"Reinforcement Learning from Meta-Evaluation optimizes language model generators using rewards from evaluators' judgments on natural-language meta-questions, enabling training without ground-truth labels while achieving comparable accuracy and sample efficiency.","ai_keywords":["reinforcement learning","large language models","meta-evaluation","reward modeling","group-relative policy optimization","language model generation","controllable trade-offs","reliable reasoning patterns"]},"publishedAt":"2026-01-29T00:02:08.000Z","title":"Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels","summary":"Most reinforcement learning (RL) methods for training large language models (LLMs) require ground-truth labels or task-specific verifiers, limiting scalability when correctness is ambiguous or expensive to obtain. We introduce Reinforcement Learning from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator's answers to natural-language meta-questions (e.g., \"Is the answer correct?\" or \"Is the reasoning logically consistent?\"). RLME treats the evaluator's probability of a positive judgment as a reward and updates the generator via group-relative policy optimization, enabling learning without labels. Across a suite of experiments, we show that RLME achieves accuracy and sample efficiency comparable to label-based training, enables controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns rather than post-hoc rationalization, and generalizes to open-domain settings where ground-truth labels are unavailable, broadening the domains in which LLMs may be trained with RL.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66fffe1b3ec4cc293d40f2d5/RhXbEWYVLwkn2b2EjR0j-.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21268.png","numComments":1,"submittedBy":{"_id":"66fffe1b3ec4cc293d40f2d5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66fffe1b3ec4cc293d40f2d5/-6Ff7rSQ_fvqBmTQRTlB4.png","fullname":"Micah Rentschler","name":"micahr234","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.20103","authors":[{"_id":"697bb0b8a67238fac88cbfbe","name":"Darshan Deshpande","hidden":false},{"_id":"697bb0b8a67238fac88cbfbf","name":"Anand Kannappan","hidden":false},{"_id":"697bb0b8a67238fac88cbfc0","name":"Rebecca Qian","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/9C-e1GfCIQSbosPZWhx0Y.png","https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/Rwx5_t7EU5Fn0rtFIciJc.png","https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/tp8lkDasc6tta-jzGvmih.png"],"publishedAt":"2026-01-27T22:45:43.000Z","submittedOnDailyAt":"2026-01-30T13:33:17.637Z","title":"Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis","submittedOnDailyBy":{"_id":"60390e04770949ef34f12d9b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1632225571380-60390e04770949ef34f12d9b.jpeg","isPro":false,"fullname":"Darshan Deshpande","user":"DarshanDeshpande","type":"user"},"summary":"Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.","upvotes":1,"discussionId":"697bb0b9a67238fac88cbfc1","ai_summary":"Researchers developed a comprehensive benchmark for detecting reward hacking in code generation environments, demonstrating that contrastive anomaly detection outperforms isolated classification approaches and revealing challenges with semantically contextualized reward hacks.","ai_keywords":["reinforcement learning","code generation","reward hacking","anomaly detection","benchmark","reward exploits","contrastive setting","isolated classification","semantically contextualized","syntactically contextualized"],"organization":{"_id":"64c98cca7fe12ecd0a9f4a3d","name":"PatronusAI","fullname":"Patronus AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/jOAWAvmqOod2rYsgdxF0w.png"}},"publishedAt":"2026-01-27T17:45:43.000Z","title":"Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis","summary":"Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/9C-e1GfCIQSbosPZWhx0Y.png","https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/Rwx5_t7EU5Fn0rtFIciJc.png","https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/tp8lkDasc6tta-jzGvmih.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20103.png","numComments":1,"submittedBy":{"_id":"60390e04770949ef34f12d9b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1632225571380-60390e04770949ef34f12d9b.jpeg","fullname":"Darshan Deshpande","name":"DarshanDeshpande","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"64c98cca7fe12ecd0a9f4a3d","name":"PatronusAI","fullname":"Patronus AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/jOAWAvmqOod2rYsgdxF0w.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.17690","authors":[{"_id":"697c3a85a67238fac88cc1cc","name":"Ziling Gong","hidden":false},{"_id":"697c3a85a67238fac88cc1cd","name":"Yunyan Ouyang","hidden":false},{"_id":"697c3a85a67238fac88cc1ce","name":"Iram Kamdar","hidden":false},{"_id":"697c3a85a67238fac88cc1cf","name":"Melody Ma","hidden":false},{"_id":"697c3a85a67238fac88cc1d0","name":"Hongjie Chen","hidden":false},{"_id":"697c3a85a67238fac88cc1d1","user":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","isPro":false,"fullname":"Franck Dernoncourt","user":"Franck-Dernoncourt","type":"user"},"name":"Franck Dernoncourt","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:35:35.306Z","hidden":false},{"_id":"697c3a85a67238fac88cc1d2","name":"Ryan A. Rossi","hidden":false},{"_id":"697c3a85a67238fac88cc1d3","name":"Nesreen K. Ahmed","hidden":false}],"publishedAt":"2026-01-25T04:32:32.000Z","submittedOnDailyAt":"2026-01-30T02:29:10.088Z","title":"Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance","submittedOnDailyBy":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","isPro":false,"fullname":"Franck Dernoncourt","user":"Franck-Dernoncourt","type":"user"},"summary":"Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.","upvotes":1,"discussionId":"697c3a85a67238fac88cc1d4","ai_summary":"Neural audio fingerprinting performance varies with segment length, with short segments (0.5-second) generally providing better retrieval accuracy, and large language models showing promise in recommending optimal segment durations.","ai_keywords":["audio fingerprinting","neural fingerprinting architecture","retrieval accuracy","segment length","audio segmentation","large language models","GPT-5-mini"]},"publishedAt":"2026-01-24T23:32:32.000Z","title":"Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance","summary":"Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17690.png","numComments":1,"submittedBy":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","fullname":"Franck Dernoncourt","name":"Franck-Dernoncourt","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.11747","authors":[{"_id":"69759e995d41524304c133ca","name":"Huaxiaoyue Wang","hidden":false},{"_id":"69759e995d41524304c133cb","name":"Sunav Choudhary","hidden":false},{"_id":"69759e995d41524304c133cc","user":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","isPro":false,"fullname":"Franck Dernoncourt","user":"Franck-Dernoncourt","type":"user"},"name":"Franck Dernoncourt","status":"claimed_verified","statusLastChangedAt":"2026-01-26T08:31:50.250Z","hidden":false},{"_id":"69759e995d41524304c133cd","name":"Yu Shen","hidden":false},{"_id":"69759e995d41524304c133ce","name":"Stefano Petrangeli","hidden":false}],"publishedAt":"2026-01-16T19:56:13.000Z","submittedOnDailyAt":"2026-01-30T02:25:03.785Z","title":"PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement","submittedOnDailyBy":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","isPro":false,"fullname":"Franck Dernoncourt","user":"Franck-Dernoncourt","type":"user"},"summary":"Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.","upvotes":1,"discussionId":"69759e995d41524304c133cf","ai_summary":"PRISM leverages design data to create a knowledge base for improving graphic designs based on natural language instructions, achieving superior style alignment compared to existing methods.","ai_keywords":["visual language models","design knowledge base","stylistic modification","design clustering","design summarization","style alignment"]},"publishedAt":"2026-01-16T14:56:13.000Z","title":"PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement","summary":"Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11747.png","numComments":1,"submittedBy":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","fullname":"Franck Dernoncourt","name":"Franck-Dernoncourt","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.21872","authors":[{"_id":"697c2464a67238fac88cc0ec","name":"Yao Zhang","hidden":false},{"_id":"697c2464a67238fac88cc0ed","name":"Shijie Tang","hidden":false},{"_id":"697c2464a67238fac88cc0ee","name":"Zeyu Li","hidden":false},{"_id":"697c2464a67238fac88cc0ef","name":"Zhen Han","hidden":false},{"_id":"697c2464a67238fac88cc0f0","name":"Volker Tresp","hidden":false}],"publishedAt":"2026-01-29T15:39:50.000Z","submittedOnDailyAt":"2026-01-30T00:55:28.937Z","title":"WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents","submittedOnDailyBy":{"_id":"62cecf9c1415317d1fbf6cfe","avatarUrl":"/avatars/d30630ad96bcec1349728ba39476847a.svg","isPro":false,"fullname":"Yao Zhang","user":"ZYao720","type":"user"},"summary":"Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.","upvotes":0,"discussionId":"697c2464a67238fac88cc0f1","ai_summary":"WebArbiter introduces a reasoning-first WebPRM that formulates reward modeling as text generation to improve web navigation through structured justifications and preference verdicts, outperforming existing baselines in complex web environments.","ai_keywords":["WebPRMs","text generation","preference verdicts","reasoning distillation","reinforcement learning","reward modeling","web navigation","structured justifications","trajectory search","WebPRMBench","WebArena-Lite"],"organization":{"_id":"62e50495ae9d3f10acb6a9ca","name":"LMU","fullname":"Ludwig Maximilian University of Munich","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1659176121442-5fcaabed246881afd5b00167.png"}},"publishedAt":"2026-01-29T10:39:50.000Z","title":"WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents","summary":"Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21872.png","numComments":1,"submittedBy":{"_id":"62cecf9c1415317d1fbf6cfe","avatarUrl":"/avatars/d30630ad96bcec1349728ba39476847a.svg","fullname":"Yao Zhang","name":"ZYao720","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"62e50495ae9d3f10acb6a9ca","name":"LMU","fullname":"Ludwig Maximilian University of Munich","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1659176121442-5fcaabed246881afd5b00167.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21416","authors":[{"_id":"697c5ab8a67238fac88cc237","user":{"_id":"63ba99e3d90985e7acd820d8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg","isPro":false,"fullname":"Alexandre Chapin","user":"Beegbrain","type":"user"},"name":"Alexandre Chapin","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:35:19.240Z","hidden":false},{"_id":"697c5ab8a67238fac88cc238","name":"Bruno Machado","hidden":false},{"_id":"697c5ab8a67238fac88cc239","name":"Emmanuel Dellandra","hidden":false},{"_id":"697c5ab8a67238fac88cc23a","name":"Liming Chen","hidden":false}],"publishedAt":"2026-01-29T08:55:53.000Z","submittedOnDailyAt":"2026-01-30T04:46:57.770Z","title":"Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation","submittedOnDailyBy":{"_id":"63ba99e3d90985e7acd820d8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg","isPro":false,"fullname":"Alexandre Chapin","user":"Beegbrain","type":"user"},"summary":"The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.","upvotes":0,"discussionId":"697c5ab9a67238fac88cc23b","ai_summary":"Slot-Based Object-Centric Representations outperform global and dense feature representations in robotic manipulation tasks by providing better generalization under visual distribution shifts.","ai_keywords":["visual representations","global features","dense features","object-centric representations","slot-based representations","robotic manipulation policies","distribution shifts","generalization capabilities"]},"publishedAt":"2026-01-29T03:55:53.000Z","title":"Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation","summary":"The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21416.png","numComments":1,"submittedBy":{"_id":"63ba99e3d90985e7acd820d8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg","fullname":"Alexandre Chapin","name":"Beegbrain","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":31,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.21282","authors":[{"_id":"697c2d25a67238fac88cc14d","name":"Rishi Upadhyay","hidden":false},{"_id":"697c2d25a67238fac88cc14e","name":"Howard Zhang","hidden":false},{"_id":"697c2d25a67238fac88cc14f","name":"Jim Solomon","hidden":false},{"_id":"697c2d25a67238fac88cc150","name":"Ayush Agrawal","hidden":false},{"_id":"697c2d25a67238fac88cc151","name":"Pranay Boreddy","hidden":false},{"_id":"697c2d25a67238fac88cc152","name":"Shruti Satya Narayana","hidden":false},{"_id":"697c2d25a67238fac88cc153","name":"Yunhao Ba","hidden":false},{"_id":"697c2d25a67238fac88cc154","name":"Alex Wong","hidden":false},{"_id":"697c2d25a67238fac88cc155","name":"Celso M de Melo","hidden":false},{"_id":"697c2d25a67238fac88cc156","name":"Achuta Kadambi","hidden":false}],"publishedAt":"2026-01-29T05:31:02.000Z","submittedOnDailyAt":"2026-01-30T01:31:44.656Z","title":"WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Recent advances in generative foundational models, often termed \"world models,\" have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.","upvotes":0,"discussionId":"697c2d25a67238fac88cc157","projectPage":"https://world-bench.github.io/","ai_summary":"WorldBench is introduced as a video-based benchmark for disentangled evaluation of physical reasoning in generative models, revealing specific failure patterns in current state-of-the-art video world models.","ai_keywords":["world models","video generation","physical reasoning","concept-specific evaluation","disentangled evaluation","intuitive physical understanding","low-level physical constants","material properties","physical fidelity","video-based benchmarks"]},"publishedAt":"2026-01-29T00:31:02.000Z","title":"WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models","summary":"Recent advances in generative foundational models, often termed \"world models,\" have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21282.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":221,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.20381","authors":[{"_id":"697c59c7a67238fac88cc232","user":{"_id":"63ba99e3d90985e7acd820d8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg","isPro":false,"fullname":"Alexandre Chapin","user":"Beegbrain","type":"user"},"name":"Alexandre Chapin","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:35:21.336Z","hidden":false},{"_id":"697c59c7a67238fac88cc233","name":"Emmanuel Dellandra","hidden":false},{"_id":"697c59c7a67238fac88cc234","name":"Liming Chen","hidden":false}],"publishedAt":"2026-01-28T08:46:04.000Z","submittedOnDailyAt":"2026-01-30T04:45:32.466Z","title":"STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation","submittedOnDailyBy":{"_id":"63ba99e3d90985e7acd820d8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg","isPro":false,"fullname":"Alexandre Chapin","user":"Beegbrain","type":"user"},"summary":"Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.","upvotes":0,"discussionId":"697c59c7a67238fac88cc235","ai_summary":"STORM enhances robotic manipulation by adapting visual foundation models with semantic-aware slots through multi-phase training, improving generalization and control performance.","ai_keywords":["visual foundation models","object-centric representation","multi-phase training","visual-semantic pretraining","language embeddings","downstream manipulation policy","slot-based representation","task-aware objects","robotic manipulation","semantic consistency"]},"publishedAt":"2026-01-28T03:46:04.000Z","title":"STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation","summary":"Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20381.png","numComments":1,"submittedBy":{"_id":"63ba99e3d90985e7acd820d8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg","fullname":"Alexandre Chapin","name":"Beegbrain","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":31,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.18005","authors":[{"_id":"697d0f3f6676f933227061a4","name":"Gergely Brczi","hidden":false},{"_id":"697d0f3f6676f933227061a5","name":"Baran Hashemi","hidden":false},{"_id":"697d0f3f6676f933227061a6","name":"Jonas Klver","hidden":false}],"publishedAt":"2026-01-25T21:41:47.000Z","submittedOnDailyAt":"2026-01-30T17:41:20.816Z","title":"Flow-based Extremal Mathematical Structure Discovery","submittedOnDailyBy":{"_id":"67ee9e42c4ff6510f47b8c29","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Sl0EyXleUwjSFHLa1qsve.png","isPro":false,"fullname":"Baran Hashemi","user":"Baran47","type":"user"},"summary":"The discovery of extremal structures in mathematics requires navigating vast and nonconvex landscapes where analytical methods offer little guidance and brute-force search becomes intractable. We introduce FlowBoost, a closed-loop generative framework that learns to discover rare and extremal geometric structures by combining three components: (i) a geometry-aware conditional flow-matching model that learns to sample high-quality configurations, (ii) reward-guided policy optimization with action exploration that directly optimizes the generation process toward the objective while maintaining diversity, and (iii) stochastic local search for both training-data generation and final refinement. Unlike prior open-loop approaches, such as PatternBoost that retrains on filtered discrete samples, or AlphaEvolve which relies on frozen Large Language Models (LLMs) as evolutionary mutation operators, FlowBoost enforces geometric feasibility during sampling, and propagates reward signal directly into the generative model, closing the optimization loop and requiring much smaller training sets and shorter training times, and reducing the required outer-loop iterations by orders of magnitude, while eliminating dependence on LLMs. We demonstrate the framework on four geometric optimization problems: sphere packing in hypercubes, circle packing maximizing sum of radii, the Heilbronn triangle problem, and star discrepancy minimization. In several cases, FlowBoost discovers configurations that match or exceed the best known results. For circle packings, we improve the best known lower bounds, surpassing the LLM-based system AlphaEvolve while using substantially fewer computational resources.","upvotes":0,"discussionId":"697d0f3f6676f933227061a7","ai_summary":"FlowBoost is a closed-loop generative framework that combines geometry-aware flow-matching, reward-guided policy optimization, and stochastic local search to efficiently discover extremal geometric structures with improved results over existing methods.","ai_keywords":["conditional flow-matching model","reward-guided policy optimization","stochastic local search","geometry-aware","closed-loop generative framework","extremal geometric structures","flow-matching","policy optimization"],"organization":{"_id":"657d72bc42fc53e18b26523c","name":"ScaDS-AI","fullname":"Center for Scalable Data Analytics and Artificial Intelligence Dresden/Leipzig","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/Vcgs9xswvJ3Ni9bPdKhRr.png"}},"publishedAt":"2026-01-25T16:41:47.000Z","title":"Flow-based Extremal Mathematical Structure Discovery","summary":"The discovery of extremal structures in mathematics requires navigating vast and nonconvex landscapes where analytical methods offer little guidance and brute-force search becomes intractable. We introduce FlowBoost, a closed-loop generative framework that learns to discover rare and extremal geometric structures by combining three components: (i) a geometry-aware conditional flow-matching model that learns to sample high-quality configurations, (ii) reward-guided policy optimization with action exploration that directly optimizes the generation process toward the objective while maintaining diversity, and (iii) stochastic local search for both training-data generation and final refinement. Unlike prior open-loop approaches, such as PatternBoost that retrains on filtered discrete samples, or AlphaEvolve which relies on frozen Large Language Models (LLMs) as evolutionary mutation operators, FlowBoost enforces geometric feasibility during sampling, and propagates reward signal directly into the generative model, closing the optimization loop and requiring much smaller training sets and shorter training times, and reducing the required outer-loop iterations by orders of magnitude, while eliminating dependence on LLMs. We demonstrate the framework on four geometric optimization problems: sphere packing in hypercubes, circle packing maximizing sum of radii, the Heilbronn triangle problem, and star discrepancy minimization. In several cases, FlowBoost discovers configurations that match or exceed the best known results. For circle packings, we improve the best known lower bounds, surpassing the LLM-based system AlphaEvolve while using substantially fewer computational resources.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18005.png","numComments":1,"submittedBy":{"_id":"67ee9e42c4ff6510f47b8c29","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Sl0EyXleUwjSFHLa1qsve.png","fullname":"Baran Hashemi","name":"Baran47","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"657d72bc42fc53e18b26523c","name":"ScaDS-AI","fullname":"Center for Scalable Data Analytics and Artificial Intelligence Dresden/Leipzig","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/Vcgs9xswvJ3Ni9bPdKhRr.png"},"isAuthorParticipating":false}]
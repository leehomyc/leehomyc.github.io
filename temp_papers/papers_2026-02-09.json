[{"paper":{"id":"2602.06717","authors":[{"_id":"69898989beecc443208d2741","name":"Daniil Plyusov","hidden":false},{"_id":"69898989beecc443208d2742","user":{"_id":"62897fce5d9e25c10e4f319d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg","isPro":false,"fullname":"Alexey Gorbatovski","user":"Myashka","type":"user"},"name":"Alexey Gorbatovski","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:27:53.815Z","hidden":false},{"_id":"69898989beecc443208d2743","name":"Boris Shaposhnikov","hidden":false},{"_id":"69898989beecc443208d2744","name":"Viacheslav Sinii","hidden":false},{"_id":"69898989beecc443208d2745","user":{"_id":"636e71b2b0ebc04888157b71","avatarUrl":"/avatars/957ba705d470e3a01792741d7f0ff038.svg","isPro":false,"fullname":"Alexey Malakhov","user":"ZeL1k7","type":"user"},"name":"Alexey Malakhov","status":"claimed_verified","statusLastChangedAt":"2026-02-09T21:06:45.653Z","hidden":false},{"_id":"69898989beecc443208d2746","user":{"_id":"62a9c8edc19f92ae443ab37f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62a9c8edc19f92ae443ab37f/yczqpBOntLco_2Jn4hnT7.jpeg","isPro":false,"fullname":"Daniil Gavrilov","user":"kefirski","type":"user"},"name":"Daniil Gavrilov","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:27:57.059Z","hidden":false}],"publishedAt":"2026-02-06T14:07:30.000Z","submittedOnDailyAt":"2026-02-09T04:48:51.744Z","title":"F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare","submittedOnDailyBy":{"_id":"62897fce5d9e25c10e4f319d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg","isPro":false,"fullname":"Alexey Gorbatovski","user":"Myashka","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.","upvotes":54,"discussionId":"69898989beecc443208d2747","ai_summary":"RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.","ai_keywords":["reinforcement learning","verifiable rewards","group sampling","advantage estimation","policy updates","Focal loss","GRPO","DAPO","CISPO","pass@k metrics"],"organization":{"_id":"675861e944dbb69c2673c71c","name":"t-tech","fullname":"T-Tech","avatar":"https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}},"publishedAt":"2026-02-06T09:07:30.000Z","title":"F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06717.png","numComments":1,"submittedBy":{"_id":"62897fce5d9e25c10e4f319d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg","fullname":"Alexey Gorbatovski","name":"Myashka","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"675861e944dbb69c2673c71c","name":"t-tech","fullname":"T-Tech","avatar":"https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.06570","authors":[{"_id":"69895518beecc443208d2680","name":"Baichuan-M3 Team","hidden":false},{"_id":"69895518beecc443208d2682","name":"Chengfeng Dou","hidden":false},{"_id":"69895518beecc443208d2683","user":{"_id":"641c45c921964f8f6d451d16","avatarUrl":"/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg","isPro":false,"fullname":"FanYang","user":"fairyang","type":"user"},"name":"Fan Yang","status":"claimed_verified","statusLastChangedAt":"2026-02-09T14:32:20.075Z","hidden":false},{"_id":"69895518beecc443208d2684","user":{"_id":"6464dd5234acce85aea186c7","avatarUrl":"/avatars/3428029b0ae5f12885092c7aea588065.svg","isPro":false,"fullname":"lifei","user":"lifei926926","type":"user"},"name":"Fei Li","status":"claimed_verified","statusLastChangedAt":"2026-02-09T21:07:14.055Z","hidden":false},{"_id":"69895518beecc443208d2685","name":"Jiyuan Jia","hidden":false},{"_id":"69895518beecc443208d2686","name":"Qiang Ju","hidden":false},{"_id":"69895518beecc443208d2687","name":"Shuai Wang","hidden":false},{"_id":"69895518beecc443208d2688","name":"Tianpeng Li","hidden":false},{"_id":"69895518beecc443208d2689","name":"Xiangrong Zeng","hidden":false},{"_id":"69895518beecc443208d268a","name":"Yijie Zhou","hidden":false},{"_id":"69895518beecc443208d268b","name":"Hongda Zhang","hidden":false},{"_id":"69895518beecc443208d268c","name":"Jinyang Tai","hidden":false},{"_id":"69895518beecc443208d268d","name":"Linzhuang Sun","hidden":false},{"_id":"69895518beecc443208d268e","user":{"_id":"6487e2e1eec01aee99cf4c10","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6487e2e1eec01aee99cf4c10/1U6zZ2OaUOrR1ueD5yraR.jpeg","isPro":false,"fullname":"Peidong Guo","user":"GuoPD","type":"user"},"name":"Peidong Guo","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:36:44.605Z","hidden":false},{"_id":"69895518beecc443208d268f","name":"Yichuan Mo","hidden":false},{"_id":"69895518beecc443208d2690","name":"Xiaochuan Wang","hidden":false},{"_id":"69895518beecc443208d2691","name":"Hengfu Cui","hidden":false},{"_id":"69895518beecc443208d2692","name":"Zhishou Zhang","hidden":false}],"publishedAt":"2026-02-06T10:08:59.000Z","submittedOnDailyAt":"2026-02-09T03:00:35.501Z","title":"Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making","submittedOnDailyBy":{"_id":"641c45c921964f8f6d451d16","avatarUrl":"/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg","isPro":false,"fullname":"FanYang","user":"fairyang","type":"user"},"summary":"We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.","upvotes":54,"discussionId":"69895518beecc443208d2693","githubRepo":"https://github.com/baichuan-inc/Baichuan-M3-235B","githubRepoAddedBy":"user","ai_summary":"Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.","ai_keywords":["large language model","clinical decision support","proactive information acquisition","long-horizon reasoning","hallucination suppression","HealthBench","HealthBench-Hallu","ScanBench"],"githubStars":190,"organization":{"_id":"648457d38cf0b32b0ba0a913","name":"baichuan-inc","fullname":"Baichuan Intelligent Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}},"publishedAt":"2026-02-06T05:08:59.000Z","title":"Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making","summary":"We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06570.png","numComments":2,"submittedBy":{"_id":"641c45c921964f8f6d451d16","avatarUrl":"/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg","fullname":"FanYang","name":"fairyang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"organization":{"_id":"648457d38cf0b32b0ba0a913","name":"baichuan-inc","fullname":"Baichuan Intelligent Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.05843","authors":[{"_id":"698567834ad556f294b7ec03","user":{"_id":"64e6cf78ecce34cb442dc889","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg","isPro":false,"fullname":"Fangzhi Xu","user":"xufangzhi","type":"user"},"name":"Fangzhi Xu","status":"claimed_verified","statusLastChangedAt":"2026-02-06T18:51:15.682Z","hidden":false},{"_id":"698567834ad556f294b7ec04","name":"Hang Yan","hidden":false},{"_id":"698567834ad556f294b7ec05","user":{"_id":"6064a0eeb1703ddba0d458b9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png","isPro":false,"fullname":"Qiushi","user":"QiushiSun","type":"user"},"name":"Qiushi Sun","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:35:17.132Z","hidden":false},{"_id":"698567834ad556f294b7ec06","user":{"_id":"6747de57f8cab58c22ec94a2","avatarUrl":"/avatars/5bae0341862fac24564781c0fa32aac5.svg","isPro":false,"fullname":"Jinyang Wu","user":"Jinyang23","type":"user"},"name":"Jinyang Wu","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:35:19.089Z","hidden":false},{"_id":"698567834ad556f294b7ec07","name":"Zixian Huang","hidden":false},{"_id":"698567834ad556f294b7ec08","user":{"_id":"6628859f1a5c7e6b445868c1","avatarUrl":"/avatars/a7684d2bd0fd60824c5e810356953243.svg","isPro":false,"fullname":"Muye Huang","user":"MuyeHuang","type":"user"},"name":"Muye Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:35:25.705Z","hidden":false},{"_id":"698567834ad556f294b7ec09","name":"Jingyang Gong","hidden":false},{"_id":"698567834ad556f294b7ec0a","user":{"_id":"642b9861bb77f8456634b048","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg","isPro":false,"fullname":"Zichen Ding","user":"heroding77","type":"user"},"name":"Zichen Ding","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:35:15.144Z","hidden":false},{"_id":"698567834ad556f294b7ec0b","name":"Kanzhi Cheng","hidden":false},{"_id":"698567834ad556f294b7ec0c","name":"Yian Wang","hidden":false},{"_id":"698567834ad556f294b7ec0d","name":"Xinyu Che","hidden":false},{"_id":"698567834ad556f294b7ec0e","name":"Zeyi Sun","hidden":false},{"_id":"698567834ad556f294b7ec0f","user":{"_id":"658be7fe135580745c510323","avatarUrl":"/avatars/830e5cec4565efdc23226a86a0fcef0e.svg","isPro":false,"fullname":"Jian Zhang","user":"VentureZJ","type":"user"},"name":"Jian Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-06T18:51:08.819Z","hidden":false},{"_id":"698567834ad556f294b7ec10","name":"Zhangyue Yin","hidden":false},{"_id":"698567834ad556f294b7ec11","name":"Haoran Luo","hidden":false},{"_id":"698567834ad556f294b7ec12","name":"Xuanjing Huang","hidden":false},{"_id":"698567834ad556f294b7ec13","name":"Ben Kao","hidden":false},{"_id":"698567834ad556f294b7ec14","name":"Jun Liu","hidden":false},{"_id":"698567834ad556f294b7ec15","user":{"_id":"66ac77011cfb12c087605acb","avatarUrl":"/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg","isPro":false,"fullname":"Lin","user":"Qika","type":"user"},"name":"Qika Lin","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:35:23.699Z","hidden":false}],"publishedAt":"2026-02-05T16:31:43.000Z","submittedOnDailyAt":"2026-02-09T03:14:11.152Z","title":"OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions","submittedOnDailyBy":{"_id":"64e6cf78ecce34cb442dc889","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg","isPro":false,"fullname":"Fangzhi Xu","user":"xufangzhi","type":"user"},"summary":"The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena","upvotes":51,"discussionId":"698567834ad556f294b7ec16","projectPage":"https://yayayacc.github.io/Odyssey-Home/","githubRepo":"https://github.com/xufangzhi/Odyssey-Arena","githubRepoAddedBy":"user","ai_summary":"OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.","ai_keywords":["Large Language Models","autonomous agents","inductive reasoning","long-horizon planning","agent evaluation","transition laws","OdysseyArena","OdysseyArena-Lite","OdysseyArena-Challenge"],"githubStars":25},"publishedAt":"2026-02-05T11:31:43.000Z","title":"OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions","summary":"The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05843.png","numComments":2,"submittedBy":{"_id":"64e6cf78ecce34cb442dc889","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg","fullname":"Fangzhi Xu","name":"xufangzhi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":18,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.05027","authors":[{"_id":"69871a872d626112378ad69f","user":{"_id":"660fd34df03515e4ff3f2b64","avatarUrl":"/avatars/0c2a29b1081ece881234acdd8ef9371a.svg","isPro":false,"fullname":"Georgii Aparin","user":"Egorgij21","type":"user"},"name":"Georgii Aparin","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:34:22.635Z","hidden":false},{"_id":"69871a872d626112378ad6a0","name":"Tasnima Sadekova","hidden":false},{"_id":"69871a872d626112378ad6a1","name":"Alexey Rukhovich","hidden":false},{"_id":"69871a872d626112378ad6a2","name":"Assel Yermekova","hidden":false},{"_id":"69871a872d626112378ad6a3","name":"Laida Kushnareva","hidden":false},{"_id":"69871a872d626112378ad6a4","name":"Vadim Popov","hidden":false},{"_id":"69871a872d626112378ad6a5","name":"Kristian Kuznetsov","hidden":false},{"_id":"69871a872d626112378ad6a6","name":"Irina Piontkovskaya","hidden":false}],"publishedAt":"2026-02-04T20:29:16.000Z","submittedOnDailyAt":"2026-02-09T05:28:19.089Z","title":"AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders","submittedOnDailyBy":{"_id":"636254dc2691058b19d9276a","avatarUrl":"/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg","isPro":false,"fullname":"Kushnareva","user":"Kushnareva","type":"user"},"summary":"Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.","upvotes":49,"discussionId":"69871a872d626112378ad6a7","githubRepo":"https://github.com/audiosae/audiosae_demo","githubRepoAddedBy":"user","ai_summary":"Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.","ai_keywords":["Sparse Autoencoders","encoder layers","Whisper","HuBERT","feature steering","false speech detections","WER","EEG activity","speech perception"],"githubStars":7,"organization":{"_id":"5f83c275f0801648bf88454a","name":"huawei-noah","fullname":"HUAWEI Noah's Ark Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}},"publishedAt":"2026-02-04T15:29:16.000Z","title":"AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders","summary":"Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05027.png","numComments":2,"submittedBy":{"_id":"636254dc2691058b19d9276a","avatarUrl":"/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg","fullname":"Kushnareva","name":"Kushnareva","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":12,"isUserFollowing":false},"organization":{"_id":"5f83c275f0801648bf88454a","name":"huawei-noah","fullname":"HUAWEI Noah's Ark Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03392","authors":[{"_id":"69858ae34ad556f294b7ec93","user":{"_id":"652f7bf41ad13fee8c407247","avatarUrl":"/avatars/5c7a74a9edf748025bffeeba97a61505.svg","isPro":false,"fullname":"Shumin","user":"Mystery","type":"user"},"name":"Shumin Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-06T18:50:45.887Z","hidden":false},{"_id":"69858ae34ad556f294b7ec94","name":"Yuexiang Xie","hidden":false},{"_id":"69858ae34ad556f294b7ec95","name":"Wenhao Zhang","hidden":false},{"_id":"69858ae34ad556f294b7ec96","user":{"_id":"6541b3d54f939214d3abbfbc","avatarUrl":"/avatars/37aa9cc51fd98198805456ad04b90023.svg","isPro":false,"fullname":"yuchang","user":"hiyuchang","type":"user"},"name":"Yuchang Sun","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:35:09.981Z","hidden":false},{"_id":"69858ae34ad556f294b7ec97","name":"Yanxi Chen","hidden":false},{"_id":"69858ae34ad556f294b7ec98","name":"Yaliang Li","hidden":false},{"_id":"69858ae34ad556f294b7ec99","name":"Yanyong Zhang","hidden":false}],"publishedAt":"2026-02-03T11:14:58.000Z","submittedOnDailyAt":"2026-02-09T00:31:59.901Z","title":"On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models","submittedOnDailyBy":{"_id":"652f7bf41ad13fee8c407247","avatarUrl":"/avatars/5c7a74a9edf748025bffeeba97a61505.svg","isPro":false,"fullname":"Shumin","user":"Mystery","type":"user"},"summary":"Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.","upvotes":45,"discussionId":"69858ae34ad556f294b7ec9a","githubRepo":"https://github.com/agentscope-ai/Trinity-RFT","githubRepoAddedBy":"user","ai_summary":"The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.","ai_keywords":["entropy","large language models","reinforcement fine-tuning","RFT","logit update","Group Relative Policy Optimization","GRPO","entropy-discriminator clipping","exploration-exploitation balance"],"githubStars":521},"publishedAt":"2026-02-03T06:14:58.000Z","title":"On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models","summary":"Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03392.png","numComments":3,"submittedBy":{"_id":"652f7bf41ad13fee8c407247","avatarUrl":"/avatars/5c7a74a9edf748025bffeeba97a61505.svg","fullname":"Shumin","name":"Mystery","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.01734","authors":[{"_id":"6987609dbeecc443208d2375","name":"Lianhai Ren","hidden":false},{"_id":"6987609dbeecc443208d2376","name":"Yucheng Ding","hidden":false},{"_id":"6987609dbeecc443208d2377","user":{"_id":"63fb6e281b4b1bd4e7ffc5be","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg","isPro":false,"fullname":"Xiao Liu","user":"lx865712528","type":"user"},"name":"Xiao Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:31:02.000Z","hidden":false},{"_id":"6987609dbeecc443208d2378","name":"Qianxiao Li","hidden":false},{"_id":"6987609dbeecc443208d2379","name":"Peng Cheng","hidden":false},{"_id":"6987609dbeecc443208d237a","name":"Yeyun Gong","hidden":false}],"publishedAt":"2026-02-02T07:18:45.000Z","submittedOnDailyAt":"2026-02-09T02:17:53.967Z","title":"MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration","submittedOnDailyBy":{"_id":"63fb6e281b4b1bd4e7ffc5be","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg","isPro":false,"fullname":"Xiao Liu","user":"lx865712528","type":"user"},"summary":"Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via μP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.","upvotes":29,"discussionId":"6987609dbeecc443208d237b","ai_summary":"Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.","ai_keywords":["large language model","pretraining","gradient explosions","weight matrix stable rank","Frobenius norm","spectral norm","Jacobian","matrix sign operations","optimizer"],"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}},"publishedAt":"2026-02-02T02:18:45.000Z","title":"MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration","summary":"Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via μP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01734.png","numComments":2,"submittedBy":{"_id":"63fb6e281b4b1bd4e7ffc5be","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg","fullname":"Xiao Liu","name":"lx865712528","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":12,"isUserFollowing":false},"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.18415","authors":[{"_id":"698061616676f93322706708","user":{"_id":"62b1e0f76a5435fd9a60a8dc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1655824626110-noauth.png","isPro":false,"fullname":"Ivan Bondarenko","user":"bond005","type":"user"},"name":"Ivan Bondarenko","status":"claimed_verified","statusLastChangedAt":"2026-02-02T15:43:40.523Z","hidden":false},{"_id":"698061616676f93322706709","user":{"_id":"63cb976d80ba2ca4151b67a2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1675713278440-63cb976d80ba2ca4151b67a2.jpeg","isPro":false,"fullname":"Daniel Grebenkin","user":"dangrebenkin","type":"user"},"name":"Daniil Grebenkin","status":"claimed_verified","statusLastChangedAt":"2026-02-02T15:43:46.074Z","hidden":false},{"_id":"698061616676f9332270670a","user":{"_id":"61dd9daedb45389905634d3f","avatarUrl":"/avatars/a0482317b6118d37da294b9dc2bf5a39.svg","isPro":false,"fullname":"Oleg Sedukhin","user":"greyzyablik","type":"user"},"name":"Oleg Sedukhin","status":"claimed_verified","statusLastChangedAt":"2026-02-02T15:43:42.792Z","hidden":true},{"_id":"698061616676f9332270670b","user":{"_id":"662282de25b14cbf4b0c3f7b","avatarUrl":"/avatars/78231051b4f29538c83ff9935e54d974.svg","isPro":false,"fullname":"Klementev Mikhail","user":"Klemaaaaa","type":"user"},"name":"Mikhail Klementev","status":"claimed_verified","statusLastChangedAt":"2026-02-02T15:43:38.368Z","hidden":false},{"_id":"698061616676f9332270670c","user":{"_id":"6415cb01486c7c9a5d1560f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg","isPro":false,"fullname":"Roman Derunets","user":"rmndrnts","type":"user"},"name":"Roman Derunets","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:51:53.503Z","hidden":false},{"_id":"698061616676f9332270670d","name":"Lyudmila Budneva","hidden":false}],"publishedAt":"2026-01-26T12:14:51.000Z","submittedOnDailyAt":"2026-02-09T10:55:09.411Z","title":"Pisets: A Robust Speech Recognition System for Lectures and Interviews","submittedOnDailyBy":{"_id":"6415cb01486c7c9a5d1560f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg","isPro":false,"fullname":"Roman Derunets","user":"rmndrnts","type":"user"},"summary":"This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.","upvotes":29,"discussionId":"698061616676f9332270670e","ai_summary":"A three-component speech-to-text system combines Wav2Vec2, AST, and Whisper models with curriculum learning and uncertainty modeling to improve transcription accuracy and reduce hallucinations in Russian speech recognition.","ai_keywords":["Wav2Vec2","Audio Spectrogram Transformer","Whisper","curriculum learning","uncertainty modeling","speech recognition","transcription quality"],"organization":{"_id":"62b1e262f4a72794188b6757","name":"NSU","fullname":"Novosibirsk State University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1655825086683-62b1e0f76a5435fd9a60a8dc.png"}},"publishedAt":"2026-01-26T07:14:51.000Z","title":"Pisets: A Robust Speech Recognition System for Lectures and Interviews","summary":"This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18415.png","numComments":2,"submittedBy":{"_id":"6415cb01486c7c9a5d1560f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg","fullname":"Roman Derunets","name":"rmndrnts","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"62b1e262f4a72794188b6757","name":"NSU","fullname":"Novosibirsk State University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1655825086683-62b1e0f76a5435fd9a60a8dc.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.06949","authors":[{"_id":"69894c74beecc443208d25db","name":"Shenyuan Gao","hidden":false},{"_id":"69894c74beecc443208d25dc","name":"William Liang","hidden":false},{"_id":"69894c74beecc443208d25dd","name":"Kaiyuan Zheng","hidden":false},{"_id":"69894c74beecc443208d25de","name":"Ayaan Malik","hidden":false},{"_id":"69894c74beecc443208d25df","name":"Seonghyeon Ye","hidden":false},{"_id":"69894c74beecc443208d25e0","name":"Sihyun Yu","hidden":false},{"_id":"69894c74beecc443208d25e1","name":"Wei-Cheng Tseng","hidden":false},{"_id":"69894c74beecc443208d25e2","name":"Yuzhu Dong","hidden":false},{"_id":"69894c74beecc443208d25e3","name":"Kaichun Mo","hidden":false},{"_id":"69894c74beecc443208d25e4","name":"Chen-Hsuan Lin","hidden":false},{"_id":"69894c74beecc443208d25e5","name":"Qianli Ma","hidden":false},{"_id":"69894c74beecc443208d25e6","name":"Seungjun Nah","hidden":false},{"_id":"69894c74beecc443208d25e7","name":"Loic Magne","hidden":false},{"_id":"69894c74beecc443208d25e8","name":"Jiannan Xiang","hidden":false},{"_id":"69894c74beecc443208d25e9","name":"Yuqi Xie","hidden":false},{"_id":"69894c74beecc443208d25ea","name":"Ruijie Zheng","hidden":false},{"_id":"69894c74beecc443208d25eb","name":"Dantong Niu","hidden":false},{"_id":"69894c74beecc443208d25ec","name":"You Liang Tan","hidden":false},{"_id":"69894c74beecc443208d25ed","name":"K. R. Zentner","hidden":false},{"_id":"69894c74beecc443208d25ee","name":"George Kurian","hidden":false},{"_id":"69894c74beecc443208d25ef","name":"Suneel Indupuru","hidden":false},{"_id":"69894c74beecc443208d25f0","name":"Pooya Jannaty","hidden":false},{"_id":"69894c74beecc443208d25f1","name":"Jinwei Gu","hidden":false},{"_id":"69894c74beecc443208d25f2","name":"Jun Zhang","hidden":false},{"_id":"69894c74beecc443208d25f3","name":"Jitendra Malik","hidden":false},{"_id":"69894c74beecc443208d25f4","name":"Pieter Abbeel","hidden":false},{"_id":"69894c74beecc443208d25f5","name":"Ming-Yu Liu","hidden":false},{"_id":"69894c74beecc443208d25f6","name":"Yuke Zhu","hidden":false},{"_id":"69894c74beecc443208d25f7","name":"Joel Jang","hidden":false},{"_id":"69894c74beecc443208d25f8","name":"Linxi \"Jim\" Fan","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MN-A84kxkw1l1lyftyRTR.mp4"],"publishedAt":"2026-02-06T18:49:43.000Z","submittedOnDailyAt":"2026-02-09T00:32:34.350Z","title":"DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.","upvotes":21,"discussionId":"69894c74beecc443208d25f9","projectPage":"https://dreamdojo-world.github.io/","ai_summary":"DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.","ai_keywords":["world model","egocentric videos","continuous latent actions","action labels","distillation pipeline","real-time speed","teleoperation","policy evaluation","model-based planning","out-of-distribution benchmarks"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-02-06T13:49:43.000Z","title":"DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos","summary":"Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MN-A84kxkw1l1lyftyRTR.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06949.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":230,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06130","authors":[{"_id":"6989cb0bbeecc443208d2864","name":"Yifu Qiu","hidden":false},{"_id":"6989cb0bbeecc443208d2865","user":{"_id":"64ba8e9d5299e0f164491e45","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ba8e9d5299e0f164491e45/ZK4bUJZuUO8xsj0RZmzMn.jpeg","isPro":false,"fullname":"Zheng Zhao","user":"zsquaredz","type":"user"},"name":"Zheng Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-09T21:06:19.908Z","hidden":false},{"_id":"6989cb0bbeecc443208d2866","name":"Waylon Li","hidden":false},{"_id":"6989cb0bbeecc443208d2867","name":"Yftah Ziser","hidden":false},{"_id":"6989cb0bbeecc443208d2868","name":"Anna Korhonen","hidden":false},{"_id":"6989cb0bbeecc443208d2869","name":"Shay B. Cohen","hidden":false},{"_id":"6989cb0bbeecc443208d286a","name":"Edoardo M. Ponti","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64686434f43574d9556b1fa6/j_R6vn6-FY-h0yKdFj-3X.gif","https://cdn-uploads.huggingface.co/production/uploads/64686434f43574d9556b1fa6/zJRIVeC0mBpuOlMZ19NFx.png"],"publishedAt":"2026-02-05T19:04:41.000Z","submittedOnDailyAt":"2026-02-09T09:28:38.468Z","title":"Self-Improving World Modelling with Latent Actions","submittedOnDailyBy":{"_id":"64686434f43574d9556b1fa6","avatarUrl":"/avatars/64183d643cfc3b274714a6167c354e39.svg","isPro":false,"fullname":"Yifu Qiu","user":"yfqiu-nlp","type":"user"},"summary":"Internal modelling of the world -- predicting transitions between previous states X and next states Y under actions Z -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) P_θ(Y|X,Z) and an Inverse Dynamics Modelling (IDM) Q_φ(Z|X,Y). SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.","upvotes":19,"discussionId":"6989cb0bbeecc443208d286b","ai_summary":"SWIRL is a self-improvement framework that learns world models from state-only sequences by alternating between forward and inverse dynamics modeling with variational information maximization and ELBO maximization, achieving improved performance on various reasoning and planning benchmarks.","ai_keywords":["Forward World Modelling","Inverse Dynamics Modelling","variational information maximisation","ELBO maximisation","reinforcement learning","GRPO","latent variables","conditional mutual information","coordinate ascent","world models","state-only sequences"]},"publishedAt":"2026-02-05T14:04:41.000Z","title":"Self-Improving World Modelling with Latent Actions","summary":"Internal modelling of the world -- predicting transitions between previous states X and next states Y under actions Z -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) P_θ(Y|X,Z) and an Inverse Dynamics Modelling (IDM) Q_φ(Z|X,Y). SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64686434f43574d9556b1fa6/j_R6vn6-FY-h0yKdFj-3X.gif","https://cdn-uploads.huggingface.co/production/uploads/64686434f43574d9556b1fa6/zJRIVeC0mBpuOlMZ19NFx.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06130.png","numComments":1,"submittedBy":{"_id":"64686434f43574d9556b1fa6","avatarUrl":"/avatars/64183d643cfc3b274714a6167c354e39.svg","fullname":"Yifu Qiu","name":"yfqiu-nlp","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.06291","authors":[{"_id":"698964b8beecc443208d26d8","name":"Guijin Son","hidden":false},{"_id":"698964b8beecc443208d26d9","name":"Donghun Yang","hidden":false},{"_id":"698964b8beecc443208d26da","name":"Hitesh Laxmichand Patel","hidden":false},{"_id":"698964b8beecc443208d26db","name":"Hyunwoo Ko","hidden":false},{"_id":"698964b8beecc443208d26dc","name":"Amit Agarwal","hidden":false},{"_id":"698964b8beecc443208d26dd","name":"Sunghee Ahn","hidden":false},{"_id":"698964b8beecc443208d26de","name":"Kyong-Ha Lee","hidden":false},{"_id":"698964b8beecc443208d26df","name":"Youngjae Yu","hidden":false}],"publishedAt":"2026-02-06T01:10:28.000Z","submittedOnDailyAt":"2026-02-09T02:08:27.389Z","title":"Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math","submittedOnDailyBy":{"_id":"60d3e619b8448e1785bbda2a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg","isPro":true,"fullname":"GUIJIN SON","user":"amphora","type":"user"},"summary":"Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.","upvotes":16,"discussionId":"698964b8beecc443208d26e0","ai_summary":"Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.","ai_keywords":["reasoning models","research-level mathematics","verification","oracle-free evaluator","in-context exemplar","reward models","generative reward models","LLM judges","Acc@1","AUC","solver-evaluator gap"]},"publishedAt":"2026-02-05T20:10:28.000Z","title":"Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math","summary":"Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06291.png","numComments":1,"submittedBy":{"_id":"60d3e619b8448e1785bbda2a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg","fullname":"GUIJIN SON","name":"amphora","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":76,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.05940","authors":[{"_id":"6985662b4ad556f294b7ebf8","user":{"_id":"68356f5db243fb809813a715","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68356f5db243fb809813a715/grhHvANfDRp75rMJxWlQo.jpeg","isPro":false,"fullname":"LiuJunxiao","user":"master-lan","type":"user"},"name":"Junxiao Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-06T18:51:11.681Z","hidden":false},{"_id":"6985662b4ad556f294b7ebf9","name":"Zhijun Wang","hidden":false},{"_id":"6985662b4ad556f294b7ebfa","name":"Yixiao Li","hidden":false},{"_id":"6985662b4ad556f294b7ebfb","user":{"_id":"643525ea0b30bd434ea15363","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643525ea0b30bd434ea15363/7sAzllfWUPtt68NY1gDLj.png","isPro":false,"fullname":"Jackie Lai","user":"DreamW1ngs","type":"user"},"name":"Zhejian Lai","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:35:21.194Z","hidden":false},{"_id":"6985662b4ad556f294b7ebfc","name":"Liqian Huang","hidden":false},{"_id":"6985662b4ad556f294b7ebfd","name":"Xin Huang","hidden":false},{"_id":"6985662b4ad556f294b7ebfe","name":"Xue Han","hidden":false},{"_id":"6985662b4ad556f294b7ebff","name":"Junlan Feng","hidden":false},{"_id":"6985662b4ad556f294b7ec00","name":"Shujian Huang","hidden":false}],"publishedAt":"2026-02-05T17:55:09.000Z","submittedOnDailyAt":"2026-02-09T01:14:36.127Z","title":"Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training","submittedOnDailyBy":{"_id":"68356f5db243fb809813a715","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68356f5db243fb809813a715/grhHvANfDRp75rMJxWlQo.jpeg","isPro":false,"fullname":"LiuJunxiao","user":"master-lan","type":"user"},"summary":"Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.","upvotes":16,"discussionId":"6985662b4ad556f294b7ec01","ai_summary":"TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.","ai_keywords":["multilingual reasoning","translation reasoning integrated training","cross-lingual question alignment","COMET","FLORES-200","MMATH"]},"publishedAt":"2026-02-05T12:55:09.000Z","title":"Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training","summary":"Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05940.png","numComments":2,"submittedBy":{"_id":"68356f5db243fb809813a715","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68356f5db243fb809813a715/grhHvANfDRp75rMJxWlQo.jpeg","fullname":"LiuJunxiao","name":"master-lan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.06391","authors":[{"_id":"69894bc6beecc443208d25c0","name":"Zhongyin Zhao","hidden":false},{"_id":"69894bc6beecc443208d25c1","user":{"_id":"64d47a7a508a6313e33faedd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64d47a7a508a6313e33faedd/8DrbXdxA-sKZb4jgNhvY1.jpeg","isPro":false,"fullname":"Yuan Liu","user":"YuanLiuuuuuu","type":"user"},"name":"Yuan Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:29:55.119Z","hidden":false},{"_id":"69894bc6beecc443208d25c2","name":"Yikun Liu","hidden":false},{"_id":"69894bc6beecc443208d25c3","name":"Haicheng Wang","hidden":false},{"_id":"69894bc6beecc443208d25c4","name":"Le Tian","hidden":false},{"_id":"69894bc6beecc443208d25c5","name":"Xiao Zhou","hidden":false},{"_id":"69894bc6beecc443208d25c6","name":"Yangxiu You","hidden":false},{"_id":"69894bc6beecc443208d25c7","name":"Zilin Yu","hidden":false},{"_id":"69894bc6beecc443208d25c8","name":"Yang Yu","hidden":false},{"_id":"69894bc6beecc443208d25c9","name":"Jie Zhou","hidden":false}],"publishedAt":"2026-02-06T05:14:11.000Z","submittedOnDailyAt":"2026-02-09T00:30:58.259Z","title":"POINTS-GUI-G: GUI-Grounding Journey","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.","upvotes":14,"discussionId":"69894bc6beecc443208d25ca","githubRepo":"https://github.com/Tencent/POINTS-GUI","githubRepoAddedBy":"user","ai_summary":"GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.","ai_keywords":["vision-language models","GUI agents","GUI grounding","ScreenSpot-Pro","OSWorld-G","ScreenSpot-v2","UI-Vision","data engineering","training strategies","reinforcement learning","verifiable rewards"],"githubStars":22},"publishedAt":"2026-02-06T00:14:11.000Z","title":"POINTS-GUI-G: GUI-Grounding Journey","summary":"The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06391.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":230,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.05281","authors":[{"_id":"69889d19beecc443208d24b2","name":"Pengyi Li","hidden":false},{"_id":"69889d19beecc443208d24b3","name":"Elizaveta Goncharova","hidden":false},{"_id":"69889d19beecc443208d24b4","user":{"_id":"643984dceb7c5616ef3f5d54","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg","isPro":false,"fullname":"Andrey Kuznetsov","user":"kuznetsoffandrey","type":"user"},"name":"Andrey Kuznetsov","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:30:30.309Z","hidden":false},{"_id":"69889d19beecc443208d24b5","name":"Ivan Oseledets","hidden":false}],"publishedAt":"2026-02-05T04:06:55.000Z","submittedOnDailyAt":"2026-02-09T01:25:49.518Z","title":"Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities","submittedOnDailyBy":{"_id":"6734e315c1aadce903f73aea","avatarUrl":"/avatars/95d95c49419372debc201cb63c354b86.svg","isPro":false,"fullname":"Li Pengyi","user":"LiPengyi29","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.","upvotes":13,"discussionId":"69889d19beecc443208d24b6","ai_summary":"A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","policy optimization","Group Relative Policy Optimization","advantage estimation","Prompt Perplexity","Answer Confidence","entropy collapse","generative diversity","response entropy","exploration","exploitation","reasoning tasks","Pass@1","Pass@32"]},"publishedAt":"2026-02-04T23:06:55.000Z","title":"Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05281.png","numComments":2,"submittedBy":{"_id":"6734e315c1aadce903f73aea","avatarUrl":"/avatars/95d95c49419372debc201cb63c354b86.svg","fullname":"Li Pengyi","name":"LiPengyi29","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.06075","authors":[{"_id":"698949ccbeecc443208d25a3","user":{"_id":"64d761b98ebc40443831f82a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png","isPro":false,"fullname":"Guangyi Liu","user":"lgy0404","type":"user"},"name":"Guangyi Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-09T14:32:13.554Z","hidden":false},{"_id":"698949ccbeecc443208d25a4","name":"Pengxiang Zhao","hidden":false},{"_id":"698949ccbeecc443208d25a5","name":"Yaozhen Liang","hidden":false},{"_id":"698949ccbeecc443208d25a6","name":"Qinyi Luo","hidden":false},{"_id":"698949ccbeecc443208d25a7","name":"Shunye Tang","hidden":false},{"_id":"698949ccbeecc443208d25a8","name":"Yuxiang Chai","hidden":false},{"_id":"698949ccbeecc443208d25a9","name":"Weifeng Lin","hidden":false},{"_id":"698949ccbeecc443208d25aa","name":"Han Xiao","hidden":false},{"_id":"698949ccbeecc443208d25ab","name":"WenHao Wang","hidden":false},{"_id":"698949ccbeecc443208d25ac","name":"Siheng Chen","hidden":false},{"_id":"698949ccbeecc443208d25ad","user":{"_id":"676127cf11b19ea602bb202a","avatarUrl":"/avatars/dfd802a24bd63e509728159ebb1769f6.svg","isPro":false,"fullname":"Zhengxi Lu","user":"LZXzju","type":"user"},"name":"Zhengxi Lu","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:29:59.917Z","hidden":false},{"_id":"698949ccbeecc443208d25ae","name":"Gao Wu","hidden":false},{"_id":"698949ccbeecc443208d25af","name":"Hao Wang","hidden":false},{"_id":"698949ccbeecc443208d25b0","name":"Liang Liu","hidden":false},{"_id":"698949ccbeecc443208d25b1","name":"Yong Liu","hidden":false}],"publishedAt":"2026-02-03T17:01:59.000Z","submittedOnDailyAt":"2026-02-09T00:13:26.013Z","title":"MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments","submittedOnDailyBy":{"_id":"64d761b98ebc40443831f82a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png","isPro":false,"fullname":"Guangyi Liu","user":"lgy0404","type":"user"},"summary":"Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.","upvotes":13,"discussionId":"698949ccbeecc443208d25b2","projectPage":"https://lgy0404.github.io/MemGUI-Bench/","githubRepo":"https://github.com/lgy0404/MemGUI-Bench","githubRepoAddedBy":"user","ai_summary":"A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.","ai_keywords":["memory-centric benchmark","LLM-as-judge evaluation","memory taxonomy","cross-temporal retention","cross-spatial retention","automated pipeline","Progressive Scrutiny","hierarchical metrics","state-of-the-art agents","failure modes","design implications"],"githubStars":19,"organization":{"_id":"61bac2af530e5c78d7b99667","name":"zju","fullname":"Zhejiang University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}},"publishedAt":"2026-02-03T12:01:59.000Z","title":"MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments","summary":"Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06075.png","numComments":0,"submittedBy":{"_id":"64d761b98ebc40443831f82a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png","fullname":"Guangyi Liu","name":"lgy0404","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"61bac2af530e5c78d7b99667","name":"zju","fullname":"Zhejiang University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06079","authors":[{"_id":"69895559beecc443208d26a2","name":"Liangyu Wang","hidden":false},{"_id":"69895559beecc443208d26a3","name":"Siqi Zhang","hidden":false},{"_id":"69895559beecc443208d26a4","name":"Junjie Wang","hidden":false},{"_id":"69895559beecc443208d26a5","name":"Yiming Dong","hidden":false},{"_id":"69895559beecc443208d26a6","name":"Bo Zheng","hidden":false},{"_id":"69895559beecc443208d26a7","name":"Zihan Qiu","hidden":false},{"_id":"69895559beecc443208d26a8","name":"Shengkun Tang","hidden":false},{"_id":"69895559beecc443208d26a9","name":"Di Wang","hidden":false},{"_id":"69895559beecc443208d26aa","name":"Rui Men","hidden":false},{"_id":"69895559beecc443208d26ab","name":"Dayiheng Liu","hidden":false}],"publishedAt":"2026-02-04T07:38:24.000Z","submittedOnDailyAt":"2026-02-09T05:54:16.367Z","title":"Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers","submittedOnDailyBy":{"_id":"66224a84afbc88c1e4881ad7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66224a84afbc88c1e4881ad7/fGDiIiqhTBQri3khSqNcU.jpeg","isPro":false,"fullname":"Liangyu Wang","user":"ly4096","type":"user"},"summary":"The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.","upvotes":12,"discussionId":"6989555abeecc443208d26ac","ai_summary":"Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.","ai_keywords":["Large Language Models","matrix-based optimizers","Shampoo","Muon","SOAP","distributed frameworks","Megatron","synchronous approaches","layer-wise partitioning","geometric constraints","Canzona","Data Parallelism","alpha-Balanced Static Partitioning","tensor fragmentation","Tensor Parallelism","Asynchronous Compute pipeline","Micro-Group Scheduling","optimizer step latency","end-to-end iteration time"],"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}},"publishedAt":"2026-02-04T02:38:24.000Z","title":"Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers","summary":"The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06079.png","numComments":2,"submittedBy":{"_id":"66224a84afbc88c1e4881ad7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66224a84afbc88c1e4881ad7/fGDiIiqhTBQri3khSqNcU.jpeg","fullname":"Liangyu Wang","name":"ly4096","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06139","authors":[{"_id":"69894d1dbeecc443208d25fb","name":"Ashish Seth","hidden":false},{"_id":"69894d1dbeecc443208d25fc","name":"Xinhao Mei","hidden":false},{"_id":"69894d1dbeecc443208d25fd","name":"Changsheng Zhao","hidden":false},{"_id":"69894d1dbeecc443208d25fe","name":"Varun Nagaraja","hidden":false},{"_id":"69894d1dbeecc443208d25ff","name":"Ernie Chang","hidden":false},{"_id":"69894d1dbeecc443208d2600","name":"Gregory P. Meyer","hidden":false},{"_id":"69894d1dbeecc443208d2601","name":"Gael Le Lan","hidden":false},{"_id":"69894d1dbeecc443208d2602","name":"Yunyang Xiong","hidden":false},{"_id":"69894d1dbeecc443208d2603","name":"Vikas Chandra","hidden":false},{"_id":"69894d1dbeecc443208d2604","name":"Yangyang Shi","hidden":false},{"_id":"69894d1dbeecc443208d2605","name":"Dinesh Manocha","hidden":false},{"_id":"69894d1dbeecc443208d2606","name":"Zhipeng Cai","hidden":false}],"publishedAt":"2026-02-05T19:16:55.000Z","submittedOnDailyAt":"2026-02-09T00:44:50.275Z","title":"EgoAVU: Egocentric Audio-Visual Understanding","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.","upvotes":9,"discussionId":"69894d1dbeecc443208d2607","ai_summary":"Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.","ai_keywords":["multi-modal large language models","egocentric videos","audio-visual narrations","cross-modal correlation modeling","token-based video filtering","modular curation","graph-based curation","EgoAVU-Instruct","EgoAVU-Bench","EgoTempo","EgoIllusion"]},"publishedAt":"2026-02-05T14:16:55.000Z","title":"EgoAVU: Egocentric Audio-Visual Understanding","summary":"Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06139.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":230,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.05367","authors":[{"_id":"6985919b4ad556f294b7ec9c","name":"Youngcheon You","hidden":false},{"_id":"6985919b4ad556f294b7ec9d","name":"Banseok Lee","hidden":false},{"_id":"6985919b4ad556f294b7ec9e","name":"Minseop Choi","hidden":false},{"_id":"6985919b4ad556f294b7ec9f","name":"Seonyoung Kim","hidden":false},{"_id":"6985919b4ad556f294b7eca0","user":{"_id":"6670d2ec92412fd464eac919","avatarUrl":"/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg","isPro":false,"fullname":"Hyochan Chong","user":"d7chong","type":"user"},"name":"Hyochan Chong","status":"claimed_verified","statusLastChangedAt":"2026-02-06T18:50:42.521Z","hidden":false},{"_id":"6985919b4ad556f294b7eca1","name":"Changdong Kim","hidden":false},{"_id":"6985919b4ad556f294b7eca2","name":"Youngmin Kim","hidden":false},{"_id":"6985919b4ad556f294b7eca3","user":{"_id":"67f25ca8cef233be93bec839","avatarUrl":"/avatars/9e0bf25d4d7b1ab2a0a59b1bf04b0d80.svg","isPro":false,"fullname":"Dongkyu Kim","user":"dongkyu-kim","type":"user"},"name":"Dongkyu Kim","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:35:27.743Z","hidden":false}],"publishedAt":"2026-02-05T06:41:11.000Z","submittedOnDailyAt":"2026-02-09T02:54:59.622Z","title":"RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs","submittedOnDailyBy":{"_id":"6670d2ec92412fd464eac919","avatarUrl":"/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg","isPro":false,"fullname":"Hyochan Chong","user":"d7chong","type":"user"},"summary":"Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (pm1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a 4.49times inference speed-up over full-precision models on an RTX 4090.","upvotes":7,"discussionId":"6985919b4ad556f294b7eca4","ai_summary":"Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.","ai_keywords":["residual binarization","quantization-aware training","QAT","inter-path adaptation","residual hierarchy","binary layers","error-compensation structure","RaBiT","Vector Quantization","VQ","inference speed-up","RTX 4090"],"organization":{"_id":"686df54910a52f2c2cf03c06","name":"SamsungResearch","fullname":"Samsung Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"}},"publishedAt":"2026-02-05T01:41:11.000Z","title":"RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs","summary":"Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (pm1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a 4.49times inference speed-up over full-precision models on an RTX 4090.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05367.png","numComments":2,"submittedBy":{"_id":"6670d2ec92412fd464eac919","avatarUrl":"/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg","fullname":"Hyochan Chong","name":"d7chong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"686df54910a52f2c2cf03c06","name":"SamsungResearch","fullname":"Samsung Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.06960","authors":[{"_id":"69894ae3beecc443208d25b4","user":{"_id":"64098738342c26884c792c93","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg","isPro":false,"fullname":"Yuchen Yan","user":"yanyc","type":"user"},"name":"Yuchen Yan","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:29:57.465Z","hidden":false},{"_id":"69894ae3beecc443208d25b5","name":"Liang Jiang","hidden":false},{"_id":"69894ae3beecc443208d25b6","name":"Jin Jiang","hidden":false},{"_id":"69894ae3beecc443208d25b7","name":"Shuaicheng Li","hidden":false},{"_id":"69894ae3beecc443208d25b8","name":"Zujie Wen","hidden":false},{"_id":"69894ae3beecc443208d25b9","name":"Zhiqiang Zhang","hidden":false},{"_id":"69894ae3beecc443208d25ba","name":"Jun Zhou","hidden":false},{"_id":"69894ae3beecc443208d25bb","name":"Jian Shao","hidden":false},{"_id":"69894ae3beecc443208d25bc","name":"Yueting Zhuang","hidden":false},{"_id":"69894ae3beecc443208d25bd","name":"Yongliang Shen","hidden":false}],"publishedAt":"2026-02-06T18:59:27.000Z","submittedOnDailyAt":"2026-02-09T00:18:12.522Z","title":"InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.","upvotes":6,"discussionId":"69894ae3beecc443208d25be","projectPage":"https://zju-real.github.io/InftyThink-Plus/","githubRepo":"https://github.com/ZJU-REAL/InftyThink-Plus","githubRepoAddedBy":"user","ai_summary":"InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.","ai_keywords":["chain-of-thought","iterative reasoning","reinforcement learning","trajectory-level reinforcement learning","summarization","reasoning efficiency","inference latency"],"githubStars":11,"organization":{"_id":"61bac2af530e5c78d7b99667","name":"zju","fullname":"Zhejiang University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}},"publishedAt":"2026-02-06T13:59:27.000Z","title":"InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning","summary":"Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06960.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":230,"isUserFollowing":false},"organization":{"_id":"61bac2af530e5c78d7b99667","name":"zju","fullname":"Zhejiang University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06869","authors":[{"_id":"698a27971b2dc6b37d61ae0c","user":{"_id":"642f742270daaa6e7209a2c8","avatarUrl":"/avatars/746fb840c220329f69a17905ea519322.svg","isPro":false,"fullname":"Yining Lu","user":"ylu610","type":"user"},"name":"Yining Lu","status":"claimed_verified","statusLastChangedAt":"2026-02-09T21:07:08.436Z","hidden":false},{"_id":"698a27971b2dc6b37d61ae0d","name":"Meng Jiang","hidden":false}],"publishedAt":"2026-02-06T16:55:27.000Z","submittedOnDailyAt":"2026-02-09T16:37:44.755Z","title":"Uncovering Cross-Objective Interference in Multi-Objective Alignment","submittedOnDailyBy":{"_id":"642f742270daaa6e7209a2c8","avatarUrl":"/avatars/746fb840c220329f69a17905ea519322.svg","isPro":false,"fullname":"Yining Lu","user":"ylu610","type":"user"},"summary":"We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.\n  To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.","upvotes":5,"discussionId":"698a27971b2dc6b37d61ae0e","githubRepo":"https://github.com/yining610/ctwa","githubRepoAddedBy":"user","ai_summary":"Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals.","ai_keywords":["multi-objective alignment","large language models","cross-objective interference","scalarization algorithms","local covariance law","clipped surrogate objectives","covariance targeted weight adaptation","Polyak--Łojasiewicz condition","non-convex optimization","global convergence"],"githubStars":0,"organization":{"_id":"698a2b8c599fceb2ec79b6a6","name":"DM2-ND","fullname":"University of Notre Dame DM2 Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/642f742270daaa6e7209a2c8/HftV6o39IOFmMo-d5pSvc.jpeg"}},"publishedAt":"2026-02-06T11:55:27.000Z","title":"Uncovering Cross-Objective Interference in Multi-Objective Alignment","summary":"We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.\n  To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06869.png","numComments":1,"submittedBy":{"_id":"642f742270daaa6e7209a2c8","avatarUrl":"/avatars/746fb840c220329f69a17905ea519322.svg","fullname":"Yining Lu","name":"ylu610","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"698a2b8c599fceb2ec79b6a6","name":"DM2-ND","fullname":"University of Notre Dame DM2 Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/642f742270daaa6e7209a2c8/HftV6o39IOFmMo-d5pSvc.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.06669","authors":[{"_id":"6989d925beecc443208d28bd","name":"Lucie Termignon","hidden":false},{"_id":"6989d925beecc443208d28be","user":{"_id":"649d986a474bf415c03b772c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649d986a474bf415c03b772c/hcBoKXvrQHuXnaiGH1YOV.jpeg","isPro":false,"fullname":"Simonas Zilinskas","user":"monsimas","type":"user"},"name":"Simonas Zilinskas","status":"claimed_verified","statusLastChangedAt":"2026-02-09T14:31:15.121Z","hidden":false},{"_id":"6989d925beecc443208d28bf","name":"Hadrien Pélissier","hidden":false},{"_id":"6989d925beecc443208d28c0","name":"Aurélien Barrot","hidden":false},{"_id":"6989d925beecc443208d28c1","name":"Nicolas Chesnais","hidden":false},{"_id":"6989d925beecc443208d28c2","name":"Elie Gavoty","hidden":false}],"publishedAt":"2026-02-06T12:53:44.000Z","submittedOnDailyAt":"2026-02-09T10:28:59.162Z","title":"compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data","submittedOnDailyBy":{"_id":"667d6f513a685cd30f542fef","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/667d6f513a685cd30f542fef/Eba61coS1WNNEVWBSjXF0.png","isPro":true,"fullname":"compar:IA","user":"comparIA","type":"user"},"summary":"Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.","upvotes":5,"discussionId":"6989d926beecc443208d28c3","projectPage":"https://comparia.beta.gouv.fr/","githubRepo":"https://github.com/betagouv/ComparIA","githubRepoAddedBy":"user","ai_summary":"Compar:IA is an open-source platform that collects large-scale human preference data for multilingual language model training and evaluation, featuring a blind pairwise comparison interface and releasing three datasets under open licenses.","ai_keywords":["Reinforcement Learning from Human Feedback","Direct Preference Optimization","human preference data","language models","multilingual model training","human-AI interaction"],"githubStars":61,"organization":{"_id":"6686a1713d2f59aa670daf57","name":"ministere-culture","fullname":"Ministère de la Culture (SNUM)","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/8GP6GKPj374y5JkFeysuz.png"}},"publishedAt":"2026-02-06T07:53:44.000Z","title":"compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data","summary":"Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06669.png","numComments":1,"submittedBy":{"_id":"667d6f513a685cd30f542fef","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/667d6f513a685cd30f542fef/Eba61coS1WNNEVWBSjXF0.png","fullname":"compar:IA","name":"comparIA","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":20,"isUserFollowing":false},"organization":{"_id":"6686a1713d2f59aa670daf57","name":"ministere-culture","fullname":"Ministère de la Culture (SNUM)","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/8GP6GKPj374y5JkFeysuz.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06663","authors":[{"_id":"698959b9beecc443208d26c2","user":{"_id":"656ae4088fb1ddf0d5ec9ac5","avatarUrl":"/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg","isPro":false,"fullname":"Junxian Li","user":"Duke-de-Artois","type":"user"},"name":"Junxian Li","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:29:28.216Z","hidden":false},{"_id":"698959b9beecc443208d26c3","name":"Kai Liu","hidden":false},{"_id":"698959b9beecc443208d26c4","name":"Leyang Chen","hidden":false},{"_id":"698959b9beecc443208d26c5","user":{"_id":"661b9d96c153e4a0a25adc3e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg","isPro":false,"fullname":"Weida Wang","user":"weidawang","type":"user"},"name":"Weida Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:29:30.996Z","hidden":false},{"_id":"698959b9beecc443208d26c6","name":"Zhixin Wang","hidden":false},{"_id":"698959b9beecc443208d26c7","name":"Jiaqi Xu","hidden":false},{"_id":"698959b9beecc443208d26c8","name":"Fan Li","hidden":false},{"_id":"698959b9beecc443208d26c9","name":"Renjing Pei","hidden":false},{"_id":"698959b9beecc443208d26ca","name":"Linghe Kong","hidden":false},{"_id":"698959b9beecc443208d26cb","name":"Yulun Zhang","hidden":false}],"publishedAt":"2026-02-06T12:47:16.000Z","submittedOnDailyAt":"2026-02-09T01:24:35.879Z","title":"PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks","submittedOnDailyBy":{"_id":"656ae4088fb1ddf0d5ec9ac5","avatarUrl":"/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg","isPro":false,"fullname":"Junxian Li","user":"Duke-de-Artois","type":"user"},"summary":"Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.","upvotes":5,"discussionId":"698959b9beecc443208d26cc","projectPage":"https://github.com/lijunxian111/PlanViz","githubRepo":"https://github.com/lijunxian111/PlanViz","githubRepoAddedBy":"user","ai_summary":"PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.","ai_keywords":["unified multimodal models","image generation","image editing","spatial reasoning","procedural understanding","computer-use planning","PlanViz","route planning","work diagramming","web&UI displaying","PlanScore"],"githubStars":16},"publishedAt":"2026-02-06T07:47:16.000Z","title":"PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks","summary":"Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06663.png","numComments":1,"submittedBy":{"_id":"656ae4088fb1ddf0d5ec9ac5","avatarUrl":"/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg","fullname":"Junxian Li","name":"Duke-de-Artois","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.04837","authors":[{"_id":"6985980f4ad556f294b7ecb6","name":"Zhaotian Weng","hidden":false},{"_id":"6985980f4ad556f294b7ecb7","name":"Antonis Antoniades","hidden":false},{"_id":"6985980f4ad556f294b7ecb8","name":"Deepak Nathani","hidden":false},{"_id":"6985980f4ad556f294b7ecb9","name":"Zhen Zhang","hidden":false},{"_id":"6985980f4ad556f294b7ecba","name":"Xiao Pu","hidden":false},{"_id":"6985980f4ad556f294b7ecbb","user":{"_id":"64679a226192d39142245e5e","avatarUrl":"/avatars/05abee0b6317f100923936ca2099e9eb.svg","isPro":false,"fullname":"Xin Eric Wang","user":"xw-eric","type":"user"},"name":"Xin Eric Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-06T18:50:40.073Z","hidden":false}],"publishedAt":"2026-02-04T18:29:36.000Z","submittedOnDailyAt":"2026-02-09T06:11:10.033Z","title":"Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing","submittedOnDailyBy":{"_id":"64679a226192d39142245e5e","avatarUrl":"/avatars/05abee0b6317f100923936ca2099e9eb.svg","isPro":false,"fullname":"Xin Eric Wang","user":"xw-eric","type":"user"},"summary":"Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.","upvotes":5,"discussionId":"698598104ad556f294b7ecbc","ai_summary":"Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.","ai_keywords":["open-ended self-improving agents","structural design modification","evolutionary unit","experience sharing","exploratory diversity","self-evolving methods","coding benchmarks","SWE-bench Verified","Polyglot","agent frameworks","framework-level bugs"]},"publishedAt":"2026-02-04T13:29:36.000Z","title":"Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing","summary":"Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04837.png","numComments":2,"submittedBy":{"_id":"64679a226192d39142245e5e","avatarUrl":"/avatars/05abee0b6317f100923936ca2099e9eb.svg","fullname":"Xin Eric Wang","name":"xw-eric","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.02581","authors":[{"_id":"6987aa52beecc443208d23dc","user":{"_id":"63c6eb9e8bfd6a208eb2a593","avatarUrl":"/avatars/a55ea118af3d2bab2bc38c559abe3148.svg","isPro":false,"fullname":"Nan Zhang","user":"nanzhang","type":"user"},"name":"Nan Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:30:46.297Z","hidden":false},{"_id":"6987aa52beecc443208d23dd","name":"Eugene Kwek","hidden":false},{"_id":"6987aa52beecc443208d23de","name":"Yusen Zhang","hidden":false},{"_id":"6987aa52beecc443208d23df","name":"Muyu Pan","hidden":false},{"_id":"6987aa52beecc443208d23e0","name":"Suhang Wang","hidden":false},{"_id":"6987aa52beecc443208d23e1","name":"Prasenjit Mitra","hidden":false},{"_id":"6987aa52beecc443208d23e2","name":"Rui Zhang","hidden":false}],"publishedAt":"2026-01-31T16:19:20.000Z","submittedOnDailyAt":"2026-02-09T06:07:28.260Z","title":"QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals","submittedOnDailyBy":{"_id":"63c6eb9e8bfd6a208eb2a593","avatarUrl":"/avatars/a55ea118af3d2bab2bc38c559abe3148.svg","isPro":false,"fullname":"Nan Zhang","user":"nanzhang","type":"user"},"summary":"Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term \"protecting both ends\". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.","upvotes":5,"discussionId":"6987aa53beecc443208d23e3","githubRepo":"https://github.com/psunlpgroup/QuantLRM","githubRepoAddedBy":"user","ai_summary":"QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.","ai_keywords":["weight-only quantization","Large Language Models","Large Reasoning Models","fine-tuning","magnitude pruning","weight updates","restricted quadratic functions","channel importance","pseudo-fine-tuning"],"githubStars":3,"organization":{"_id":"623c72b6483fb88b35620a27","name":"PennState","fullname":"Pennsylvania State University"}},"publishedAt":"2026-01-31T11:19:20.000Z","title":"QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals","summary":"Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term \"protecting both ends\". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02581.png","numComments":2,"submittedBy":{"_id":"63c6eb9e8bfd6a208eb2a593","avatarUrl":"/avatars/a55ea118af3d2bab2bc38c559abe3148.svg","fullname":"Nan Zhang","name":"nanzhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"623c72b6483fb88b35620a27","name":"PennState","fullname":"Pennsylvania State University"},"isAuthorParticipating":true},{"paper":{"id":"2602.06854","authors":[{"_id":"69896b56beecc443208d26ee","name":"Mingqian Feng","hidden":false},{"_id":"69896b56beecc443208d26ef","name":"Xiaodong Liu","hidden":false},{"_id":"69896b56beecc443208d26f0","name":"Weiwei Yang","hidden":false},{"_id":"69896b56beecc443208d26f1","name":"Jialin Song","hidden":false},{"_id":"69896b56beecc443208d26f2","name":"Xuekai Zhu","hidden":false},{"_id":"69896b56beecc443208d26f3","name":"Chenliang Xu","hidden":false},{"_id":"69896b56beecc443208d26f4","name":"Jianfeng Gao","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/a_3LkBK51Y0fO2Pmf2uaq.png","https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/vr5XDSyOSXw6D2WZ05pvn.png","https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/YVFcTdCSbiqE-ww4JPpJb.png","https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/HiZycfFyN1JUsteSVVPE0.png"],"publishedAt":"2026-02-06T16:44:57.000Z","submittedOnDailyAt":"2026-02-09T02:38:48.605Z","title":"SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks","submittedOnDailyBy":{"_id":"66332a98e39731d65a4b7e45","avatarUrl":"/avatars/193e9ee3ac7488960229d6edddb9d1e9.svg","isPro":true,"fullname":"Mingqian Feng","user":"fmmarkmq","type":"user"},"summary":"Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.","upvotes":4,"discussionId":"69896b56beecc443208d26f5","ai_summary":"A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and datasets.","ai_keywords":["multi-turn jailbreaks","reinforcement learning","intent-drift-aware reward","supervised fine-tuning","direct preference optimization","attack success rates","adversarial prompts","victim models","jailbreak judges","open-loop attack regime"],"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}},"publishedAt":"2026-02-06T11:44:57.000Z","title":"SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks","summary":"Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/a_3LkBK51Y0fO2Pmf2uaq.png","https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/vr5XDSyOSXw6D2WZ05pvn.png","https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/YVFcTdCSbiqE-ww4JPpJb.png","https://cdn-uploads.huggingface.co/production/uploads/66332a98e39731d65a4b7e45/HiZycfFyN1JUsteSVVPE0.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06854.png","numComments":1,"submittedBy":{"_id":"66332a98e39731d65a4b7e45","avatarUrl":"/avatars/193e9ee3ac7488960229d6edddb9d1e9.svg","fullname":"Mingqian Feng","name":"fmmarkmq","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06176","authors":[{"_id":"698a479d1b2dc6b37d61ae76","user":{"_id":"649c5cf5c1ae48cf4d7dda34","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649c5cf5c1ae48cf4d7dda34/bSJXATqkqBn8ypUy0OezY.jpeg","isPro":false,"fullname":"Peiyang Song","user":"p-song1","type":"user"},"name":"Peiyang Song","status":"claimed_verified","statusLastChangedAt":"2026-02-09T21:07:00.512Z","hidden":false},{"_id":"698a479d1b2dc6b37d61ae77","name":"Pengrui Han","hidden":false},{"_id":"698a479d1b2dc6b37d61ae78","name":"Noah Goodman","hidden":false}],"publishedAt":"2026-02-05T20:29:26.000Z","submittedOnDailyAt":"2026-02-09T18:20:10.537Z","title":"Large Language Model Reasoning Failures","submittedOnDailyBy":{"_id":"649c5cf5c1ae48cf4d7dda34","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649c5cf5c1ae48cf4d7dda34/bSJXATqkqBn8ypUy0OezY.jpeg","isPro":false,"fullname":"Peiyang Song","user":"p-song1","type":"user"},"summary":"Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.","upvotes":4,"discussionId":"698a479e1b2dc6b37d61ae79","githubRepo":"https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures","githubRepoAddedBy":"user","ai_summary":"Large language models exhibit significant reasoning failures that can be categorized into embodied and non-embodied types, with fundamental, application-specific, and robustness-related subtypes, requiring systematic analysis and mitigation strategies.","ai_keywords":["large language models","reasoning capabilities","reasoning failures","embodied reasoning","non-embodied reasoning","informal reasoning","formal reasoning","fundamental failures","application-specific limitations","robustness issues"],"githubStars":15,"organization":{"_id":"672c672dcf09d152f4da04c4","name":"StanfordUniversity","fullname":"Stanford University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"}},"publishedAt":"2026-02-05T15:29:26.000Z","title":"Large Language Model Reasoning Failures","summary":"Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06176.png","numComments":2,"submittedBy":{"_id":"649c5cf5c1ae48cf4d7dda34","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649c5cf5c1ae48cf4d7dda34/bSJXATqkqBn8ypUy0OezY.jpeg","fullname":"Peiyang Song","name":"p-song1","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"672c672dcf09d152f4da04c4","name":"StanfordUniversity","fullname":"Stanford University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.05847","authors":[{"_id":"6989c105beecc443208d2838","name":"Zhangquan Chen","hidden":false},{"_id":"6989c105beecc443208d2839","name":"Jiale Tao","hidden":false},{"_id":"6989c105beecc443208d283a","name":"Ruihuang Li","hidden":false},{"_id":"6989c105beecc443208d283b","name":"Yihao Hu","hidden":false},{"_id":"6989c105beecc443208d283c","name":"Ruitao Chen","hidden":false},{"_id":"6989c105beecc443208d283d","name":"Zhantao Yang","hidden":false},{"_id":"6989c105beecc443208d283e","name":"Xinlei Yu","hidden":false},{"_id":"6989c105beecc443208d283f","name":"Haodong Jing","hidden":false},{"_id":"6989c105beecc443208d2840","name":"Manyuan Zhang","hidden":false},{"_id":"6989c105beecc443208d2841","name":"Shuai Shao","hidden":false},{"_id":"6989c105beecc443208d2842","name":"Biao Wang","hidden":false},{"_id":"6989c105beecc443208d2843","name":"Qinglin Lu","hidden":false},{"_id":"6989c105beecc443208d2844","name":"Ruqi Huang","hidden":false}],"publishedAt":"2026-02-05T16:35:19.000Z","submittedOnDailyAt":"2026-02-09T08:44:04.774Z","title":"OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention","submittedOnDailyBy":{"_id":"6458a99c3b81018d6b93aecb","avatarUrl":"/avatars/d90f16e8d3e49e095fea4cbd899837df.svg","isPro":false,"fullname":"jankin","user":"jankin123","type":"user"},"summary":"While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.","upvotes":4,"discussionId":"6989c105beecc443208d2845","ai_summary":"OmniVideo-R1 enhances audio-visual understanding through reinforced frameworks that integrate self-supervised and contrastive learning for multimodal reasoning.","ai_keywords":["omnivideo models","mixed-modality reasoning","self-supervised learning","contrastive learning","query-intensive grounding","modality-attentive fusion"],"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-05T11:35:19.000Z","title":"OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention","summary":"While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05847.png","numComments":2,"submittedBy":{"_id":"6458a99c3b81018d6b93aecb","avatarUrl":"/avatars/d90f16e8d3e49e095fea4cbd899837df.svg","fullname":"jankin","name":"jankin123","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.05711","authors":[{"_id":"69884b90beecc443208d2475","user":{"_id":"673ab3647afcea17eb4378fd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png","isPro":false,"fullname":"Loser Cheems","user":"JingzeShi","type":"user"},"name":"Jingze Shi","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:30:35.555Z","hidden":false},{"_id":"69884b90beecc443208d2476","name":"Zhangyang Peng","hidden":false},{"_id":"69884b90beecc443208d2477","name":"Yizhang Zhu","hidden":false},{"_id":"69884b90beecc443208d2478","name":"Yifan Wu","hidden":false},{"_id":"69884b90beecc443208d2479","name":"Guang Liu","hidden":false},{"_id":"69884b90beecc443208d247a","name":"Yuyu Luo","hidden":false}],"publishedAt":"2026-02-05T14:37:32.000Z","submittedOnDailyAt":"2026-02-09T00:30:00.301Z","title":"OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale","submittedOnDailyBy":{"_id":"673ab3647afcea17eb4378fd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png","isPro":false,"fullname":"Loser Cheems","user":"JingzeShi","type":"user"},"summary":"Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.","upvotes":4,"discussionId":"69884b90beecc443208d247b","githubRepo":"https://github.com/flash-algo/omni-moe","githubRepoAddedBy":"user","ai_summary":"OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.","ai_keywords":["Mixture-of-Experts","expert specialization","atomic experts","routing complexity","memory access","Cartesian Product Router","Expert-Centric Scheduling","inference latency","zero-shot accuracy"],"githubStars":53,"organization":{"_id":"61be9739d2f9358e24ca0a4f","name":"BAAI","fullname":"Beijing Academy of Artificial Intelligence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}},"publishedAt":"2026-02-05T09:37:32.000Z","title":"OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale","summary":"Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05711.png","numComments":2,"submittedBy":{"_id":"673ab3647afcea17eb4378fd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png","fullname":"Loser Cheems","name":"JingzeShi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":43,"isUserFollowing":false},"organization":{"_id":"61be9739d2f9358e24ca0a4f","name":"BAAI","fullname":"Beijing Academy of Artificial Intelligence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.06554","authors":[{"_id":"69896f0dbeecc443208d26f7","name":"Tianyi Hu","hidden":false},{"_id":"69896f0dbeecc443208d26f8","name":"Qingxu Fu","hidden":false},{"_id":"69896f0dbeecc443208d26f9","name":"Yanxi Chen","hidden":false},{"_id":"69896f0dbeecc443208d26fa","name":"Zhaoyang Liu","hidden":false},{"_id":"69896f0dbeecc443208d26fb","name":"Bolin Ding","hidden":false}],"publishedAt":"2026-02-06T09:57:23.000Z","submittedOnDailyAt":"2026-02-09T02:52:23.412Z","title":"SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.\n  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.\n  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.\n  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.","upvotes":3,"discussionId":"69896f0dbeecc443208d26fc","ai_summary":"SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.","ai_keywords":["reinforcement learning","policy update mechanisms","advantage estimation","convergence guarantees","multi-turn scenarios","REINFORCE","Group Relative Advantage Estimation","PPO","critic-free","sequential policy updates","backward induction","multi-agent bandit problems"],"organization":{"_id":"6925b20fed452d1567c012d3","name":"Tongyi-MAI","fullname":"Tongyi-MAI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}},"publishedAt":"2026-02-06T04:57:23.000Z","title":"SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees","summary":"Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.\n  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.\n  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.\n  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06554.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":230,"isUserFollowing":false},"organization":{"_id":"6925b20fed452d1567c012d3","name":"Tongyi-MAI","fullname":"Tongyi-MAI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.06471","authors":[{"_id":"6989492abeecc443208d259d","user":{"_id":"643fb7332397d8eef5b844cd","avatarUrl":"/avatars/e403a19fc13e478d5929c67028230b0e.svg","isPro":false,"fullname":"Feng-Ting Liao","user":"FengTing","type":"user"},"name":"Feng-Ting Liao","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:30:02.299Z","hidden":false},{"_id":"6989492abeecc443208d259e","name":"Meng-Hsi Chen","hidden":false},{"_id":"6989492abeecc443208d259f","name":"Guan-Ting Yi","hidden":false},{"_id":"6989492abeecc443208d25a0","name":"Da-shan Shiu","hidden":false}],"publishedAt":"2026-02-06T07:55:30.000Z","submittedOnDailyAt":"2026-02-09T00:11:07.040Z","title":"Revisiting the Shape Convention of Transformer Language Models","submittedOnDailyBy":{"_id":"643fb7332397d8eef5b844cd","avatarUrl":"/avatars/e403a19fc13e478d5929c67028230b0e.svg","isPro":false,"fullname":"Feng-Ting Liao","user":"FengTing","type":"user"},"summary":"Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.","upvotes":3,"discussionId":"6989492abeecc443208d25a1","ai_summary":"Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.","ai_keywords":["Transformer","feed-forward network","MLP","attention module","residual pathways","hourglass MLP","narrow-wide-narrow","model scaling","parameter efficiency"],"organization":{"_id":"6388e08c5a3d2a335624705b","name":"MediaTek-Research","fullname":"MediaTek Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"}},"publishedAt":"2026-02-06T02:55:30.000Z","title":"Revisiting the Shape Convention of Transformer Language Models","summary":"Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06471.png","numComments":1,"submittedBy":{"_id":"643fb7332397d8eef5b844cd","avatarUrl":"/avatars/e403a19fc13e478d5929c67028230b0e.svg","fullname":"Feng-Ting Liao","name":"FengTing","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"6388e08c5a3d2a335624705b","name":"MediaTek-Research","fullname":"MediaTek Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1669948083850-6213410828005421265b27d3.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.03075","authors":[{"_id":"6989beb3beecc443208d282f","name":"Junjie Huang","hidden":false},{"_id":"6989beb3beecc443208d2830","name":"Jiarui Qin","hidden":false},{"_id":"6989beb3beecc443208d2831","user":{"_id":"63fc75f9b9db84750cea9c5c","avatarUrl":"/avatars/2c5bf9685e0cfc4b5785a4a86c34e0db.svg","isPro":false,"fullname":"DI YIN","user":"DIYIN","type":"user"},"name":"Di Yin","status":"claimed_verified","statusLastChangedAt":"2026-02-09T14:31:17.616Z","hidden":false},{"_id":"6989beb3beecc443208d2832","name":"Weiwen Liu","hidden":false},{"_id":"6989beb3beecc443208d2833","name":"Yong Yu","hidden":false},{"_id":"6989beb3beecc443208d2834","name":"Xing Sun","hidden":false},{"_id":"6989beb3beecc443208d2835","name":"Weinan Zhang","hidden":false}],"publishedAt":"2026-02-03T04:04:41.000Z","submittedOnDailyAt":"2026-02-09T08:34:30.813Z","title":"ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution","submittedOnDailyBy":{"_id":"63fc75f9b9db84750cea9c5c","avatarUrl":"/avatars/2c5bf9685e0cfc4b5785a4a86c34e0db.svg","isPro":false,"fullname":"DI YIN","user":"DIYIN","type":"user"},"summary":"Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.","upvotes":3,"discussionId":"6989beb3beecc443208d2836","ai_summary":"ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop.","ai_keywords":["large language models","reinforcement learning","mid-training phase","token reweighting","reasoning priors","pre-training","post-training","iterative feedback loop"],"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-02T23:04:41.000Z","title":"ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution","summary":"Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03075.png","numComments":2,"submittedBy":{"_id":"63fc75f9b9db84750cea9c5c","avatarUrl":"/avatars/2c5bf9685e0cfc4b5785a4a86c34e0db.svg","fullname":"DI YIN","name":"DIYIN","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.06883","authors":[{"_id":"698989dfbeecc443208d2749","user":{"_id":"6661f79ac36ae4c83f3213e4","avatarUrl":"/avatars/2e48f052fdf37b5b06d101a6a3232eea.svg","isPro":false,"fullname":"Ambroise Odonnat","user":"ambroiseodt","type":"user"},"name":"Ambroise Odonnat","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:27:50.665Z","hidden":false},{"_id":"698989dfbeecc443208d274a","name":"Laetitia Chapel","hidden":false},{"_id":"698989dfbeecc443208d274b","name":"Romain Tavenard","hidden":false},{"_id":"698989dfbeecc443208d274c","name":"Ievgen Redko","hidden":false}],"publishedAt":"2026-02-06T17:12:22.000Z","submittedOnDailyAt":"2026-02-09T04:59:09.364Z","title":"Vision Transformer Finetuning Benefits from Non-Smooth Components","submittedOnDailyBy":{"_id":"6661f79ac36ae4c83f3213e4","avatarUrl":"/avatars/2e48f052fdf37b5b06d101a6a3232eea.svg","isPro":false,"fullname":"Ambroise Odonnat","user":"ambroiseodt","type":"user"},"summary":"The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.","upvotes":2,"discussionId":"698989e0beecc443208d274d","githubRepo":"https://github.com/ambroiseodt/vit-plasticity","githubRepoAddedBy":"user","ai_summary":"Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.","ai_keywords":["vision transformer","attention modules","feedforward layers","plasticity","smoothness","generalization","training stability","adversarial robustness","transfer learning","finetuning performance"],"githubStars":6,"organization":{"_id":"5f83c275f0801648bf88454a","name":"huawei-noah","fullname":"HUAWEI Noah's Ark Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}},"publishedAt":"2026-02-06T12:12:22.000Z","title":"Vision Transformer Finetuning Benefits from Non-Smooth Components","summary":"The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06883.png","numComments":1,"submittedBy":{"_id":"6661f79ac36ae4c83f3213e4","avatarUrl":"/avatars/2e48f052fdf37b5b06d101a6a3232eea.svg","fullname":"Ambroise Odonnat","name":"ambroiseodt","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"5f83c275f0801648bf88454a","name":"huawei-noah","fullname":"HUAWEI Noah's Ark Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.03548","authors":[{"_id":"6986d4b52d626112378ad65f","user":{"_id":"668547e921e9f68a7a2a6d18","avatarUrl":"/avatars/5251e5c52993c7e5fb23cb4afba03f50.svg","isPro":false,"fullname":"Yuqin Dai","user":"dayll","type":"user"},"name":"Yuqin Dai","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:34:30.731Z","hidden":false},{"_id":"6986d4b52d626112378ad660","user":{"_id":"64dad2dc00b80a024c5277e4","avatarUrl":"/avatars/10be4a8f9a9d36e231b427a4946de38a.svg","isPro":false,"fullname":"高宁","user":"Aoyama7Hai","type":"user"},"name":"Ning Gao","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:34:26.859Z","hidden":false},{"_id":"6986d4b52d626112378ad661","name":"Wei Zhang","hidden":false},{"_id":"6986d4b52d626112378ad662","name":"Jie Wang","hidden":false},{"_id":"6986d4b52d626112378ad663","name":"Zichen Luo","hidden":false},{"_id":"6986d4b52d626112378ad664","name":"Jinpeng Wang","hidden":false},{"_id":"6986d4b52d626112378ad665","name":"Yujie Wang","hidden":false},{"_id":"6986d4b52d626112378ad666","name":"Ruiyuan Wu","hidden":false},{"_id":"6986d4b52d626112378ad667","name":"Chaozheng Wang","hidden":false}],"publishedAt":"2026-02-03T14:01:11.000Z","submittedOnDailyAt":"2026-02-09T08:24:20.104Z","title":"SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue","submittedOnDailyBy":{"_id":"668547e921e9f68a7a2a6d18","avatarUrl":"/avatars/5251e5c52993c7e5fb23cb4afba03f50.svg","isPro":false,"fullname":"Yuqin Dai","user":"dayll","type":"user"},"summary":"Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.","upvotes":2,"discussionId":"6986d4b52d626112378ad668","githubRepo":"https://github.com/Da1yuqin/SEAD","githubRepoAddedBy":"user","ai_summary":"SEAD framework enables service dialogue agents to learn effective strategies through self-evolving user modeling components, achieving superior task completion and dialogue efficiency compared to existing foundation and commercial models.","ai_keywords":["large language models","service dialogues","user modeling","profile controller","user role-play model","task completion rate","dialogue efficiency"],"githubStars":16,"organization":{"_id":"6282108b1c4fdf630c7943a4","name":"meituan","fullname":"meituan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/4TJKRXMsRyZsyi4H3rWsh.jpeg"}},"publishedAt":"2026-02-03T09:01:11.000Z","title":"SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue","summary":"Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03548.png","numComments":2,"submittedBy":{"_id":"668547e921e9f68a7a2a6d18","avatarUrl":"/avatars/5251e5c52993c7e5fb23cb4afba03f50.svg","fullname":"Yuqin Dai","name":"dayll","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6282108b1c4fdf630c7943a4","name":"meituan","fullname":"meituan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/4TJKRXMsRyZsyi4H3rWsh.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.06724","authors":[{"_id":"6989aa63beecc443208d27ce","user":{"_id":"627ca439a09edfe62239c671","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg","isPro":false,"fullname":"Tian Lan","user":"GMFTBY","type":"user"},"name":"Tian Lan","status":"claimed_verified","statusLastChangedAt":"2026-02-09T14:31:30.189Z","hidden":false},{"_id":"6989aa63beecc443208d27cf","name":"Felix Henry","hidden":false},{"_id":"6989aa63beecc443208d27d0","name":"Bin Zhu","hidden":false},{"_id":"6989aa63beecc443208d27d1","name":"Qianghuai Jia","hidden":false},{"_id":"6989aa63beecc443208d27d2","name":"Junyang Ren","hidden":false},{"_id":"6989aa63beecc443208d27d3","name":"Qihang Pu","hidden":false},{"_id":"6989aa63beecc443208d27d4","name":"Haijun Li","hidden":false},{"_id":"6989aa63beecc443208d27d5","name":"Longyue Wang","hidden":false},{"_id":"6989aa63beecc443208d27d6","name":"Zhao Xu","hidden":false},{"_id":"6989aa63beecc443208d27d7","name":"Weihua Luo","hidden":false}],"publishedAt":"2026-02-06T14:18:26.000Z","submittedOnDailyAt":"2026-02-09T07:05:58.454Z","title":"Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion","submittedOnDailyBy":{"_id":"627ca439a09edfe62239c671","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg","isPro":false,"fullname":"Tian Lan","user":"GMFTBY","type":"user"},"summary":"Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce Table-as-Search (TaS), a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.","upvotes":1,"discussionId":"6989aa63beecc443208d27d8","githubRepo":"https://github.com/AIDC-AI/Marco-DeepResearch/","githubRepoAddedBy":"user","ai_summary":"Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management.","ai_keywords":["information seeking","table completion","search states","structured planning","deep search","wide search","deepwide search","multi-agent framework","search agent"],"githubStars":246,"organization":{"_id":"6662a91edd706a226d18cc5a","name":"AIDC-AI","fullname":"AIDC-AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/666a9d46a638e57bb7907929/CRc-9MCuH2q9hjTScyTPE.png"}},"publishedAt":"2026-02-06T09:18:26.000Z","title":"Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion","summary":"Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce Table-as-Search (TaS), a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06724.png","numComments":1,"submittedBy":{"_id":"627ca439a09edfe62239c671","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg","fullname":"Tian Lan","name":"GMFTBY","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":18,"isUserFollowing":false},"organization":{"_id":"6662a91edd706a226d18cc5a","name":"AIDC-AI","fullname":"AIDC-AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/666a9d46a638e57bb7907929/CRc-9MCuH2q9hjTScyTPE.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.06566","authors":[{"_id":"698a6aeb1b2dc6b37d61ae97","name":"Niccolo Avogaro","hidden":false},{"_id":"698a6aeb1b2dc6b37d61ae98","name":"Nayanika Debnath","hidden":false},{"_id":"698a6aeb1b2dc6b37d61ae99","name":"Li Mi","hidden":false},{"_id":"698a6aeb1b2dc6b37d61ae9a","name":"Thomas Frick","hidden":false},{"_id":"698a6aeb1b2dc6b37d61ae9b","name":"Junling Wang","hidden":false},{"_id":"698a6aeb1b2dc6b37d61ae9c","name":"Zexue He","hidden":false},{"_id":"698a6aeb1b2dc6b37d61ae9d","name":"Hang Hua","hidden":false},{"_id":"698a6aeb1b2dc6b37d61ae9e","name":"Konrad Schindler","hidden":false},{"_id":"698a6aeb1b2dc6b37d61ae9f","name":"Mattia Rigotti","hidden":false}],"publishedAt":"2026-02-06T10:05:25.000Z","submittedOnDailyAt":"2026-02-09T20:47:39.414Z","title":"SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs","submittedOnDailyBy":{"_id":"639f8277beb95d698de007dd","avatarUrl":"/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg","isPro":false,"fullname":"HangHua","user":"hhua2","type":"user"},"summary":"Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the V^* VQA benchmark by 6.7 percentage points, and it surpasses \"thinking with images\" by 4.6 points on a challenging OOD task despite requiring a 200times lower token budget.","upvotes":1,"discussionId":"698a6aeb1b2dc6b37d61aea0","ai_summary":"SPARC is a modular framework that decouples visual perception from reasoning in vision-language models, enabling efficient test-time scaling through targeted compute allocation and improved performance on visual reasoning tasks.","ai_keywords":["vision-language models","test-time scaling","visual perception","reasoning","visual search","sequential sensory-to-cognitive processing","two-stage pipeline","visual grounding","asymmetric compute allocation","token budget","visual reasoning benchmarks","Qwen3VL-4B","V* VQA benchmark","out-of-distribution tasks"],"organization":{"_id":"6760ab6c5c9a8ea8370ab95b","name":"ibm-research","fullname":"IBM Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/npxapKcW-cXX3J2JBl2vY.png"}},"publishedAt":"2026-02-06T05:05:25.000Z","title":"SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs","summary":"Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the V^* VQA benchmark by 6.7 percentage points, and it surpasses \"thinking with images\" by 4.6 points on a challenging OOD task despite requiring a 200times lower token budget.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06566.png","numComments":1,"submittedBy":{"_id":"639f8277beb95d698de007dd","avatarUrl":"/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg","fullname":"HangHua","name":"hhua2","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6760ab6c5c9a8ea8370ab95b","name":"ibm-research","fullname":"IBM Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/npxapKcW-cXX3J2JBl2vY.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06129","authors":[{"_id":"69899348beecc443208d2778","user":{"_id":"67abad973d3f1b93ddcab1f0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-pzH6sZ-ktqRUnsvqCh1-.jpeg","isPro":false,"fullname":"Gustav Olaf Yunus Laitinen-Fredriksson Imanov","user":"olaflaitinen","type":"user"},"name":"Olaf Yunus Laitinen Imanov","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:27:38.562Z","hidden":false},{"_id":"69899348beecc443208d2779","user":{"_id":"66a90fd92c7c3ebdd78e15a5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/e5wDS9fTbPR1mS-Hx3k_-.png","isPro":false,"fullname":"Derya Umut Kulali","user":"Japyh","type":"user"},"name":"Derya Umut Kulali","status":"claimed_verified","statusLastChangedAt":"2026-02-09T21:06:24.315Z","hidden":false},{"_id":"69899348beecc443208d277a","name":"Taner Yilmaz","hidden":false}],"publishedAt":"2026-02-05T19:01:56.000Z","submittedOnDailyAt":"2026-02-09T05:27:44.623Z","title":"Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction","submittedOnDailyBy":{"_id":"67abad973d3f1b93ddcab1f0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-pzH6sZ-ktqRUnsvqCh1-.jpeg","isPro":false,"fullname":"Gustav Olaf Yunus Laitinen-Fredriksson Imanov","user":"olaflaitinen","type":"user"},"summary":"Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.","upvotes":1,"discussionId":"69899349beecc443208d277b","ai_summary":"A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.","ai_keywords":["diffusion-transformer framework","spatio-temporal urban data","climate-risk indicators","transportation-network structure","accessibility signals","intelligent vehicles","hazard-conditioned routing","uncertainty-aware accessibility layers","prompt-based conditioning","cross-modal attention mechanism","counterfactual simulator","probabilistic risk trajectories","cross-city transfer","shared latent representation"]},"publishedAt":"2026-02-05T14:01:56.000Z","title":"Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction","summary":"Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06129.png","numComments":1,"submittedBy":{"_id":"67abad973d3f1b93ddcab1f0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-pzH6sZ-ktqRUnsvqCh1-.jpeg","fullname":"Gustav Olaf Yunus Laitinen-Fredriksson Imanov","name":"olaflaitinen","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":10,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.04454","authors":[{"_id":"6985bb7b4ad556f294b7ed95","user":{"_id":"66c98c27fafc0fc87c280749","avatarUrl":"/avatars/c71db3bee0fcd9aabcc38fd871d1cb75.svg","isPro":false,"fullname":"Tianming Liang","user":"liangtm","type":"user"},"name":"Tianming Liang","status":"claimed_verified","statusLastChangedAt":"2026-02-06T18:50:11.951Z","hidden":false},{"_id":"6985bb7b4ad556f294b7ed96","name":"Qirui Du","hidden":false},{"_id":"6985bb7b4ad556f294b7ed97","name":"Jian-Fang Hu","hidden":false},{"_id":"6985bb7b4ad556f294b7ed98","name":"Haichao Jiang","hidden":false},{"_id":"6985bb7b4ad556f294b7ed99","name":"Zicheng Lin","hidden":false},{"_id":"6985bb7b4ad556f294b7ed9a","name":"Wei-Shi Zheng","hidden":false}],"publishedAt":"2026-02-04T11:33:16.000Z","submittedOnDailyAt":"2026-02-09T05:01:54.003Z","title":"Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search","submittedOnDailyBy":{"_id":"66c98c27fafc0fc87c280749","avatarUrl":"/avatars/c71db3bee0fcd9aabcc38fd871d1cb75.svg","isPro":false,"fullname":"Tianming Liang","user":"liangtm","type":"user"},"summary":"Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.","upvotes":1,"discussionId":"6985bb7b4ad556f294b7ed9b","githubRepo":"https://github.com/iSEE-Laboratory/Seg-ReSearch","githubRepoAddedBy":"user","ai_summary":"Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.","ai_keywords":["multimodal large language models","segmentation","interleaved reasoning","external search","knowledge bottleneck","hierarchical reward design","video object segmentation","OK-VOS benchmark"],"githubStars":12,"organization":{"_id":"6884742647f9088912113d8d","name":"iSEE-Laboratory","fullname":"iSEE-Laboratory","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67067633351e0c16a5c27497/ldEqoVV-aKBa-UJWJnXwk.jpeg"}},"publishedAt":"2026-02-04T06:33:16.000Z","title":"Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search","summary":"Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04454.png","numComments":2,"submittedBy":{"_id":"66c98c27fafc0fc87c280749","avatarUrl":"/avatars/c71db3bee0fcd9aabcc38fd871d1cb75.svg","fullname":"Tianming Liang","name":"liangtm","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"6884742647f9088912113d8d","name":"iSEE-Laboratory","fullname":"iSEE-Laboratory","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67067633351e0c16a5c27497/ldEqoVV-aKBa-UJWJnXwk.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.03998","authors":[{"_id":"6984b1634ad556f294b7e974","name":"Ahmed Alagha","hidden":false},{"_id":"6984b1634ad556f294b7e975","name":"Christopher Leclerc","hidden":false},{"_id":"6984b1634ad556f294b7e976","user":{"_id":"6666cd296eac1502f2d9b64e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6666cd296eac1502f2d9b64e/xIA5phPnnhAn5QhDZ_WXs.jpeg","isPro":false,"fullname":"Yousef Kotp","user":"yousefkotp","type":"user"},"name":"Yousef Kotp","status":"claimed_verified","statusLastChangedAt":"2026-02-06T18:52:41.514Z","hidden":false},{"_id":"6984b1634ad556f294b7e977","name":"Omar Metwally","hidden":false},{"_id":"6984b1634ad556f294b7e978","name":"Calvin Moras","hidden":false},{"_id":"6984b1634ad556f294b7e979","name":"Peter Rentopoulos","hidden":false},{"_id":"6984b1634ad556f294b7e97a","name":"Ghodsiyeh Rostami","hidden":false},{"_id":"6984b1634ad556f294b7e97b","name":"Bich Ngoc Nguyen","hidden":false},{"_id":"6984b1634ad556f294b7e97c","name":"Jumanah Baig","hidden":false},{"_id":"6984b1634ad556f294b7e97d","name":"Abdelhakim Khellaf","hidden":false},{"_id":"6984b1634ad556f294b7e97e","name":"Vincent Quoc-Huy Trinh","hidden":false},{"_id":"6984b1634ad556f294b7e97f","name":"Rabeb Mizouni","hidden":false},{"_id":"6984b1634ad556f294b7e980","name":"Hadi Otrok","hidden":false},{"_id":"6984b1634ad556f294b7e981","name":"Jamal Bentahar","hidden":false},{"_id":"6984b1634ad556f294b7e982","name":"Mahdi S. Hosseini","hidden":false}],"publishedAt":"2026-02-03T20:32:07.000Z","submittedOnDailyAt":"2026-02-09T14:41:29.030Z","title":"AtlasPatch: An Efficient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology","submittedOnDailyBy":{"_id":"6666cd296eac1502f2d9b64e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6666cd296eac1502f2d9b64e/xIA5phPnnhAn5QhDZ_WXs.jpeg","isPro":false,"fullname":"Yousef Kotp","user":"yousefkotp","type":"user"},"summary":"Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction, is foundational to AI-driven computational pathology workflows. This remains a major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection, or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an efficient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatch's tissue detection module is trained on a heterogeneous and semi-manually annotated dataset of ~30,000 WSI thumbnails, using efficient fine-tuning of the Segment-Anything model. The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly into common image encoders for embedding or store patch images, all efficiently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning, matching state-of-the-art performance while operating at a fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch.","upvotes":1,"discussionId":"6984b1644ad556f294b7e983","ai_summary":"AtlasPatch is an efficient and scalable whole-slide image preprocessing framework that uses fine-tuned Segment-Anything model for accurate tissue detection and high-throughput patch extraction with reduced computational overhead.","ai_keywords":["Segment-Anything model","tissue detection","patch extraction","whole-slide image","multiple-instance learning","fine-tuning","computational pathology"],"organization":{"_id":"692db292467926dd781df748","name":"AtlasAnalyticsLab","fullname":"Atlas Analytics Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6666cd296eac1502f2d9b64e/EC41CycpL6b1UoVSwa485.png"}},"publishedAt":"2026-02-03T15:32:07.000Z","title":"AtlasPatch: An Efficient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology","summary":"Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction, is foundational to AI-driven computational pathology workflows. This remains a major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection, or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an efficient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatch's tissue detection module is trained on a heterogeneous and semi-manually annotated dataset of ~30,000 WSI thumbnails, using efficient fine-tuning of the Segment-Anything model. The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly into common image encoders for embedding or store patch images, all efficiently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning, matching state-of-the-art performance while operating at a fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03998.png","numComments":1,"submittedBy":{"_id":"6666cd296eac1502f2d9b64e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6666cd296eac1502f2d9b64e/xIA5phPnnhAn5QhDZ_WXs.jpeg","fullname":"Yousef Kotp","name":"yousefkotp","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"692db292467926dd781df748","name":"AtlasAnalyticsLab","fullname":"Atlas Analytics Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6666cd296eac1502f2d9b64e/EC41CycpL6b1UoVSwa485.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.01064","authors":[{"_id":"69895b69beecc443208d26ce","name":"Ruihan Jin","hidden":false},{"_id":"69895b69beecc443208d26cf","name":"Pengpeng Shao","hidden":false},{"_id":"69895b69beecc443208d26d0","name":"Zhengqi Wen","hidden":false},{"_id":"69895b69beecc443208d26d1","user":{"_id":"6747de57f8cab58c22ec94a2","avatarUrl":"/avatars/5bae0341862fac24564781c0fa32aac5.svg","isPro":false,"fullname":"Jinyang Wu","user":"Jinyang23","type":"user"},"name":"Jinyang Wu","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:29:25.039Z","hidden":false},{"_id":"69895b69beecc443208d26d2","name":"Mingkuan Feng","hidden":false},{"_id":"69895b69beecc443208d26d3","name":"Shuo Yang","hidden":false},{"_id":"69895b69beecc443208d26d4","name":"Chu Yuan Zhang","hidden":false},{"_id":"69895b69beecc443208d26d5","name":"Jianhua Tao","hidden":false}],"publishedAt":"2026-02-01T07:19:57.000Z","submittedOnDailyAt":"2026-02-09T01:29:12.554Z","title":"Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs","submittedOnDailyBy":{"_id":"6747de57f8cab58c22ec94a2","avatarUrl":"/avatars/5bae0341862fac24564781c0fa32aac5.svg","isPro":false,"fullname":"Jinyang Wu","user":"Jinyang23","type":"user"},"summary":"Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of Knowledge Purification, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.","upvotes":1,"discussionId":"69895b6abeecc443208d26d6","ai_summary":"Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.","ai_keywords":["knowledge distillation","large language models","teacher models","knowledge conflicts","knowledge purification","router-based methods"]},"publishedAt":"2026-02-01T02:19:57.000Z","title":"Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs","summary":"Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of Knowledge Purification, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01064.png","numComments":2,"submittedBy":{"_id":"6747de57f8cab58c22ec94a2","avatarUrl":"/avatars/5bae0341862fac24564781c0fa32aac5.svg","fullname":"Jinyang Wu","name":"Jinyang23","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.23039","authors":[{"_id":"6985bff8e387e657cdfcbce2","user":{"_id":"696c8dc1c083124a6e6bc0f4","avatarUrl":"/avatars/833e1cef1f077d2882669db63c259011.svg","isPro":false,"fullname":"YIZHI LIU","user":"leon0923","type":"user"},"name":"Yizhi Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-06T18:50:09.883Z","hidden":false}],"publishedAt":"2026-01-30T14:47:18.000Z","submittedOnDailyAt":"2026-02-09T03:17:27.263Z","title":"Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference","submittedOnDailyBy":{"_id":"696c8dc1c083124a6e6bc0f4","avatarUrl":"/avatars/833e1cef1f077d2882669db63c259011.svg","isPro":false,"fullname":"YIZHI LIU","user":"leon0923","type":"user"},"summary":"Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing εto 0 is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: Premature Mode Collapse. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/ε). To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law.","upvotes":1,"discussionId":"6985bff9e387e657cdfcbce3","ai_summary":"Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.","ai_keywords":["Optimal Transport","entropy-regularized Optimal Transport","Sinkhorn fixed-point map","mode collapse","adaptive scheduling algorithm","manifold-constrained hyper-connections","gradient explosions","stability analysis"]},"publishedAt":"2026-01-30T09:47:18.000Z","title":"Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference","summary":"Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing εto 0 is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: Premature Mode Collapse. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/ε). To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23039.png","numComments":2,"submittedBy":{"_id":"696c8dc1c083124a6e6bc0f4","avatarUrl":"/avatars/833e1cef1f077d2882669db63c259011.svg","fullname":"YIZHI LIU","name":"leon0923","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.06964","authors":[{"_id":"6989527fbeecc443208d2671","user":{"_id":"630e5cc8c6b1d1bccb816f44","avatarUrl":"/avatars/ef1aa94fc793c2eb6f345eab98ae9cd9.svg","isPro":false,"fullname":"Grace Luo","user":"g-luo","type":"user"},"name":"Grace Luo","status":"claimed_verified","statusLastChangedAt":"2026-02-09T21:06:52.171Z","hidden":false},{"_id":"6989527fbeecc443208d2672","name":"Jiahai Feng","hidden":false},{"_id":"6989527fbeecc443208d2673","name":"Trevor Darrell","hidden":false},{"_id":"6989527fbeecc443208d2674","name":"Alec Radford","hidden":false},{"_id":"6989527fbeecc443208d2675","name":"Jacob Steinhardt","hidden":false}],"publishedAt":"2026-02-06T18:59:56.000Z","submittedOnDailyAt":"2026-02-09T16:13:11.444Z","title":"Learning a Generative Meta-Model of LLM Activations","submittedOnDailyBy":{"_id":"630e5cc8c6b1d1bccb816f44","avatarUrl":"/avatars/ef1aa94fc793c2eb6f345eab98ae9cd9.svg","isPro":false,"fullname":"Grace Luo","user":"g-luo","type":"user"},"summary":"Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating \"meta-models\" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.","upvotes":0,"discussionId":"69895280beecc443208d2676","ai_summary":"Training diffusion models on neural network activations creates meta-models that learn internal state distributions and improve intervention fidelity without restrictive structural assumptions.","ai_keywords":["diffusion models","residual stream activations","meta-models","generative models","intervention fidelity","sparse probing scores"],"organization":{"_id":"698257037f99218c71de21f4","name":"generative-latent-prior","fullname":"Generative Latent Prior","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630e5cc8c6b1d1bccb816f44/RgCMAbr9BPPzR_Q5xCWz5.png"}},"publishedAt":"2026-02-06T13:59:56.000Z","title":"Learning a Generative Meta-Model of LLM Activations","summary":"Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating \"meta-models\" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06964.png","numComments":1,"submittedBy":{"_id":"630e5cc8c6b1d1bccb816f44","avatarUrl":"/avatars/ef1aa94fc793c2eb6f345eab98ae9cd9.svg","fullname":"Grace Luo","name":"g-luo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"698257037f99218c71de21f4","name":"generative-latent-prior","fullname":"Generative Latent Prior","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630e5cc8c6b1d1bccb816f44/RgCMAbr9BPPzR_Q5xCWz5.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.04811","authors":[{"_id":"698a2c901b2dc6b37d61ae22","name":"Jiarui Yuan","hidden":false},{"_id":"698a2c901b2dc6b37d61ae23","name":"Tailin Jin","hidden":false},{"_id":"698a2c901b2dc6b37d61ae24","name":"Weize Chen","hidden":false},{"_id":"698a2c901b2dc6b37d61ae25","name":"Zeyuan Liu","hidden":false},{"_id":"698a2c901b2dc6b37d61ae26","name":"Zhiyuan Liu","hidden":false},{"_id":"698a2c901b2dc6b37d61ae27","name":"Maosong Sun","hidden":false}],"publishedAt":"2026-02-04T17:58:32.000Z","submittedOnDailyAt":"2026-02-09T16:21:51.582Z","title":"SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization","submittedOnDailyBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","isPro":false,"fullname":"AK","user":"akhaliq","type":"user"},"summary":"True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.","upvotes":0,"discussionId":"698a2c901b2dc6b37d61ae28","ai_summary":"SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches.","ai_keywords":["self-evolution","lifelong learning","knowledge internalization","Open-Book Paradox","Closed-Book Training","RL Gap","PPO clipping","negative gradients","Self-Play","SFT"]},"publishedAt":"2026-02-04T12:58:32.000Z","title":"SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization","summary":"True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04811.png","numComments":1,"submittedBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","fullname":"AK","name":"akhaliq","type":"user","isPro":false,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":9133,"isUserFollowing":false},"isAuthorParticipating":false}]
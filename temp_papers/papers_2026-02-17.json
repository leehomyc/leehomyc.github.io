[{"paper":{"id":"2602.10809","authors":[{"_id":"698d8a1b65c0d15a6d1622db","user":{"_id":"654c99d6e82a71cb487c2ecd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/654c99d6e82a71cb487c2ecd/hiMMOyh-3bAUaqnBM5yT4.jpeg","isPro":false,"fullname":"ChenlongDeng","user":"ChenlongDeng","type":"user"},"name":"Chenlong Deng","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:37:24.735Z","hidden":false},{"_id":"698d8a1b65c0d15a6d1622dc","name":"Mengjie Deng","hidden":false},{"_id":"698d8a1b65c0d15a6d1622dd","name":"Junjie Wu","hidden":false},{"_id":"698d8a1b65c0d15a6d1622de","name":"Dun Zeng","hidden":false},{"_id":"698d8a1b65c0d15a6d1622df","name":"Teng Wang","hidden":false},{"_id":"698d8a1b65c0d15a6d1622e0","name":"Qingsong Xie","hidden":false},{"_id":"698d8a1b65c0d15a6d1622e1","user":{"_id":"6994899fe85a4b61cbc65813","avatarUrl":"/avatars/2b0fce4d53c59eb06ef0b023485ed81f.svg","isPro":false,"fullname":"Jiadeng Huang","user":"Warden-H","type":"user"},"name":"Jiadeng Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:50:53.691Z","hidden":false},{"_id":"698d8a1b65c0d15a6d1622e2","name":"Shengjie Ma","hidden":false},{"_id":"698d8a1b65c0d15a6d1622e3","name":"Changwang Zhang","hidden":false},{"_id":"698d8a1b65c0d15a6d1622e4","name":"Zhaoxiang Wang","hidden":false},{"_id":"698d8a1b65c0d15a6d1622e5","name":"Jun Wang","hidden":false},{"_id":"698d8a1b65c0d15a6d1622e6","name":"Yutao Zhu","hidden":false},{"_id":"698d8a1b65c0d15a6d1622e7","name":"Zhicheng Dou","hidden":false}],"publishedAt":"2026-02-11T12:51:10.000Z","submittedOnDailyAt":"2026-02-17T09:24:35.490Z","title":"DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories","submittedOnDailyBy":{"_id":"654c99d6e82a71cb487c2ecd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/654c99d6e82a71cb487c2ecd/hiMMOyh-3bAUaqnBM5yT4.jpeg","isPro":false,"fullname":"ChenlongDeng","user":"ChenlongDeng","type":"user"},"summary":"Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.","upvotes":29,"discussionId":"698d8a1b65c0d15a6d1622e8","githubRepo":"https://github.com/RUC-NLPIR/DeepImageSearch","githubRepoAddedBy":"user","ai_summary":"DeepImageSearch presents an agentic approach to image retrieval that addresses limitations of traditional semantic matching by enabling multi-step reasoning over visual histories through a modular agent framework with dual-memory system.","ai_keywords":["multimodal retrieval systems","visual streams","temporal sequences","agentic paradigm","image retrieval","visual history","contextual cues","DISBench","vision-language models","spatiotemporal associations","modular agent framework","dual-memory system","long-horizon navigation"],"githubStars":27,"organization":{"_id":"6695ed048765c1560ce56423","name":"RUC-NLPIR","fullname":"NLPIR Lab @ RUC","avatar":"https://cdn-uploads.huggingface.co/production/uploads/625e62452a7279d3c77b5c38/CBwmyPCRzm4rHTGWhiCzR.jpeg"}},"publishedAt":"2026-02-11T07:51:10.000Z","title":"DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories","summary":"Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10809.png","numComments":1,"submittedBy":{"_id":"654c99d6e82a71cb487c2ecd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/654c99d6e82a71cb487c2ecd/hiMMOyh-3bAUaqnBM5yT4.jpeg","fullname":"ChenlongDeng","name":"ChenlongDeng","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"6695ed048765c1560ce56423","name":"RUC-NLPIR","fullname":"NLPIR Lab @ RUC","avatar":"https://cdn-uploads.huggingface.co/production/uploads/625e62452a7279d3c77b5c38/CBwmyPCRzm4rHTGWhiCzR.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.13949","authors":[{"_id":"69941b5e50fb2c0be4783de6","user":{"_id":"62e1b3cb3eb0730f621a83f6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg","isPro":false,"fullname":"Taiwei Shi","user":"MaksimSTW","type":"user"},"name":"Taiwei Shi","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:08:55.504Z","hidden":false},{"_id":"69941b5e50fb2c0be4783de7","name":"Sihao Chen","hidden":false},{"_id":"69941b5e50fb2c0be4783de8","name":"Bowen Jiang","hidden":false},{"_id":"69941b5e50fb2c0be4783de9","user":{"_id":"64d660308ebc40443813f014","avatarUrl":"/avatars/516bb2d2383be99794e366dfb41636b6.svg","isPro":false,"fullname":"Linxin Song","user":"linxinso","type":"user"},"name":"Linxin Song","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:08:53.295Z","hidden":false},{"_id":"69941b5e50fb2c0be4783dea","name":"Longqi Yang","hidden":false},{"_id":"69941b5e50fb2c0be4783deb","name":"Jieyu Zhao","hidden":false}],"publishedAt":"2026-02-15T01:23:48.000Z","submittedOnDailyAt":"2026-02-17T05:14:40.358Z","title":"Experiential Reinforcement Learning","submittedOnDailyBy":{"_id":"62e1b3cb3eb0730f621a83f6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg","isPro":false,"fullname":"Taiwei Shi","user":"MaksimSTW","type":"user"},"summary":"Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.","upvotes":26,"discussionId":"69941b5f50fb2c0be4783dec","ai_summary":"Experiential Reinforcement Learning introduces an explicit experience-reflection-consolidation loop that improves learning efficiency and performance in sparse-reward environments by enabling structured behavioral revision without additional inference costs.","ai_keywords":["reinforcement learning","environmental feedback","policy training","self-reflection","behavioral revision","exploration","optimization"],"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}},"publishedAt":"2026-02-14T20:23:48.000Z","title":"Experiential Reinforcement Learning","summary":"Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13949.png","numComments":2,"submittedBy":{"_id":"62e1b3cb3eb0730f621a83f6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg","fullname":"Taiwei Shi","name":"MaksimSTW","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.14234","authors":[{"_id":"6993dd2150fb2c0be4783cfa","name":"Zheng Chu","hidden":false},{"_id":"6993dd2150fb2c0be4783cfb","name":"Xiao Wang","hidden":false},{"_id":"6993dd2150fb2c0be4783cfc","name":"Jack Hong","hidden":false},{"_id":"6993dd2150fb2c0be4783cfd","name":"Huiming Fan","hidden":false},{"_id":"6993dd2150fb2c0be4783cfe","name":"Yuqi Huang","hidden":false},{"_id":"6993dd2150fb2c0be4783cff","name":"Yue Yang","hidden":false},{"_id":"6993dd2150fb2c0be4783d00","name":"Guohai Xu","hidden":false},{"_id":"6993dd2150fb2c0be4783d01","user":{"_id":"63fc5b724c57549ad5e54558","avatarUrl":"/avatars/1374c1e8969533dd7543959666f16d1a.svg","isPro":false,"fullname":"Chenxiao Zhao","user":"ChenShawn","type":"user"},"name":"Chenxiao Zhao","status":"admin_assigned","statusLastChangedAt":"2026-02-17T17:17:24.619Z","hidden":false},{"_id":"6993dd2150fb2c0be4783d02","name":"Cheng Xiang","hidden":false},{"_id":"6993dd2150fb2c0be4783d03","name":"Shengchao Hu","hidden":false},{"_id":"6993dd2150fb2c0be4783d04","name":"Dongdong Kuang","hidden":false},{"_id":"6993dd2150fb2c0be4783d05","name":"Ming Liu","hidden":false},{"_id":"6993dd2150fb2c0be4783d06","name":"Bing Qin","hidden":false},{"_id":"6993dd2150fb2c0be4783d07","name":"Xing Yu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/647416300da364bd0d009d20/u9_Uoq5LuqIoPS71iQFsn.png"],"publishedAt":"2026-02-15T17:04:46.000Z","submittedOnDailyAt":"2026-02-17T00:50:04.355Z","title":"REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents","submittedOnDailyBy":{"_id":"647416300da364bd0d009d20","avatarUrl":"/avatars/0474223271835611522a4eb488816e28.svg","isPro":false,"fullname":"Xiao Wang","user":"CherryDurian","type":"user"},"summary":"Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.","upvotes":17,"discussionId":"6993dd2250fb2c0be4783d08","projectPage":"https://redsearchagent.github.io/index/","githubRepo":"https://github.com/RedSearchAgent/REDSearcher","githubRepoAddedBy":"user","ai_summary":"REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.","ai_keywords":["task synthesis","dual-constrained optimization","graph topology","evidence dispersion","tool-augmented queries","midtraining","atomic capabilities","knowledge","planning","function calling","local simulated environment","reinforcement learning","search agents","long-horizon tasks"],"githubStars":9,"organization":{"_id":"6312d9c3830f549852f8e500","name":"xiaohongshu","fullname":"Xiaohongshu"}},"publishedAt":"2026-02-15T12:04:46.000Z","title":"REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents","summary":"Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/647416300da364bd0d009d20/u9_Uoq5LuqIoPS71iQFsn.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14234.png","numComments":1,"submittedBy":{"_id":"647416300da364bd0d009d20","avatarUrl":"/avatars/0474223271835611522a4eb488816e28.svg","fullname":"Xiao Wang","name":"CherryDurian","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6312d9c3830f549852f8e500","name":"xiaohongshu","fullname":"Xiaohongshu"},"isAuthorParticipating":false},{"paper":{"id":"2602.14265","authors":[{"_id":"6994205e50fb2c0be4783df6","name":"Zachary Bamberger","hidden":false},{"_id":"6994205e50fb2c0be4783df7","user":{"_id":"6568ee159c96f1a47bea2d53","avatarUrl":"/avatars/e08cee5504dc55e4f7fa0ed5227f16d1.svg","isPro":false,"fullname":"Till Saenger","user":"TillRS","type":"user"},"name":"Till R. Saenger","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:08:48.423Z","hidden":false},{"_id":"6994205e50fb2c0be4783df8","name":"Gilad Morad","hidden":false},{"_id":"6994205e50fb2c0be4783df9","name":"Ofra Amir","hidden":false},{"_id":"6994205e50fb2c0be4783dfa","name":"Brandon M. Stewart","hidden":false},{"_id":"6994205e50fb2c0be4783dfb","name":"Amir Feder","hidden":false}],"publishedAt":"2026-02-15T18:29:54.000Z","submittedOnDailyAt":"2026-02-17T14:02:22.127Z","title":"STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts","submittedOnDailyBy":{"_id":"64802fb6c57f629056c59966","avatarUrl":"/avatars/d5ecabaceeba759969855acf512b6649.svg","isPro":false,"fullname":"Eilam Shapira","user":"EilamSha","type":"user"},"summary":"Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.","upvotes":16,"discussionId":"6994205e50fb2c0be4783dfc","githubRepo":"https://github.com/zbambergerNLP/state-of-thoughts","githubRepoAddedBy":"user","ai_summary":"STATe presents an interpretable inference-time compute method that uses discrete textual interventions to generate diverse, high-quality, and explainable text by searching over reasoning patterns rather than relying on stochastic sampling.","ai_keywords":["Best-of-N","Tree-of-Thoughts","high-temperature sampling","inference-time compute","textual interventions","action-guided generation","reasoning patterns","argument generation","action sequences","performance estimation"],"githubStars":4},"publishedAt":"2026-02-15T13:29:54.000Z","title":"STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts","summary":"Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14265.png","numComments":1,"submittedBy":{"_id":"64802fb6c57f629056c59966","avatarUrl":"/avatars/d5ecabaceeba759969855acf512b6649.svg","fullname":"Eilam Shapira","name":"EilamSha","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.14492","authors":[{"_id":"6993e4ac50fb2c0be4783d48","user":{"_id":"67acb56faee29e2e5c1c41ec","avatarUrl":"/avatars/b47094266fe70d10786da845ae2ade2c.svg","isPro":false,"fullname":"Jiahao Yuan","user":"Jhcircle","type":"user"},"name":"Jiahao Yuan","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:54:02.776Z","hidden":false},{"_id":"6993e4ac50fb2c0be4783d49","name":"Yike Xu","hidden":false},{"_id":"6993e4ac50fb2c0be4783d4a","name":"Jinyong Wen","hidden":false},{"_id":"6993e4ac50fb2c0be4783d4b","name":"Baokun Wang","hidden":false},{"_id":"6993e4ac50fb2c0be4783d4c","name":"Ziyi Gao","hidden":false},{"_id":"6993e4ac50fb2c0be4783d4d","name":"Xiaotong Lin","hidden":false},{"_id":"6993e4ac50fb2c0be4783d4e","name":"Yun Liu","hidden":false},{"_id":"6993e4ac50fb2c0be4783d4f","name":"Xing Fu","hidden":false},{"_id":"6993e4ac50fb2c0be4783d50","name":"Yu Cheng","hidden":false},{"_id":"6993e4ac50fb2c0be4783d51","name":"Yongchao Liu","hidden":false},{"_id":"6993e4ac50fb2c0be4783d52","name":"Weiqiang Wang","hidden":false},{"_id":"6993e4ac50fb2c0be4783d53","name":"Zhongle Xie","hidden":false}],"publishedAt":"2026-02-16T06:09:31.000Z","submittedOnDailyAt":"2026-02-17T04:09:21.228Z","title":"Query as Anchor: Scenario-Adaptive User Representation via Large Language Model","submittedOnDailyBy":{"_id":"67acb56faee29e2e5c1c41ec","avatarUrl":"/avatars/b47094266fe70d10786da845ae2ade2c.svg","isPro":false,"fullname":"Jiahao Yuan","user":"Jhcircle","type":"user"},"summary":"Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.","upvotes":15,"discussionId":"6993e4ac50fb2c0be4783d54","githubRepo":"https://github.com/JhCircle/Q-Anchor","githubRepoAddedBy":"user","ai_summary":"A novel framework called Query-as-Anchor is introduced that transforms user modeling from static encoding to dynamic, query-aware synthesis using large language models with specialized architectures and training methods.","ai_keywords":["UserU","Q-Anchor Embedding","dual-tower LLMs","joint contrastive-autoregressive optimization","Cluster-based Soft Prompt Tuning","KV-cache-accelerated inference"],"githubStars":2,"organization":{"_id":"67c1d682826160b28f778510","name":"antgroup","fullname":"Ant Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}},"publishedAt":"2026-02-16T01:09:31.000Z","title":"Query as Anchor: Scenario-Adaptive User Representation via Large Language Model","summary":"Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14492.png","numComments":1,"submittedBy":{"_id":"67acb56faee29e2e5c1c41ec","avatarUrl":"/avatars/b47094266fe70d10786da845ae2ade2c.svg","fullname":"Jiahao Yuan","name":"Jhcircle","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"67c1d682826160b28f778510","name":"antgroup","fullname":"Ant Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.14041","authors":[{"_id":"6993d8d450fb2c0be4783ccc","name":"Yuang Ai","hidden":false},{"_id":"6993d8d450fb2c0be4783ccd","name":"Jiaming Han","hidden":false},{"_id":"6993d8d450fb2c0be4783cce","name":"Shaobin Zhuang","hidden":false},{"_id":"6993d8d450fb2c0be4783ccf","name":"Weijia Mao","hidden":false},{"_id":"6993d8d450fb2c0be4783cd0","name":"Xuefeng Hu","hidden":false},{"_id":"6993d8d450fb2c0be4783cd1","name":"Ziyan Yang","hidden":false},{"_id":"6993d8d450fb2c0be4783cd2","name":"Zhenheng Yang","hidden":false},{"_id":"6993d8d450fb2c0be4783cd3","name":"Huaibo Huang","hidden":false},{"_id":"6993d8d450fb2c0be4783cd4","name":"Xiangyu Yue","hidden":false},{"_id":"6993d8d450fb2c0be4783cd5","name":"Hao Chen","hidden":false}],"publishedAt":"2026-02-15T08:09:05.000Z","submittedOnDailyAt":"2026-02-17T02:18:25.240Z","title":"BitDance: Scaling Autoregressive Generative Models with Binary Tokens","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to 2^{256} states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.","upvotes":15,"discussionId":"6993d8d550fb2c0be4783cd6","githubRepo":"https://github.com/shallowdream204/BitDance","githubRepoAddedBy":"user","ai_summary":"BitDance is a scalable autoregressive image generator that uses binary visual tokens and diffusion-based methods to achieve efficient high-resolution image generation with improved speed and performance.","ai_keywords":["autoregressive image generator","binary visual tokens","high-entropy binary latents","binary diffusion head","next-patch diffusion","diffusion models","FID","parameter-efficient","text-to-image generation","photorealistic images","image generation speedup"],"githubStars":153},"publishedAt":"2026-02-15T03:09:05.000Z","title":"BitDance: Scaling Autoregressive Generative Models with Binary Tokens","summary":"We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to 2^{256} states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14041.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":232,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07824","authors":[{"_id":"6994358150fb2c0be4783e7d","name":"Yiwei Qin","hidden":false},{"_id":"6994358150fb2c0be4783e7e","name":"Zhen Huang","hidden":false},{"_id":"6994358150fb2c0be4783e7f","name":"Tiantian Mi","hidden":false},{"_id":"6994358150fb2c0be4783e80","name":"Weiye Si","hidden":false},{"_id":"6994358150fb2c0be4783e81","name":"Chenyang Zhou","hidden":false},{"_id":"6994358150fb2c0be4783e82","name":"Qipeng Guo","hidden":false},{"_id":"6994358150fb2c0be4783e83","name":"Siyuan Feng","hidden":false},{"_id":"6994358150fb2c0be4783e84","name":"Pengfei Liu","hidden":false}],"publishedAt":"2026-02-08T05:06:34.000Z","submittedOnDailyAt":"2026-02-17T07:08:00.845Z","title":"Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training","submittedOnDailyBy":{"_id":"683ebf283d659b032f260d27","avatarUrl":"/avatars/4d34494c606b07b242313b4c0c6967eb.svg","isPro":true,"fullname":"SII-Tiantian Mi","user":"Mitiantian","type":"user"},"summary":"Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.\n  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.","upvotes":14,"discussionId":"6994358150fb2c0be4783e85","githubRepo":"https://github.com/GAIR-NLP/Data-Darwinism","githubRepoAddedBy":"user","ai_summary":"Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text.","ai_keywords":["data-model co-evolution","ten-level taxonomy","generative refinement","cognitive completion","foundation models","scientific literature","continued pre-training","domain-aligned tasks"],"githubStars":12,"organization":{"_id":"630bc2d186b8b9904c33ce1b","name":"GAIR","fullname":"SII - GAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}},"publishedAt":"2026-02-08T00:06:34.000Z","title":"Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training","summary":"Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.\n  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07824.png","numComments":1,"submittedBy":{"_id":"683ebf283d659b032f260d27","avatarUrl":"/avatars/4d34494c606b07b242313b4c0c6967eb.svg","fullname":"SII-Tiantian Mi","name":"Mitiantian","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"630bc2d186b8b9904c33ce1b","name":"GAIR","fullname":"SII - GAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14699","authors":[{"_id":"6993e6d950fb2c0be4783d7c","name":"Muzhi Chen","hidden":false},{"_id":"6993e6d950fb2c0be4783d7d","name":"Xuanhe Zhou","hidden":false},{"_id":"6993e6d950fb2c0be4783d7e","name":"Wei Zhou","hidden":false},{"_id":"6993e6d950fb2c0be4783d7f","name":"Bangrui Xu","hidden":false},{"_id":"6993e6d950fb2c0be4783d80","name":"Surui Tang","hidden":false},{"_id":"6993e6d950fb2c0be4783d81","name":"Guoliang Li","hidden":false},{"_id":"6993e6d950fb2c0be4783d82","name":"Bingsheng He","hidden":false},{"_id":"6993e6d950fb2c0be4783d83","name":"Yeye He","hidden":false},{"_id":"6993e6d950fb2c0be4783d84","name":"Yitong Song","hidden":false},{"_id":"6993e6d950fb2c0be4783d85","name":"Fan Wu","hidden":false}],"publishedAt":"2026-02-16T12:39:46.000Z","submittedOnDailyAt":"2026-02-17T01:28:08.285Z","title":"Qute: Towards Quantum-Native Database","submittedOnDailyBy":{"_id":"68216c63856b96f869d1d116","avatarUrl":"/avatars/f69026ca75377e6754ab3e317879e35a.svg","isPro":false,"fullname":"Wei Zhou","user":"weizhoudb","type":"user"},"summary":"This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute.","upvotes":13,"discussionId":"6993e6d950fb2c0be4783d86","githubRepo":"https://github.com/weAIDB/Qute","githubRepoAddedBy":"user","githubStars":6},"publishedAt":"2026-02-16T07:39:46.000Z","title":"Qute: Towards Quantum-Native Database","summary":"This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14699.png","numComments":1,"submittedBy":{"_id":"68216c63856b96f869d1d116","avatarUrl":"/avatars/f69026ca75377e6754ab3e317879e35a.svg","fullname":"Wei Zhou","name":"weizhoudb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.14367","authors":[{"_id":"699422d450fb2c0be4783e14","name":"Shuofei Qiao","hidden":false},{"_id":"699422d450fb2c0be4783e15","name":"Yunxiang Wei","hidden":false},{"_id":"699422d450fb2c0be4783e16","name":"Xuehai Wang","hidden":false},{"_id":"699422d450fb2c0be4783e17","name":"Bin Wu","hidden":false},{"_id":"699422d450fb2c0be4783e18","name":"Boyang Xue","hidden":false},{"_id":"699422d450fb2c0be4783e19","user":{"_id":"620b3bbb0668e435407c8d0a","avatarUrl":"/avatars/e0fccbb2577d76088e09f054c35cffbc.svg","isPro":true,"fullname":"Ningyu Zhang","user":"Ningyu","type":"user"},"name":"Ningyu Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:08:45.960Z","hidden":false},{"_id":"699422d450fb2c0be4783e1a","name":"Hossein A. Rahmani","hidden":false},{"_id":"699422d450fb2c0be4783e1b","name":"Yanshan Wang","hidden":false},{"_id":"699422d450fb2c0be4783e1c","name":"Qiang Zhang","hidden":false},{"_id":"699422d450fb2c0be4783e1d","name":"Keyan Ding","hidden":false},{"_id":"699422d450fb2c0be4783e1e","name":"Jeff Z. Pan","hidden":false},{"_id":"699422d450fb2c0be4783e1f","name":"Huajun Chen","hidden":false},{"_id":"699422d450fb2c0be4783e20","name":"Emine Yilmaz","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6447800f30fa4ecb85ddad80/57QqW-zf15OPtwtDGuVa7.qt"],"publishedAt":"2026-02-16T00:40:31.000Z","submittedOnDailyAt":"2026-02-17T06:09:43.596Z","title":"InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem","submittedOnDailyBy":{"_id":"6447800f30fa4ecb85ddad80","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6447800f30fa4ecb85ddad80/NsmXIaMsWctmTNA7tFVkX.jpeg","isPro":false,"fullname":"Shuofei Qiao","user":"GoooDte","type":"user"},"summary":"The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.","upvotes":13,"discussionId":"699422d550fb2c0be4783e21","projectPage":"http://innoeval.zjukg.cn/","githubRepo":"https://github.com/zjunlp/InnoEval","githubRepoAddedBy":"user","ai_summary":"InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation.","ai_keywords":["Large Language Models","idea evaluation","deep innovation evaluation framework","knowledge-grounded reasoning","multi-perspective reasoning","heterogeneous deep knowledge search engine","innovation review board","multi-dimensional decoupled evaluation","peer-reviewed submissions"],"githubStars":9,"organization":{"_id":"6464cc20860abf030496e986","name":"UniversityCollegeLondon","fullname":"University College London","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6464cadff072e09adec1c372/OszOYithNLCeeNuMIjaoI.png"}},"publishedAt":"2026-02-15T19:40:31.000Z","title":"InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem","summary":"The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6447800f30fa4ecb85ddad80/57QqW-zf15OPtwtDGuVa7.qt"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14367.png","numComments":1,"submittedBy":{"_id":"6447800f30fa4ecb85ddad80","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6447800f30fa4ecb85ddad80/NsmXIaMsWctmTNA7tFVkX.jpeg","fullname":"Shuofei Qiao","name":"GoooDte","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"6464cc20860abf030496e986","name":"UniversityCollegeLondon","fullname":"University College London","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6464cadff072e09adec1c372/OszOYithNLCeeNuMIjaoI.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.13367","authors":[{"_id":"699431ed50fb2c0be4783e65","name":"Chen Yang","hidden":false},{"_id":"699431ed50fb2c0be4783e66","name":"Guangyue Peng","hidden":false},{"_id":"699431ed50fb2c0be4783e67","name":"Jiaying Zhu","hidden":false},{"_id":"699431ed50fb2c0be4783e68","name":"Ran Le","hidden":false},{"_id":"699431ed50fb2c0be4783e69","name":"Ruixiang Feng","hidden":false},{"_id":"699431ed50fb2c0be4783e6a","name":"Tao Zhang","hidden":false},{"_id":"699431ed50fb2c0be4783e6b","name":"Xiyun Xu","hidden":false},{"_id":"699431ed50fb2c0be4783e6c","name":"Yang Song","hidden":false},{"_id":"699431ed50fb2c0be4783e6d","name":"Yiming Jia","hidden":false},{"_id":"699431ed50fb2c0be4783e6e","name":"Yuntao Wen","hidden":false},{"_id":"699431ed50fb2c0be4783e6f","name":"Yunzhi Xu","hidden":false},{"_id":"699431ed50fb2c0be4783e70","name":"Zekai Wang","hidden":false},{"_id":"699431ed50fb2c0be4783e71","name":"Zhenwei An","hidden":false},{"_id":"699431ed50fb2c0be4783e72","name":"Zhicong Sun","hidden":false},{"_id":"699431ed50fb2c0be4783e73","name":"Zongchao Chen","hidden":false}],"publishedAt":"2026-02-13T13:10:46.000Z","submittedOnDailyAt":"2026-02-17T08:33:06.878Z","title":"Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts","submittedOnDailyBy":{"_id":"6947f69751d7ae7c3c7b6908","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PuIDZB9XDShHohKhYmdmp.png","isPro":true,"fullname":"Ben Kelly","user":"YellowjacketGames","type":"user"},"summary":"We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.","upvotes":13,"discussionId":"699431ed50fb2c0be4783e74","projectPage":"https://huggingface.co/Nanbeige/Nanbeige4.1-3B","ai_summary":"Nanbeige4.1-3B is a 3B-parameter unified language model that demonstrates superior performance in agentic behavior, code generation, and reasoning compared to larger models through advanced reward modeling and training techniques.","ai_keywords":["unified generalist language model","reward modeling","reinforcement learning","tool-call turns","deep search","complex data synthesis","turn-level supervision","point-wise reward modeling","pair-wise reward modeling","code generation","general reasoning","agentic behavior","human-aligned responses","model optimization"],"organization":{"_id":"6533c00a9860c1cb37bff25f","name":"Nanbeige","fullname":"Nanbeige LLM Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/646f0d118ff94af23bc44aab/GXHCollpMRgvYqUXQ2BQ7.png"}},"publishedAt":"2026-02-13T08:10:46.000Z","title":"Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts","summary":"We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13367.png","numComments":1,"submittedBy":{"_id":"6947f69751d7ae7c3c7b6908","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PuIDZB9XDShHohKhYmdmp.png","fullname":"Ben Kelly","name":"YellowjacketGames","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"organization":{"_id":"6533c00a9860c1cb37bff25f","name":"Nanbeige","fullname":"Nanbeige LLM Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/646f0d118ff94af23bc44aab/GXHCollpMRgvYqUXQ2BQ7.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14178","authors":[{"_id":"6993e66350fb2c0be4783d6b","name":"Shaobin Zhuang","hidden":false},{"_id":"6993e66350fb2c0be4783d6c","name":"Yuang Ai","hidden":false},{"_id":"6993e66350fb2c0be4783d6d","name":"Jiaming Han","hidden":false},{"_id":"6993e66350fb2c0be4783d6e","name":"Weijia Mao","hidden":false},{"_id":"6993e66350fb2c0be4783d6f","name":"Xiaohui Li","hidden":false},{"_id":"6993e66350fb2c0be4783d70","name":"Fangyikang Wang","hidden":false},{"_id":"6993e66350fb2c0be4783d71","name":"Xiao Wang","hidden":false},{"_id":"6993e66350fb2c0be4783d72","name":"Yan Li","hidden":false},{"_id":"6993e66350fb2c0be4783d73","name":"Shanchuan Lin","hidden":false},{"_id":"6993e66350fb2c0be4783d74","name":"Kun Xu","hidden":false},{"_id":"6993e66350fb2c0be4783d75","name":"Zhenheng Yang","hidden":false},{"_id":"6993e66350fb2c0be4783d76","name":"Huaibo Huang","hidden":false},{"_id":"6993e66350fb2c0be4783d77","name":"Xiangyu Yue","hidden":false},{"_id":"6993e66350fb2c0be4783d78","name":"Hao Chen","hidden":false},{"_id":"6993e66350fb2c0be4783d79","name":"Yali Wang","hidden":false}],"publishedAt":"2026-02-15T15:07:19.000Z","submittedOnDailyAt":"2026-02-17T01:24:15.735Z","title":"UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook (2^{128}). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.","upvotes":9,"discussionId":"6993e66350fb2c0be4783d7a","ai_summary":"UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.","ai_keywords":["visual tokenizers","discrete tokenizer","binary codebook","Pre-Post Distillation","Generative-Aware Prior","convolution-attention hybrid architecture","SigLu activation function","token entropy loss","commitment loss","three-stage training framework","multimodal understanding","image generation","DPG Score","GEdit Overall Score"],"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}},"publishedAt":"2026-02-15T10:07:19.000Z","title":"UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model","summary":"Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook (2^{128}). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14178.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":232,"isUserFollowing":false},"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.11574","authors":[{"_id":"698f24193ae80e6a12af8e20","user":{"_id":"657a33bb06e44e4565422dfa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/657a33bb06e44e4565422dfa/WA1wznlOFKpmmsePIDnzq.jpeg","isPro":false,"fullname":"Aditya Taparia","user":"aditya-taparia","type":"user"},"name":"Aditya Taparia","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:51:35.020Z","hidden":false},{"_id":"698f24193ae80e6a12af8e21","name":"Som Sagar","hidden":false},{"_id":"698f24193ae80e6a12af8e22","name":"Ransalu Senanayake","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/657a33bb06e44e4565422dfa/xKbDrhSoOKsD1Koy4-Dvo.jpeg"],"publishedAt":"2026-02-12T04:45:44.000Z","submittedOnDailyAt":"2026-02-17T17:00:55.832Z","title":"Learning to Configure Agentic AI Systems","submittedOnDailyBy":{"_id":"657a33bb06e44e4565422dfa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/657a33bb06e44e4565422dfa/WA1wznlOFKpmmsePIDnzq.jpeg","isPro":false,"fullname":"Aditya Taparia","user":"aditya-taparia","type":"user"},"summary":"Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to \"one size fits all\" designs.","upvotes":7,"discussionId":"698f241a3ae80e6a12af8e23","githubRepo":"https://github.com/somsagar07/Context_Optimization","githubRepoAddedBy":"user","ai_summary":"Learning per-query agent configurations through reinforcement learning improves task accuracy while reducing computational costs compared to fixed templates and hand-tuned heuristics.","ai_keywords":["LLM-based agent systems","reinforcement learning","hierarchical policy","query-wise decision problem","agent configuration","token budget","prompt engineering","tool-augmented question answering","reasoning tasks","task accuracy","computational efficiency"],"githubStars":2,"organization":{"_id":"6994c024db3cbf241bd24b0b","name":"lens-lab-AI","fullname":"LENS Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/657a33bb06e44e4565422dfa/hOk6Tv7V7OSECOvyk_lOU.webp"}},"publishedAt":"2026-02-11T23:45:44.000Z","title":"Learning to Configure Agentic AI Systems","summary":"Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to \"one size fits all\" designs.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/657a33bb06e44e4565422dfa/xKbDrhSoOKsD1Koy4-Dvo.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11574.png","numComments":1,"submittedBy":{"_id":"657a33bb06e44e4565422dfa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/657a33bb06e44e4565422dfa/WA1wznlOFKpmmsePIDnzq.jpeg","fullname":"Aditya Taparia","name":"aditya-taparia","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6994c024db3cbf241bd24b0b","name":"lens-lab-AI","fullname":"LENS Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/657a33bb06e44e4565422dfa/hOk6Tv7V7OSECOvyk_lOU.webp"},"isAuthorParticipating":true},{"paper":{"id":"2602.13823","authors":[{"_id":"6993db6a50fb2c0be4783ce8","name":"Haonan Jiang","hidden":false},{"_id":"6993db6a50fb2c0be4783ce9","user":{"_id":"659bb678e57c59004625c624","avatarUrl":"/avatars/32b395c3504acb1fe29cceb65508b351.svg","isPro":false,"fullname":"Voyage_Wang","user":"VoyageWang","type":"user"},"name":"Yuji Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:53:53.575Z","hidden":false},{"_id":"6993db6a50fb2c0be4783cea","name":"Yongjie Zhu","hidden":false},{"_id":"6993db6a50fb2c0be4783ceb","name":"Xin Lu","hidden":false},{"_id":"6993db6a50fb2c0be4783cec","name":"Wenyu Qin","hidden":false},{"_id":"6993db6a50fb2c0be4783ced","name":"Meng Wang","hidden":false},{"_id":"6993db6a50fb2c0be4783cee","name":"Pengfei Wan","hidden":false},{"_id":"6993db6a50fb2c0be4783cef","name":"Yansong Tang","hidden":false}],"publishedAt":"2026-02-14T15:35:03.000Z","submittedOnDailyAt":"2026-02-17T00:39:51.814Z","title":"Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings","submittedOnDailyBy":{"_id":"659bb678e57c59004625c624","avatarUrl":"/avatars/32b395c3504acb1fe29cceb65508b351.svg","isPro":false,"fullname":"Voyage_Wang","user":"VoyageWang","type":"user"},"summary":"Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.","upvotes":6,"discussionId":"6993db6a50fb2c0be4783cf0","githubRepo":"https://github.com/ZoengHN/Embed-RL","githubRepoAddedBy":"user","ai_summary":"A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance.","ai_keywords":["Multimodal Large Language Models","Universal Multimodal Embeddings","Chain-of-Thought reasoning","Embedder-Guided Reinforcement Learning","Reasoner","Traceability CoT","cross-modal tasks","multimodal cues","retrieval-oriented alignment","fine-grained matching"],"githubStars":4,"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"}},"publishedAt":"2026-02-14T10:35:03.000Z","title":"Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings","summary":"Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13823.png","numComments":1,"submittedBy":{"_id":"659bb678e57c59004625c624","avatarUrl":"/avatars/32b395c3504acb1fe29cceb65508b351.svg","fullname":"Voyage_Wang","name":"VoyageWang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"},"isAuthorParticipating":true},{"paper":{"id":"2602.12876","authors":[{"_id":"6993e3db50fb2c0be4783d2d","name":"Huanyao Zhang","hidden":false},{"_id":"6993e3db50fb2c0be4783d2e","name":"Jiepeng Zhou","hidden":false},{"_id":"6993e3db50fb2c0be4783d2f","name":"Bo Li","hidden":false},{"_id":"6993e3db50fb2c0be4783d30","name":"Bowen Zhou","hidden":false},{"_id":"6993e3db50fb2c0be4783d31","name":"Yanzhe Dan","hidden":false},{"_id":"6993e3db50fb2c0be4783d32","name":"Haishan Lu","hidden":false},{"_id":"6993e3db50fb2c0be4783d33","name":"Zhiyong Cao","hidden":false},{"_id":"6993e3db50fb2c0be4783d34","name":"Jiaoyang Chen","hidden":false},{"_id":"6993e3db50fb2c0be4783d35","name":"Yuqian Han","hidden":false},{"_id":"6993e3db50fb2c0be4783d36","name":"Zinan Sheng","hidden":false},{"_id":"6993e3db50fb2c0be4783d37","name":"Zhengwei Tao","hidden":false},{"_id":"6993e3db50fb2c0be4783d38","name":"Hao Liang","hidden":false},{"_id":"6993e3db50fb2c0be4783d39","name":"Jialong Wu","hidden":false},{"_id":"6993e3db50fb2c0be4783d3a","name":"Yang Shi","hidden":false},{"_id":"6993e3db50fb2c0be4783d3b","name":"Yuanpeng He","hidden":false},{"_id":"6993e3db50fb2c0be4783d3c","name":"Jiaye Lin","hidden":false},{"_id":"6993e3db50fb2c0be4783d3d","name":"Qintong Zhang","hidden":false},{"_id":"6993e3db50fb2c0be4783d3e","name":"Guochen Yan","hidden":false},{"_id":"6993e3db50fb2c0be4783d3f","name":"Runhao Zhao","hidden":false},{"_id":"6993e3db50fb2c0be4783d40","name":"Zhengpin Li","hidden":false},{"_id":"6993e3db50fb2c0be4783d41","name":"Xiaohan Yu","hidden":false},{"_id":"6993e3db50fb2c0be4783d42","name":"Lang Mei","hidden":false},{"_id":"6993e3db50fb2c0be4783d43","name":"Chong Chen","hidden":false},{"_id":"6993e3db50fb2c0be4783d44","name":"Wentao Zhang","hidden":false},{"_id":"6993e3db50fb2c0be4783d45","name":"Bin Cui","hidden":false}],"publishedAt":"2026-02-13T12:25:13.000Z","submittedOnDailyAt":"2026-02-17T01:13:32.734Z","title":"BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-V^3, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.","upvotes":6,"discussionId":"6993e3db50fb2c0be4783d46","ai_summary":"A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities.","ai_keywords":["multimodal large language models","multimodal browsing","deep search","web browsing","multimodal information integration","fine-grained perception","multimodal browsing agent framework","web search","visual perception"]},"publishedAt":"2026-02-13T07:25:13.000Z","title":"BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents","summary":"Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-V^3, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12876.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":232,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.14721","authors":[{"_id":"6993ef8a50fb2c0be4783d88","name":"Zikai Xiao","hidden":false},{"_id":"6993ef8a50fb2c0be4783d89","name":"Jianhong Tu","hidden":false},{"_id":"6993ef8a50fb2c0be4783d8a","name":"Chuhang Zou","hidden":false},{"_id":"6993ef8a50fb2c0be4783d8b","name":"Yuxin Zuo","hidden":false},{"_id":"6993ef8a50fb2c0be4783d8c","name":"Zhi Li","hidden":false},{"_id":"6993ef8a50fb2c0be4783d8d","name":"Peng Wang","hidden":false},{"_id":"6993ef8a50fb2c0be4783d8e","name":"Bowen Yu","hidden":false},{"_id":"6993ef8a50fb2c0be4783d8f","name":"Fei Huang","hidden":false},{"_id":"6993ef8a50fb2c0be4783d90","name":"Junyang Lin","hidden":false},{"_id":"6993ef8a50fb2c0be4783d91","name":"Zuozhu Liu","hidden":false}],"publishedAt":"2026-02-16T13:06:49.000Z","submittedOnDailyAt":"2026-02-17T02:03:22.564Z","title":"WebWorld: A Large-Scale World Model for Web Agent Training","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce WebWorld series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.","upvotes":5,"discussionId":"6993ef8a50fb2c0be4783d92","ai_summary":"WebWorld is an open-web simulator trained on over one million interactions that supports long-horizon reasoning and multi-format data, achieving performance comparable to advanced models like Gemini-3-Pro and GPT-4o.","ai_keywords":["web simulator","open-web environment","intrinsic evaluation","extrinsic evaluation","world model","inference-time search","cross-domain generalization"],"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}},"publishedAt":"2026-02-16T08:06:49.000Z","title":"WebWorld: A Large-Scale World Model for Web Agent Training","summary":"Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce WebWorld series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14721.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":232,"isUserFollowing":false},"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.13195","authors":[{"_id":"699410d550fb2c0be4783de2","name":"Aadarsh Sahoo","hidden":false},{"_id":"699410d550fb2c0be4783de3","name":"Georgia Gkioxari","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/4LGL0aqBQfh4P-xX35B-1.qt"],"publishedAt":"2026-02-13T18:58:30.000Z","submittedOnDailyAt":"2026-02-17T04:27:44.786Z","title":"Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision","submittedOnDailyBy":{"_id":"638e5fc6485360fbdfeb1301","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png","isPro":true,"fullname":"Aadarsh Sahoo","user":"aadarsh99","type":"user"},"summary":"Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., \"left-most apple\") and overlooks functional and physical reasoning (e.g., \"where can I safely store the knife?\"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/","upvotes":4,"discussionId":"699410d550fb2c0be4783de4","projectPage":"https://glab-caltech.github.io/converseg/","githubRepo":"https://github.com/AadSah/ConverSeg","githubRepoAddedBy":"user","ai_summary":"Conversational image segmentation addresses functional and physical reasoning tasks by introducing a new benchmark and model that combines segmentation priors with language understanding.","ai_keywords":["conversational image segmentation","referring image grounding","language-guided segmentation","segmentation priors","language understanding","AI-powered data engine","prompt-mask pairs"],"githubStars":9,"organization":{"_id":"68489d5fef240a91f79ba016","name":"caltech","fullname":"California institute of technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68489cc6e0f0802846902f01/RnxRgxa3Oc-31kS597SMI.png"}},"publishedAt":"2026-02-13T13:58:30.000Z","title":"Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision","summary":"Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., \"left-most apple\") and overlooks functional and physical reasoning (e.g., \"where can I safely store the knife?\"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/4LGL0aqBQfh4P-xX35B-1.qt"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13195.png","numComments":1,"submittedBy":{"_id":"638e5fc6485360fbdfeb1301","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png","fullname":"Aadarsh Sahoo","name":"aadarsh99","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"68489d5fef240a91f79ba016","name":"caltech","fullname":"California institute of technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68489cc6e0f0802846902f01/RnxRgxa3Oc-31kS597SMI.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14147","authors":[{"_id":"6993e08850fb2c0be4783d1a","name":"Shufan Li","hidden":false},{"_id":"6993e08850fb2c0be4783d1b","name":"Yuchen Zhu","hidden":false},{"_id":"6993e08850fb2c0be4783d1c","name":"Jiuxiang Gu","hidden":false},{"_id":"6993e08850fb2c0be4783d1d","name":"Kangning Liu","hidden":false},{"_id":"6993e08850fb2c0be4783d1e","name":"Zhe Lin","hidden":false},{"_id":"6993e08850fb2c0be4783d1f","name":"Yongxin Chen","hidden":false},{"_id":"6993e08850fb2c0be4783d20","name":"Molei Tao","hidden":false},{"_id":"6993e08850fb2c0be4783d21","name":"Aditya Grover","hidden":false},{"_id":"6993e08850fb2c0be4783d22","name":"Jason Kuen","hidden":false}],"publishedAt":"2026-02-15T13:52:45.000Z","submittedOnDailyAt":"2026-02-17T00:59:50.962Z","title":"LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models","submittedOnDailyBy":{"_id":"6310531914aa81e1044363ed","avatarUrl":"/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg","isPro":false,"fullname":"Shufan Li","user":"jacklishufan","type":"user"},"summary":"Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.","upvotes":3,"discussionId":"6993e08850fb2c0be4783d23","ai_summary":"LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks.","ai_keywords":["diffusion language models","multimodal understanding","multimodal generation","unified post-training framework","supervised fine-tuning","multi-task reinforcement learning","answer-forcing","tree search","complementary likelihood estimation","visual math reasoning","reason-intensive grounding","image editing"],"organization":{"_id":"61e5d14f77496de0a6d95c6b","name":"adobe","fullname":"Adobe","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}},"publishedAt":"2026-02-15T08:52:45.000Z","title":"LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models","summary":"Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14147.png","numComments":1,"submittedBy":{"_id":"6310531914aa81e1044363ed","avatarUrl":"/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg","fullname":"Shufan Li","name":"jacklishufan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"61e5d14f77496de0a6d95c6b","name":"adobe","fullname":"Adobe","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.13344","authors":[{"_id":"6993e59450fb2c0be4783d56","name":"Super Intelligence Team","hidden":false},{"_id":"6993e59450fb2c0be4783d57","name":"Changhao Qiao","hidden":false},{"_id":"6993e59450fb2c0be4783d58","name":"Chao Hui","hidden":false},{"_id":"6993e59450fb2c0be4783d59","name":"Chen Li","hidden":false},{"_id":"6993e59450fb2c0be4783d5a","name":"Cunzheng Wang","hidden":false},{"_id":"6993e59450fb2c0be4783d5b","name":"Dejia Song","hidden":false},{"_id":"6993e59450fb2c0be4783d5c","name":"Jiale Zhang","hidden":false},{"_id":"6993e59450fb2c0be4783d5d","name":"Jing Li","hidden":false},{"_id":"6993e59450fb2c0be4783d5e","name":"Qiang Xiang","hidden":false},{"_id":"6993e59450fb2c0be4783d5f","name":"Runqi Wang","hidden":false},{"_id":"6993e59450fb2c0be4783d60","name":"Shuang Sun","hidden":false},{"_id":"6993e59450fb2c0be4783d61","name":"Wei Zhu","hidden":false},{"_id":"6993e59450fb2c0be4783d62","name":"Xu Tang","hidden":false},{"_id":"6993e59450fb2c0be4783d63","name":"Yao Hu","hidden":false},{"_id":"6993e59450fb2c0be4783d64","name":"Yibo Chen","hidden":false},{"_id":"6993e59450fb2c0be4783d65","name":"Yuhao Huang","hidden":false},{"_id":"6993e59450fb2c0be4783d66","name":"Yuxuan Duan","hidden":false},{"_id":"6993e59450fb2c0be4783d67","name":"Zhiyi Chen","hidden":false},{"_id":"6993e59450fb2c0be4783d68","name":"Ziyuan Guo","hidden":false}],"publishedAt":"2026-02-12T17:51:44.000Z","submittedOnDailyAt":"2026-02-17T01:21:03.585Z","title":"FireRed-Image-Edit-1.0 Techinical Report","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.","upvotes":3,"discussionId":"6993e59550fb2c0be4783d69","projectPage":"https://huggingface.co/spaces/FireRedTeam/FireRed-Image-Edit-1.0","githubRepo":"https://github.com/FireRedTeam/FireRed-Image-Edit","githubRepoAddedBy":"user","ai_summary":"FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability.","ai_keywords":["diffusion transformer","data curation","training methodology","evaluation design","text-to-image","image editing","multi-condition aware bucket sampler","stochastic instruction alignment","asymmetric gradient optimization","DPO","diffusionNFT","layout-aware OCR rewards","differentiable consistency loss","REDEdit-Bench","ImgEdit","GEdit"],"githubStars":390},"publishedAt":"2026-02-12T12:51:44.000Z","title":"FireRed-Image-Edit-1.0 Techinical Report","summary":"We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13344.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":232,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.14534","authors":[{"_id":"6993fab150fb2c0be4783d9a","name":"Hongpeng Wang","hidden":false},{"_id":"6993fab150fb2c0be4783d9b","name":"Zeyu Zhang","hidden":false},{"_id":"6993fab150fb2c0be4783d9c","name":"Wenhao Li","hidden":false},{"_id":"6993fab150fb2c0be4783d9d","name":"Hao Tang","hidden":false}],"publishedAt":"2026-02-16T07:42:45.000Z","submittedOnDailyAt":"2026-02-17T02:51:29.947Z","title":"MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation","submittedOnDailyBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"summary":"Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.","upvotes":2,"discussionId":"6993fab150fb2c0be4783d9e","projectPage":"https://aigeeksgroup.github.io/MoRL/","githubRepo":"https://github.com/AIGeeksGroup/MoRL","githubRepoAddedBy":"user","ai_summary":"A unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards improves human motion understanding and generation through semantic alignment, reasoning coherence, and physical plausibility.","ai_keywords":["multimodal motion model","supervised fine-tuning","reinforcement learning","verifiable rewards","semantic alignment","reasoning coherence","physical plausibility","text-motion consistency","Chain-of-Motion","CoT datasets","HumanML3D","KIT-ML"],"githubStars":1,"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}},"publishedAt":"2026-02-16T02:42:45.000Z","title":"MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation","summary":"Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14534.png","numComments":1,"submittedBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","fullname":"Zeyu Zhang","name":"SteveZeyuZhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14060","authors":[{"_id":"6994768e046aed98c9b8fafb","user":{"_id":"6191cc9e6d34e827404cebab","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674119843175-6191cc9e6d34e827404cebab.jpeg","isPro":false,"fullname":"Yang","user":"jacklanda","type":"user"},"name":"Yang Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:08:23.567Z","hidden":false},{"_id":"6994768e046aed98c9b8fafc","name":"Jiaye Yang","hidden":false},{"_id":"6994768e046aed98c9b8fafd","name":"Weikang Li","hidden":false},{"_id":"6994768e046aed98c9b8fafe","name":"Jiahui Liang","hidden":false},{"_id":"6994768e046aed98c9b8faff","name":"Yang Li","hidden":false},{"_id":"6994768e046aed98c9b8fb00","name":"Lingyong Yan","hidden":false}],"publishedAt":"2026-02-15T09:18:22.000Z","submittedOnDailyAt":"2026-02-17T11:41:11.091Z","title":"LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts","submittedOnDailyBy":{"_id":"6191cc9e6d34e827404cebab","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674119843175-6191cc9e6d34e827404cebab.jpeg","isPro":false,"fullname":"Yang","user":"jacklanda","type":"user"},"summary":"We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.","upvotes":2,"discussionId":"6994768e046aed98c9b8fb01","projectPage":"https://lm-lexicon.github.io","githubRepo":"https://github.com/jacklanda/LMLexicon","githubRepoAddedBy":"user","ai_summary":"LM-Lexicon improves definition modeling through data clustering, semantic expert learning, and sparse mixture-of-experts architecture, achieving higher BLEU scores and better expert specialization.","ai_keywords":["definition modeling","data clustering","semantic expert learning","model merging","sparse mixture-of-experts architecture","domain experts","semantic domains","language models","expert specialization","semantic-aware domain-level routing"],"githubStars":2,"organization":{"_id":"63a95ac93453852ef5399a77","name":"bigai","fullname":"Beijing Institute for General Artificial Intelligence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}},"publishedAt":"2026-02-15T04:18:22.000Z","title":"LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts","summary":"We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14060.png","numComments":2,"submittedBy":{"_id":"6191cc9e6d34e827404cebab","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674119843175-6191cc9e6d34e827404cebab.jpeg","fullname":"Yang","name":"jacklanda","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"63a95ac93453852ef5399a77","name":"bigai","fullname":"Beijing Institute for General Artificial Intelligence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.09185","authors":[{"_id":"6990fe7c50fb2c0be478374e","user":{"_id":"62b4f3b7464e664268bf4e85","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg","isPro":false,"fullname":"Leo","user":"hao-li","type":"user"},"name":"Hao Li","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:52:23.468Z","hidden":false},{"_id":"6990fe7c50fb2c0be478374f","name":"Haoxiang Zhang","hidden":false},{"_id":"6990fe7c50fb2c0be4783750","name":"Ahmed E. Hassan","hidden":false}],"publishedAt":"2026-02-09T20:45:58.000Z","submittedOnDailyAt":"2026-02-17T01:18:44.324Z","title":"AIDev: Studying AI Coding Agents on GitHub","submittedOnDailyBy":{"_id":"62b4f3b7464e664268bf4e85","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg","isPro":false,"fullname":"Leo","user":"hao-li","type":"user"},"summary":"AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.\n  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering","upvotes":2,"discussionId":"6990fe7c50fb2c0be4783751","projectPage":"https://huggingface.co/datasets/hao-li/AIDev","ai_summary":"AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios.","ai_keywords":["AI coding agents","pull requests","GitHub repositories","developer productivity","human-AI collaboration","agentic software engineering"]},"publishedAt":"2026-02-09T15:45:58.000Z","title":"AIDev: Studying AI Coding Agents on GitHub","summary":"AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.\n  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09185.png","numComments":2,"submittedBy":{"_id":"62b4f3b7464e664268bf4e85","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg","fullname":"Leo","name":"hao-li","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.15031","authors":[{"_id":"699494bad2ea89ac106cf9e7","name":"Yehonathan Litman","hidden":false},{"_id":"699494bad2ea89ac106cf9e8","name":"Shikun Liu","hidden":false},{"_id":"699494bad2ea89ac106cf9e9","name":"Dario Seyb","hidden":false},{"_id":"699494bad2ea89ac106cf9ea","name":"Nicholas Milef","hidden":false},{"_id":"699494bad2ea89ac106cf9eb","name":"Yang Zhou","hidden":false},{"_id":"699494bad2ea89ac106cf9ec","name":"Carl Marshall","hidden":false},{"_id":"699494bad2ea89ac106cf9ed","name":"Shubham Tulsiani","hidden":false},{"_id":"699494bad2ea89ac106cf9ee","name":"Caleb Leak","hidden":false}],"publishedAt":"2026-02-16T18:59:58.000Z","submittedOnDailyAt":"2026-02-17T13:50:01.718Z","title":"EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing","submittedOnDailyBy":{"_id":"646ff038799a974be31bb344","avatarUrl":"/avatars/d9dc17246fba8360e709235f55445ef5.svg","isPro":false,"fullname":"Yehonathan Litman","user":"thebluser","type":"user"},"summary":"High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.","upvotes":1,"discussionId":"699494bad2ea89ac106cf9ef","projectPage":"https://yehonathanlitman.github.io/edit_ctrl/","githubRepo":"https://github.com/yehonathanlitman/EditCtrl","githubRepoAddedBy":"user","ai_summary":"Efficient video inpainting framework that focuses computation on masked regions while maintaining global context consistency through a lightweight embedder.","ai_keywords":["video foundation models","video inpainting","computational efficiency","masked tokens","local video context module","temporal global context embedder","autoregressive content propagation"],"githubStars":2,"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"}},"publishedAt":"2026-02-16T13:59:58.000Z","title":"EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing","summary":"High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15031.png","numComments":1,"submittedBy":{"_id":"646ff038799a974be31bb344","avatarUrl":"/avatars/d9dc17246fba8360e709235f55445ef5.svg","fullname":"Yehonathan Litman","name":"thebluser","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14941","authors":[{"_id":"69949784d2ea89ac106cfa09","name":"Zun Wang","hidden":false},{"_id":"69949784d2ea89ac106cfa0a","name":"Han Lin","hidden":false},{"_id":"69949784d2ea89ac106cfa0b","name":"Jaehong Yoon","hidden":false},{"_id":"69949784d2ea89ac106cfa0c","name":"Jaemin Cho","hidden":false},{"_id":"69949784d2ea89ac106cfa0d","name":"Yue Zhang","hidden":false},{"_id":"69949784d2ea89ac106cfa0e","name":"Mohit Bansal","hidden":false}],"publishedAt":"2026-02-16T17:23:08.000Z","submittedOnDailyAt":"2026-02-17T14:02:50.281Z","title":"AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories","submittedOnDailyBy":{"_id":"646e86350867c99c2d3f2ecf","avatarUrl":"/avatars/b89798ff623abffb169eacda2ac32fde.svg","isPro":true,"fullname":"Han Lin","user":"hanlincs","type":"user"},"summary":"Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.","upvotes":1,"discussionId":"69949784d2ea89ac106cfa0f","projectPage":"https://zunwang1.github.io/AnchorWeave","githubRepo":"https://github.com/wz0919/AnchorWeave","githubRepoAddedBy":"user","ai_summary":"AnchorWeave addresses long-term video generation consistency by replacing global 3D scene reconstruction with multiple local geometric memories and a multi-anchor weaving controller to reconcile cross-view inconsistencies.","ai_keywords":["camera-controllable video generation","memory-based approaches","3D scene reconstruction","cross-view misalignment","local geometric memories","multi-anchor weaving controller","coverage-driven retrieval"],"githubStars":1,"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}},"publishedAt":"2026-02-16T12:23:08.000Z","title":"AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories","summary":"Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14941.png","numComments":1,"submittedBy":{"_id":"646e86350867c99c2d3f2ecf","avatarUrl":"/avatars/b89798ff623abffb169eacda2ac32fde.svg","fullname":"Han Lin","name":"hanlincs","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14689","authors":[{"_id":"69941f8550fb2c0be4783df1","name":"Lukas Struppek","hidden":false},{"_id":"69941f8550fb2c0be4783df2","name":"Adam Gleave","hidden":false},{"_id":"69941f8550fb2c0be4783df3","name":"Kellin Pelrine","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6305ca82d37ce67e0e48aadb/uYonLHu4SGpeT2VzH4Jbk.png"],"publishedAt":"2026-02-16T12:24:21.000Z","submittedOnDailyAt":"2026-02-17T05:29:18.761Z","title":"Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks","submittedOnDailyBy":{"_id":"6305ca82d37ce67e0e48aadb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6305ca82d37ce67e0e48aadb/gyTcyIURKmXSFXtr07FGu.jpeg","isPro":false,"fullname":"Lukas Struppek","user":"lukas-struppek","type":"user"},"summary":"As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses, open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that prefill attacks are consistently effective against all major contemporary open-weight models, revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling, they remain vulnerable to tailored, model-specific strategies. Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.","upvotes":1,"discussionId":"69941f8650fb2c0be4783df4","ai_summary":"Prefill attacks represent a significant and underexplored vulnerability in open-weight language models, affecting major contemporary models despite some resistance from large reasoning models.","ai_keywords":["large language models","open-weight models","red-teaming","jailbreaking","prefilling","attack vectors","model-specific strategies","defenses"],"organization":{"_id":"64000f65b09f82a81a245859","name":"AlignmentResearch","fullname":"FAR AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6294db83b9d93a419906c9bb/LGJNz9yvM6gNEVHj_8KTn.png"}},"publishedAt":"2026-02-16T07:24:21.000Z","title":"Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks","summary":"As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses, open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that prefill attacks are consistently effective against all major contemporary open-weight models, revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling, they remain vulnerable to tailored, model-specific strategies. Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6305ca82d37ce67e0e48aadb/uYonLHu4SGpeT2VzH4Jbk.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14689.png","numComments":1,"submittedBy":{"_id":"6305ca82d37ce67e0e48aadb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6305ca82d37ce67e0e48aadb/gyTcyIURKmXSFXtr07FGu.jpeg","fullname":"Lukas Struppek","name":"lukas-struppek","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"64000f65b09f82a81a245859","name":"AlignmentResearch","fullname":"FAR AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6294db83b9d93a419906c9bb/LGJNz9yvM6gNEVHj_8KTn.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.12586","authors":[{"_id":"6994720d046aed98c9b8faf3","name":"Joshua Ong Jun Leang","hidden":false},{"_id":"6994720d046aed98c9b8faf4","name":"Yu Zhao","hidden":false},{"_id":"6994720d046aed98c9b8faf5","name":"Mihaela Ctlina Stoian","hidden":false},{"_id":"6994720d046aed98c9b8faf6","name":"Wenda Li","hidden":false},{"_id":"6994720d046aed98c9b8faf7","name":"Shay B. Cohen","hidden":false},{"_id":"6994720d046aed98c9b8faf8","name":"Eleonora Giunchiglia","hidden":false}],"publishedAt":"2026-02-13T03:56:22.000Z","submittedOnDailyAt":"2026-02-17T11:22:25.574Z","title":"Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models","submittedOnDailyBy":{"_id":"6537dd20aa214745cbc89ad9","avatarUrl":"/avatars/cb6bda58ae6063562eb02cf92c0e7c4e.svg","isPro":false,"fullname":"Joshua Ong Jun Leang","user":"Jforeverss","type":"user"},"summary":"While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.","upvotes":1,"discussionId":"6994720d046aed98c9b8faf9","ai_summary":"McDiffuSE enhances Masked Diffusion Models by optimizing slot infilling order through Monte Carlo Tree Search, improving reasoning task performance through strategic exploration of generation sequences.","ai_keywords":["Masked Diffusion Models","slot selection","Monte Carlo Tree Search","look-ahead simulations","combinatorial space","generation orders","autoregressive baselines","plan-and-infill decoding"]},"publishedAt":"2026-02-12T22:56:22.000Z","title":"Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models","summary":"While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12586.png","numComments":1,"submittedBy":{"_id":"6537dd20aa214745cbc89ad9","avatarUrl":"/avatars/cb6bda58ae6063562eb02cf92c0e7c4e.svg","fullname":"Joshua Ong Jun Leang","name":"Jforeverss","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.13346","authors":[{"_id":"6994d4998d17d1ee8c10eb7b","name":"Zhen Wang","hidden":false},{"_id":"6994d4998d17d1ee8c10eb7c","name":"Yiming Gao","hidden":false},{"_id":"6994d4998d17d1ee8c10eb7d","name":"Jieyuan Liu","hidden":false},{"_id":"6994d4998d17d1ee8c10eb7e","name":"Enze Ma","hidden":false},{"_id":"6994d4998d17d1ee8c10eb7f","name":"Jefferson Chen","hidden":false},{"_id":"6994d4998d17d1ee8c10eb80","name":"Mark Antkowiak","hidden":false},{"_id":"6994d4998d17d1ee8c10eb81","name":"Mengzhou Hu","hidden":false},{"_id":"6994d4998d17d1ee8c10eb82","name":"JungHo Kong","hidden":false},{"_id":"6994d4998d17d1ee8c10eb83","name":"Dexter Pratt","hidden":false},{"_id":"6994d4998d17d1ee8c10eb84","name":"Zhiting Hu","hidden":false},{"_id":"6994d4998d17d1ee8c10eb85","name":"Wei Wang","hidden":false},{"_id":"6994d4998d17d1ee8c10eb86","name":"Trey Ideker","hidden":false},{"_id":"6994d4998d17d1ee8c10eb87","name":"Eric P. Xing","hidden":false}],"publishedAt":"2026-02-12T20:20:22.000Z","submittedOnDailyAt":"2026-02-17T18:21:24.923Z","title":"CellMaster: Collaborative Cell Type Annotation in Single-Cell Analysis","submittedOnDailyBy":{"_id":"64a833d2f152bba4b550c913","avatarUrl":"/avatars/cff37a427c01c6b6691f588481d96416.svg","isPro":false,"fullname":"Zhen Wang","user":"zhenwang9102","type":"user"},"summary":"Single-cell RNA-seq (scRNA-seq) enables atlas-scale profiling of complex tissues, revealing rare lineages and transient states. Yet, assigning biologically valid cell identities remains a bottleneck because markers are tissue- and state-dependent, and novel states lack references. We present CellMaster, an AI agent that mimics expert practice for zero-shot cell-type annotation. Unlike existing automated tools, CellMaster leverages LLM-encoded knowledge (e.g., GPT-4o) to perform on-the-fly annotation with interpretable rationales, without pre-training or fixed marker databases. Across 9 datasets spanning 8 tissues, CellMaster improved accuracy by 7.1% over best-performing baselines (including CellTypist and scTab) in automatic mode. With human-in-the-loop refinement, this advantage increased to 18.6%, with a 22.1% gain on subtype populations. The system demonstrates particular strength in rare and novel cell states where baselines often fail. Source code and the web application are available at https://github.com/AnonymousGym/CellMaster{https://github.com/AnonymousGym/CellMaster}.","upvotes":1,"discussionId":"6994d4998d17d1ee8c10eb88","ai_summary":"CellMaster uses LLM-encoded knowledge for zero-shot cell-type annotation in single-cell RNA sequencing, improving accuracy over existing tools through interpretable rationales without pre-training.","ai_keywords":["single-cell RNA-seq","cell-type annotation","LLM-encoded knowledge","zero-shot learning","interpretable rationales","CellTypist","scTab"],"organization":{"_id":"697e87d12cc19315a8497001","name":"UCSanDiego","fullname":"University of California at San Diego","avatar":"https://cdn-uploads.huggingface.co/production/uploads/697e8687c00f332cf492d29e/KUQpvngxP4r9oBSDZwIwZ.png"}},"publishedAt":"2026-02-12T15:20:22.000Z","title":"CellMaster: Collaborative Cell Type Annotation in Single-Cell Analysis","summary":"Single-cell RNA-seq (scRNA-seq) enables atlas-scale profiling of complex tissues, revealing rare lineages and transient states. Yet, assigning biologically valid cell identities remains a bottleneck because markers are tissue- and state-dependent, and novel states lack references. We present CellMaster, an AI agent that mimics expert practice for zero-shot cell-type annotation. Unlike existing automated tools, CellMaster leverages LLM-encoded knowledge (e.g., GPT-4o) to perform on-the-fly annotation with interpretable rationales, without pre-training or fixed marker databases. Across 9 datasets spanning 8 tissues, CellMaster improved accuracy by 7.1% over best-performing baselines (including CellTypist and scTab) in automatic mode. With human-in-the-loop refinement, this advantage increased to 18.6%, with a 22.1% gain on subtype populations. The system demonstrates particular strength in rare and novel cell states where baselines often fail. Source code and the web application are available at https://github.com/AnonymousGym/CellMaster{https://github.com/AnonymousGym/CellMaster}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13346.png","numComments":1,"submittedBy":{"_id":"64a833d2f152bba4b550c913","avatarUrl":"/avatars/cff37a427c01c6b6691f588481d96416.svg","fullname":"Zhen Wang","name":"zhenwang9102","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"697e87d12cc19315a8497001","name":"UCSanDiego","fullname":"University of California at San Diego","avatar":"https://cdn-uploads.huggingface.co/production/uploads/697e8687c00f332cf492d29e/KUQpvngxP4r9oBSDZwIwZ.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.11968","authors":[{"_id":"69935a9d50fb2c0be4783bf1","user":{"_id":"67045bace5debf722e7412b3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67045bace5debf722e7412b3/3U_LNsyNEvtSASE9ceLXB.jpeg","isPro":false,"fullname":"Maria F","user":"MariaFjodorowa","type":"user"},"name":"Mariia Fedorova","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:53:33.550Z","hidden":false},{"_id":"69935a9d50fb2c0be4783bf2","name":"Andrey Kutuzov","hidden":false},{"_id":"69935a9d50fb2c0be4783bf3","name":"Khonzoda Umarova","hidden":false}],"publishedAt":"2026-02-12T14:01:40.000Z","submittedOnDailyAt":"2026-02-17T11:37:12.447Z","title":"DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling","submittedOnDailyBy":{"_id":"67045bace5debf722e7412b3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67045bace5debf722e7412b3/3U_LNsyNEvtSASE9ceLXB.jpeg","isPro":false,"fullname":"Maria F","user":"MariaFjodorowa","type":"user"},"summary":"In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.","upvotes":1,"discussionId":"69935a9e50fb2c0be4783bf4","projectPage":"https://data.hplt-project.org/three/diachronic/","githubRepo":"https://github.com/ltgoslo/scdisc_hplt","githubRepoAddedBy":"user","githubStars":0,"organization":{"_id":"6492a69fe393c9e9897116ee","name":"HPLT","fullname":"HPLT","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61b3167c0c167add56f97529/1ZEKkpQlSfeCMAsMasxG-.png"}},"publishedAt":"2026-02-12T09:01:40.000Z","title":"DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling","summary":"In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11968.png","numComments":1,"submittedBy":{"_id":"67045bace5debf722e7412b3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67045bace5debf722e7412b3/3U_LNsyNEvtSASE9ceLXB.jpeg","fullname":"Maria F","name":"MariaFjodorowa","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6492a69fe393c9e9897116ee","name":"HPLT","fullname":"HPLT","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61b3167c0c167add56f97529/1ZEKkpQlSfeCMAsMasxG-.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.12299","authors":[{"_id":"699409b850fb2c0be4783ddf","name":"Mandip Goswami","hidden":false}],"publishedAt":"2026-02-11T04:07:05.000Z","submittedOnDailyAt":"2026-02-17T03:57:54.532Z","title":"Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization","submittedOnDailyBy":{"_id":"67d43aeb6ffb8add49ea6712","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg","isPro":false,"fullname":"Mandip Goswami","user":"mandipgoswami","type":"user"},"summary":"Room acoustics analysis plays a central role in architectural design, audio engineering, speech intelligibility assessment, and hearing research. Despite the availability of standardized metrics such as reverberation time, clarity, and speech transmission index, accessible tools that combine rigorous signal processing with intuitive visualization remain scarce. This paper presents AcoustiVision Pro, an open-source web-based platform for comprehensive room impulse response (RIR) analysis. The system computes twelve distinct acoustic parameters from uploaded or dataset-sourced RIRs, provides interactive 3D visualizations of early reflections, generates frequency-dependent decay characteristics through waterfall plots, and checks compliance against international standards including ANSI S12.60 and ISO 3382. We introduce the accompanying RIRMega and RIRMega Speech datasets hosted on Hugging Face, containing thousands of simulated room impulse responses with full metadata. The platform supports real-time auralization through FFT-based convolution, exports detailed PDF reports suitable for engineering documentation, and provides CSV data export for further analysis. We describe the mathematical foundations underlying each acoustic metric, detail the system architecture, and present preliminary case studies demonstrating the platform's utility across diverse application domains including classroom acoustics, healthcare facility design, and recording studio evaluation.","upvotes":1,"discussionId":"699409b850fb2c0be4783de0","projectPage":"https://huggingface.co/spaces/mandipgoswami/acoustivision-pro"},"publishedAt":"2026-02-10T23:07:05.000Z","title":"Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization","summary":"Room acoustics analysis plays a central role in architectural design, audio engineering, speech intelligibility assessment, and hearing research. Despite the availability of standardized metrics such as reverberation time, clarity, and speech transmission index, accessible tools that combine rigorous signal processing with intuitive visualization remain scarce. This paper presents AcoustiVision Pro, an open-source web-based platform for comprehensive room impulse response (RIR) analysis. The system computes twelve distinct acoustic parameters from uploaded or dataset-sourced RIRs, provides interactive 3D visualizations of early reflections, generates frequency-dependent decay characteristics through waterfall plots, and checks compliance against international standards including ANSI S12.60 and ISO 3382. We introduce the accompanying RIRMega and RIRMega Speech datasets hosted on Hugging Face, containing thousands of simulated room impulse responses with full metadata. The platform supports real-time auralization through FFT-based convolution, exports detailed PDF reports suitable for engineering documentation, and provides CSV data export for further analysis. We describe the mathematical foundations underlying each acoustic metric, detail the system architecture, and present preliminary case studies demonstrating the platform's utility across diverse application domains including classroom acoustics, healthcare facility design, and recording studio evaluation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12299.png","numComments":1,"submittedBy":{"_id":"67d43aeb6ffb8add49ea6712","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67d43aeb6ffb8add49ea6712/0h4bV3Ptgh_zTRUHVqwe2.jpeg","fullname":"Mandip Goswami","name":"mandipgoswami","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.09319","authors":[{"_id":"6994359350fb2c0be4783e87","name":"Zhisheng Qi","hidden":false},{"_id":"6994359350fb2c0be4783e88","name":"Utkarsh Sahu","hidden":false},{"_id":"6994359350fb2c0be4783e89","name":"Li Ma","hidden":false},{"_id":"6994359350fb2c0be4783e8a","name":"Haoyu Han","hidden":false},{"_id":"6994359350fb2c0be4783e8b","name":"Ryan Rossi","hidden":false},{"_id":"6994359350fb2c0be4783e8c","user":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","isPro":false,"fullname":"Franck Dernoncourt","user":"Franck-Dernoncourt","type":"user"},"name":"Franck Dernoncourt","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:08:43.351Z","hidden":false},{"_id":"6994359350fb2c0be4783e8d","name":"Mahantesh Halappanavar","hidden":false},{"_id":"6994359350fb2c0be4783e8e","name":"Nesreen Ahmed","hidden":false},{"_id":"6994359350fb2c0be4783e8f","name":"Yushun Dong","hidden":false},{"_id":"6994359350fb2c0be4783e90","name":"Yue Zhao","hidden":false},{"_id":"6994359350fb2c0be4783e91","name":"Yu Zhang","hidden":false},{"_id":"6994359350fb2c0be4783e92","name":"Yu Wang","hidden":false}],"publishedAt":"2026-02-10T01:27:46.000Z","submittedOnDailyAt":"2026-02-17T07:03:27.961Z","title":"Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation","submittedOnDailyBy":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","isPro":false,"fullname":"Franck Dernoncourt","user":"Franck-Dernoncourt","type":"user"},"summary":"Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.","upvotes":1,"discussionId":"6994359350fb2c0be4783e93","ai_summary":"A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols.","ai_keywords":["Retrieval-Augmented Generation","knowledge-extraction attacks","attack and defense strategies","retrieval embedding models","generation models","standardized protocols","reproducible evaluation"]},"publishedAt":"2026-02-09T20:27:46.000Z","title":"Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation","summary":"Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09319.png","numComments":1,"submittedBy":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","fullname":"Franck Dernoncourt","name":"Franck-Dernoncourt","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":12,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07673","authors":[{"_id":"6994361550fb2c0be4783ea0","name":"Jiangnan Fang","hidden":false},{"_id":"6994361550fb2c0be4783ea1","name":"Cheng-Tse Liu","hidden":false},{"_id":"6994361550fb2c0be4783ea2","name":"Hanieh Deilamsalehy","hidden":false},{"_id":"6994361550fb2c0be4783ea3","name":"Nesreen K. Ahmed","hidden":false},{"_id":"6994361550fb2c0be4783ea4","name":"Puneet Mathur","hidden":false},{"_id":"6994361550fb2c0be4783ea5","name":"Nedim Lipka","hidden":false},{"_id":"6994361550fb2c0be4783ea6","user":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","isPro":false,"fullname":"Franck Dernoncourt","user":"Franck-Dernoncourt","type":"user"},"name":"Franck Dernoncourt","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:08:40.658Z","hidden":false},{"_id":"6994361550fb2c0be4783ea7","name":"Ryan A. Rossi","hidden":false}],"publishedAt":"2026-02-07T19:39:28.000Z","submittedOnDailyAt":"2026-02-17T07:05:09.271Z","title":"Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation","submittedOnDailyBy":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","isPro":false,"fullname":"Franck Dernoncourt","user":"Franck-Dernoncourt","type":"user"},"summary":"Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.","upvotes":1,"discussionId":"6994361550fb2c0be4783ea8","ai_summary":"LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.","ai_keywords":["large language model","LLM judge","summarization","ROUGE","BLEU","overlap metric","parameter count","Gemma 3","LLaMA 3"]},"publishedAt":"2026-02-07T14:39:28.000Z","title":"Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation","summary":"Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07673.png","numComments":1,"submittedBy":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","fullname":"Franck Dernoncourt","name":"Franck-Dernoncourt","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":12,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.14696","authors":[{"_id":"6993dfb650fb2c0be4783d0f","user":{"_id":"60d0e7ff0c9ba111563b81d7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60d0e7ff0c9ba111563b81d7/MudCH3WtpBIQGuodc7GND.jpeg","isPro":false,"fullname":"Nihal Nayak","user":"nihalnayak","type":"user"},"name":"Nihal V. Nayak","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:53:59.035Z","hidden":false},{"_id":"6993dfb650fb2c0be4783d10","name":"Paula Rodriguez-Diaz","hidden":false},{"_id":"6993dfb650fb2c0be4783d11","name":"Neha Hulkund","hidden":false},{"_id":"6993dfb650fb2c0be4783d12","name":"Sara Beery","hidden":false},{"_id":"6993dfb650fb2c0be4783d13","name":"David Alvarez-Melis","hidden":false}],"publishedAt":"2026-02-16T12:33:05.000Z","submittedOnDailyAt":"2026-02-17T00:57:37.305Z","title":"A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)","submittedOnDailyBy":{"_id":"60d0e7ff0c9ba111563b81d7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60d0e7ff0c9ba111563b81d7/MudCH3WtpBIQGuodc7GND.jpeg","isPro":false,"fullname":"Nihal Nayak","user":"nihalnayak","type":"user"},"summary":"Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.","upvotes":0,"discussionId":"6993dfb650fb2c0be4783d14","ai_summary":"Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets.","ai_keywords":["instruction fine-tuning","large language models","data representation","selection algorithms","gradient-based representations","greedy round-robin selection","approximate distance minimization","generalization bounds"],"organization":{"_id":"68e5678a3f1c67fe1c5aadc7","name":"Harvard-DCML","fullname":"Harvard Data-Centric Machine Learning Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60d0e7ff0c9ba111563b81d7/bcJgV9xDfzZmsFI8uOTud.png"}},"publishedAt":"2026-02-16T07:33:05.000Z","title":"A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)","summary":"Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14696.png","numComments":1,"submittedBy":{"_id":"60d0e7ff0c9ba111563b81d7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60d0e7ff0c9ba111563b81d7/MudCH3WtpBIQGuodc7GND.jpeg","fullname":"Nihal Nayak","name":"nihalnayak","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"organization":{"_id":"68e5678a3f1c67fe1c5aadc7","name":"Harvard-DCML","fullname":"Harvard Data-Centric Machine Learning Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60d0e7ff0c9ba111563b81d7/bcJgV9xDfzZmsFI8uOTud.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.14560","authors":[{"_id":"6993fd0a50fb2c0be4783da0","user":{"_id":"68d51061ab9204b0c8a0ceb2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d51061ab9204b0c8a0ceb2/CVmg6wo4yBqIZgGZUYSX8.jpeg","isPro":false,"fullname":"Sandy Hardian Susanto Herho","user":"sandyherho","type":"user"},"name":"Sandy H. S. Herho","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:54:04.692Z","hidden":false},{"_id":"6993fd0a50fb2c0be4783da1","name":"Rusmawan Suwarman","hidden":false},{"_id":"6993fd0a50fb2c0be4783da2","name":"Nurjanna J. Trilaksono","hidden":false},{"_id":"6993fd0a50fb2c0be4783da3","name":"Iwan P. Anwar","hidden":false},{"_id":"6993fd0a50fb2c0be4783da4","name":"Faiz R. Fajary","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/68d51061ab9204b0c8a0ceb2/FQtxMMM7c4uwO2TQrO0vC.jpeg"],"publishedAt":"2026-02-16T08:40:01.000Z","submittedOnDailyAt":"2026-02-17T03:31:31.062Z","title":"Preliminary sonification of ENSO using traditional Javanese gamelan scales","submittedOnDailyBy":{"_id":"68d51061ab9204b0c8a0ceb2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d51061ab9204b0c8a0ceb2/CVmg6wo4yBqIZgGZUYSX8.jpeg","isPro":false,"fullname":"Sandy Hardian Susanto Herho","user":"sandyherho","type":"user"},"summary":"Sonification -- the mapping of data to non-speech audio -- offers an underexplored channel for representing complex dynamical systems. We treat El Nio-Southern Oscillation (ENSO), a canonical example of low-dimensional climate chaos, as a test case for culturally-situated sonification evaluated through complex systems diagnostics. Using parameter-mapping sonification of the Nio 3.4 sea surface temperature anomaly index (1870--2024), we encode ENSO variability into two traditional Javanese gamelan pentatonic systems (pelog and slendro) across four composition strategies, then analyze the resulting audio as trajectories in a two-dimensional acoustic phase space. Recurrence-based diagnostics, convex hull geometry, and coupling analysis reveal that the sonification pipeline preserves key dynamical signatures: alternating modes produce the highest trajectory recurrence rates, echoing ENSO's quasi-periodicity; layered polyphonic modes explore the broadest phase space regions; and the two scale families induce qualitatively distinct coupling regimes between spectral brightness and energy -- predominantly anti-phase in pelog but near-independent in slendro. Phase space trajectory analysis provides a rigorous geometric framework for comparing sonification designs within a complex systems context. Perceptual validation remains necessary; we contribute the dynamical systems methodology for evaluating such mappings.","upvotes":0,"discussionId":"6993fd0a50fb2c0be4783da5","projectPage":"https://doi.org/10.17605/OSF.IO/QY82M","githubRepo":"https://github.com/sandyherho/suppl-enso-javanese-sonification","githubRepoAddedBy":"user","ai_summary":"Parameter-mapping sonification of ENSO data preserves dynamical signatures through acoustic phase space analysis, revealing distinct coupling regimes in traditional musical scales.","ai_keywords":["sonification","El Nio-Southern Oscillation","ENSO","dynamical systems","parameter-mapping","acoustic phase space","recurrence-based diagnostics","convex hull geometry","coupling analysis","gamelan","pelog","slendro"],"githubStars":2,"organization":{"_id":"643f6b84ec817b766686ba98","name":"ITB","fullname":"Institut Teknologi Bandung","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/FLKHDmfoM8RvukvHEgXj4.png"}},"publishedAt":"2026-02-16T03:40:01.000Z","title":"Preliminary sonification of ENSO using traditional Javanese gamelan scales","summary":"Sonification -- the mapping of data to non-speech audio -- offers an underexplored channel for representing complex dynamical systems. We treat El Nio-Southern Oscillation (ENSO), a canonical example of low-dimensional climate chaos, as a test case for culturally-situated sonification evaluated through complex systems diagnostics. Using parameter-mapping sonification of the Nio 3.4 sea surface temperature anomaly index (1870--2024), we encode ENSO variability into two traditional Javanese gamelan pentatonic systems (pelog and slendro) across four composition strategies, then analyze the resulting audio as trajectories in a two-dimensional acoustic phase space. Recurrence-based diagnostics, convex hull geometry, and coupling analysis reveal that the sonification pipeline preserves key dynamical signatures: alternating modes produce the highest trajectory recurrence rates, echoing ENSO's quasi-periodicity; layered polyphonic modes explore the broadest phase space regions; and the two scale families induce qualitatively distinct coupling regimes between spectral brightness and energy -- predominantly anti-phase in pelog but near-independent in slendro. Phase space trajectory analysis provides a rigorous geometric framework for comparing sonification designs within a complex systems context. Perceptual validation remains necessary; we contribute the dynamical systems methodology for evaluating such mappings.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/68d51061ab9204b0c8a0ceb2/FQtxMMM7c4uwO2TQrO0vC.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14560.png","numComments":2,"submittedBy":{"_id":"68d51061ab9204b0c8a0ceb2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d51061ab9204b0c8a0ceb2/CVmg6wo4yBqIZgGZUYSX8.jpeg","fullname":"Sandy Hardian Susanto Herho","name":"sandyherho","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"643f6b84ec817b766686ba98","name":"ITB","fullname":"Institut Teknologi Bandung","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/FLKHDmfoM8RvukvHEgXj4.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.13516","authors":[{"_id":"69947c04d2ea89ac106cf9a9","name":"Jaechul Roh","hidden":false},{"_id":"69947c04d2ea89ac106cf9aa","name":"Eugene Bagdasarian","hidden":false},{"_id":"69947c04d2ea89ac106cf9ab","name":"Hamed Haddadi","hidden":false},{"_id":"69947c04d2ea89ac106cf9ac","name":"Ali Shahin Shamsabadi","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6504ea59f3060ea8409a1f9e/PDE-wYT5oJKepSEXD2leL.png"],"publishedAt":"2026-02-13T23:02:50.000Z","submittedOnDailyAt":"2026-02-17T12:06:52.899Z","title":"SPILLage: Agentic Oversharing on the Web","submittedOnDailyBy":{"_id":"6504ea59f3060ea8409a1f9e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6504ea59f3060ea8409a1f9e/QEluRfqLHLT7OpdZNRy4X.png","isPro":false,"fullname":"Jaechul Roh","user":"jroh","type":"user"},"summary":"LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act \"in the wild\", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of \"output\" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.","upvotes":0,"discussionId":"69947c04d2ea89ac106cf9ad","githubRepo":"https://github.com/jrohsc/SPILLage","githubRepoAddedBy":"user","ai_summary":"Web agents inadvertently disclose user information through both content and behavioral traces, with behavioral oversharing being more prevalent than content oversharing, and this issue persists despite mitigation efforts.","ai_keywords":["web agents","natural agentic oversharing","SPILLage","behavioral oversharing","content oversharing","task-irrelevant information","prompt-level mitigation","task success"],"githubStars":0,"organization":{"_id":"63ffb928e7767a89533a8673","name":"bravesoftware","fullname":"Brave Software","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1677703450713-63ffb579b09f82a81a2083f4.jpeg"}},"publishedAt":"2026-02-13T18:02:50.000Z","title":"SPILLage: Agentic Oversharing on the Web","summary":"LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act \"in the wild\", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of \"output\" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6504ea59f3060ea8409a1f9e/PDE-wYT5oJKepSEXD2leL.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13516.png","numComments":1,"submittedBy":{"_id":"6504ea59f3060ea8409a1f9e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6504ea59f3060ea8409a1f9e/QEluRfqLHLT7OpdZNRy4X.png","fullname":"Jaechul Roh","name":"jroh","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"63ffb928e7767a89533a8673","name":"bravesoftware","fullname":"Brave Software","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1677703450713-63ffb579b09f82a81a2083f4.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.10458","authors":[{"_id":"698f40ff0cf1f9a6bbe7fb45","user":{"_id":"66e73f1b55f01fbea93f959f","avatarUrl":"/avatars/efe55aca1f9e744a1e2d639901fa6baa.svg","isPro":false,"fullname":"Yansong Qu","user":"ys-qu","type":"user"},"name":"Yansong Qu","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:51:40.369Z","hidden":false},{"_id":"698f40ff0cf1f9a6bbe7fb46","name":"Zihao Sheng","hidden":false},{"_id":"698f40ff0cf1f9a6bbe7fb47","name":"Zilin Huang","hidden":false},{"_id":"698f40ff0cf1f9a6bbe7fb48","name":"Jiancong Chen","hidden":false},{"_id":"698f40ff0cf1f9a6bbe7fb49","name":"Yuhao Luo","hidden":false},{"_id":"698f40ff0cf1f9a6bbe7fb4a","name":"Tianyi Wang","hidden":false},{"_id":"698f40ff0cf1f9a6bbe7fb4b","name":"Yiheng Feng","hidden":false},{"_id":"698f40ff0cf1f9a6bbe7fb4c","name":"Samuel Labi","hidden":false},{"_id":"698f40ff0cf1f9a6bbe7fb4d","name":"Sikai Chen","hidden":false}],"publishedAt":"2026-02-11T02:56:04.000Z","submittedOnDailyAt":"2026-02-17T21:48:39.703Z","title":"Found-RL: foundation model-enhanced reinforcement learning for autonomous driving","submittedOnDailyBy":{"_id":"66e73f1b55f01fbea93f959f","avatarUrl":"/avatars/efe55aca1f9e744a1e2d639901fa6baa.svg","isPro":false,"fullname":"Yansong Qu","user":"ys-qu","type":"user"},"summary":"Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.","upvotes":0,"discussionId":"698f40ff0cf1f9a6bbe7fb4e","projectPage":"https://ys-qu.github.io/found-rl-website/","githubRepo":"https://github.com/ys-qu/found-rl","githubRepoAddedBy":"user","ai_summary":"Found-RL integrates vision-language models with reinforcement learning for autonomous driving, addressing sample efficiency and latency issues through asynchronous inference and specialized supervision mechanisms.","ai_keywords":["Reinforcement Learning","Vision-Language Models","asynchronous batch inference","Value-Margin Regularization","Advantage-Weighted Action Guidance","CLIP","Conditional Contrastive Action Alignment","real-time learning","end-to-end pipeline"],"organization":{"_id":"6400300fe7767a89533fa2c0","name":"Purdue","fullname":"Purdue University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1677733893266-64002f32cafc9d54986439cb.jpeg"}},"publishedAt":"2026-02-10T21:56:04.000Z","title":"Found-RL: foundation model-enhanced reinforcement learning for autonomous driving","summary":"Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10458.png","numComments":1,"submittedBy":{"_id":"66e73f1b55f01fbea93f959f","avatarUrl":"/avatars/efe55aca1f9e744a1e2d639901fa6baa.svg","fullname":"Yansong Qu","name":"ys-qu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6400300fe7767a89533fa2c0","name":"Purdue","fullname":"Purdue University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1677733893266-64002f32cafc9d54986439cb.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.13294","authors":[{"_id":"6993ffc550fb2c0be4783dcf","name":"Jiarong Liang","hidden":false},{"_id":"6993ffc550fb2c0be4783dd0","name":"Max Ku","hidden":false},{"_id":"6993ffc550fb2c0be4783dd1","name":"Ka-Hei Hui","hidden":false},{"_id":"6993ffc550fb2c0be4783dd2","name":"Ping Nie","hidden":false},{"_id":"6993ffc550fb2c0be4783dd3","name":"Wenhu Chen","hidden":false}],"publishedAt":"2026-02-09T05:46:44.000Z","submittedOnDailyAt":"2026-02-17T20:02:09.103Z","title":"VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction","submittedOnDailyBy":{"_id":"68cccfe16e4618473d571b1f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68cccfe16e4618473d571b1f/xeF5Ojc5Q1qvtnWy7XoQL.jpeg","isPro":false,"fullname":"Jiarong Liang","user":"lllqaq","type":"user"},"summary":"Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.","upvotes":0,"discussionId":"6993ffc550fb2c0be4783dd4","projectPage":"https://tiger-ai-lab.github.io/VisPhyWorld/","githubRepo":"https://github.com/TIGER-AI-Lab/VisPhyWorld","githubRepoAddedBy":"user","ai_summary":"VisPhyWorld framework evaluates physical reasoning in MLLMs by requiring executable simulator code generation from visual observations, separating physical reasoning from rendering and enabling inspectable, falsifiable world representations.","ai_keywords":["Multimodal Large Language Models","Visual Question Answering","Violation of Expectation","execution-based framework","simulator code generation","physical reasoning","world representation","physical parameters","physical dynamics","semantic scene understanding"],"githubStars":0,"organization":{"_id":"6313a90017838d05194fd282","name":"TIGER-Lab","fullname":"TIGER-Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"}},"publishedAt":"2026-02-09T00:46:44.000Z","title":"VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction","summary":"Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13294.png","numComments":1,"submittedBy":{"_id":"68cccfe16e4618473d571b1f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68cccfe16e4618473d571b1f/xeF5Ojc5Q1qvtnWy7XoQL.jpeg","fullname":"Jiarong Liang","name":"lllqaq","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6313a90017838d05194fd282","name":"TIGER-Lab","fullname":"TIGER-Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/Noi3Qq3RYz8Jdq6BaFteq.png"},"isAuthorParticipating":false}]
[{"paper":{"id":"2602.04705","authors":[{"_id":"698424a7e34659da7e1f4e6f","name":"Haifeng Wang","hidden":false},{"_id":"698424a7e34659da7e1f4e70","name":"Hua Wu","hidden":false},{"_id":"698424a7e34659da7e1f4e71","name":"Tian Wu","hidden":false},{"_id":"698424a7e34659da7e1f4e72","name":"Yu Sun","hidden":false},{"_id":"698424a7e34659da7e1f4e73","name":"Jing Liu","hidden":false},{"_id":"698424a7e34659da7e1f4e74","name":"Dianhai Yu","hidden":false},{"_id":"698424a7e34659da7e1f4e75","name":"Yanjun Ma","hidden":false},{"_id":"698424a7e34659da7e1f4e76","name":"Jingzhou He","hidden":false},{"_id":"698424a7e34659da7e1f4e77","name":"Zhongjun He","hidden":false},{"_id":"698424a7e34659da7e1f4e78","name":"Dou Hong","hidden":false},{"_id":"698424a7e34659da7e1f4e79","name":"Qiwen Liu","hidden":false},{"_id":"698424a7e34659da7e1f4e7a","name":"Shuohuan Wang","hidden":false},{"_id":"698424a7e34659da7e1f4e7b","user":{"_id":"62cd9632342b1d5dab8df4c3","avatarUrl":"/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg","isPro":false,"fullname":"Junyuan Shang","user":"sjy1203","type":"user"},"name":"Junyuan Shang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:28.482Z","hidden":false},{"_id":"698424a7e34659da7e1f4e7c","user":{"_id":"67f37f78b36e82d366dedeec","avatarUrl":"/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg","isPro":false,"fullname":"Max Zhenyu Zhang","user":"max-zhenyu-zhang","type":"user"},"name":"Zhenyu Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:03.972Z","hidden":false},{"_id":"698424a7e34659da7e1f4e7d","name":"Yuchen Ding","hidden":false},{"_id":"698424a7e34659da7e1f4e7e","name":"Jinle Zeng","hidden":false},{"_id":"698424a7e34659da7e1f4e7f","name":"Jiabin Yang","hidden":false},{"_id":"698424a7e34659da7e1f4e80","name":"Liang Shen","hidden":false},{"_id":"698424a7e34659da7e1f4e81","name":"Ruibiao Chen","hidden":false},{"_id":"698424a7e34659da7e1f4e82","name":"Weichong Yin","hidden":false},{"_id":"698424a7e34659da7e1f4e83","name":"Siyu Ding","hidden":false},{"_id":"698424a7e34659da7e1f4e84","name":"Dai Dai","hidden":false},{"_id":"698424a7e34659da7e1f4e85","name":"Shikun Feng","hidden":false},{"_id":"698424a7e34659da7e1f4e86","name":"Siqi Bao","hidden":false},{"_id":"698424a7e34659da7e1f4e87","name":"Bolei He","hidden":false},{"_id":"698424a7e34659da7e1f4e88","name":"Yan Chen","hidden":false},{"_id":"698424a7e34659da7e1f4e89","name":"Zhenyu Jiao","hidden":false},{"_id":"698424a7e34659da7e1f4e8a","name":"Ruiqing Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4e8b","name":"Zeyu Chen","hidden":false},{"_id":"698424a7e34659da7e1f4e8c","name":"Qingqing Dang","hidden":false},{"_id":"698424a7e34659da7e1f4e8d","name":"Kaipeng Deng","hidden":false},{"_id":"698424a7e34659da7e1f4e8e","name":"Jiajun Jiang","hidden":false},{"_id":"698424a7e34659da7e1f4e8f","name":"Enlei Gong","hidden":false},{"_id":"698424a7e34659da7e1f4e90","name":"Guoxia Wang","hidden":false},{"_id":"698424a7e34659da7e1f4e91","name":"Yanlin Sha","hidden":false},{"_id":"698424a7e34659da7e1f4e92","name":"Yi Liu","hidden":false},{"_id":"698424a7e34659da7e1f4e93","name":"Yehan Zheng","hidden":false},{"_id":"698424a7e34659da7e1f4e94","name":"Weijian Xu","hidden":false},{"_id":"698424a7e34659da7e1f4e95","name":"Jiaxiang Liu","hidden":false},{"_id":"698424a7e34659da7e1f4e96","name":"Zengfeng Zeng","hidden":false},{"_id":"698424a7e34659da7e1f4e97","name":"Yingqi Qu","hidden":false},{"_id":"698424a7e34659da7e1f4e98","name":"Zhongli Li","hidden":false},{"_id":"698424a7e34659da7e1f4e99","name":"Zhengkun Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4e9a","name":"Xiyang Wang","hidden":false},{"_id":"698424a7e34659da7e1f4e9b","name":"Zixiang Xu","hidden":false},{"_id":"698424a7e34659da7e1f4e9c","name":"Xinchao Xu","hidden":false},{"_id":"698424a7e34659da7e1f4e9d","name":"Zhengjie Huang","hidden":false},{"_id":"698424a7e34659da7e1f4e9e","name":"Dong Wang","hidden":false},{"_id":"698424a7e34659da7e1f4e9f","name":"Bingjin Chen","hidden":false},{"_id":"698424a7e34659da7e1f4ea0","name":"Yue Chang","hidden":false},{"_id":"698424a7e34659da7e1f4ea1","name":"Xing Yuan","hidden":false},{"_id":"698424a7e34659da7e1f4ea2","name":"Shiwei Huang","hidden":false},{"_id":"698424a7e34659da7e1f4ea3","name":"Qiao Zhao","hidden":false},{"_id":"698424a7e34659da7e1f4ea4","name":"Xinzhe Ding","hidden":false},{"_id":"698424a7e34659da7e1f4ea5","name":"Shuangshuang Qiao","hidden":false},{"_id":"698424a7e34659da7e1f4ea6","name":"Baoshan Yang","hidden":false},{"_id":"698424a7e34659da7e1f4ea7","name":"Bihong Tang","hidden":false},{"_id":"698424a7e34659da7e1f4ea8","name":"Bin Li","hidden":false},{"_id":"698424a7e34659da7e1f4ea9","name":"Bingquan Wang","hidden":false},{"_id":"698424a7e34659da7e1f4eaa","name":"Binhan Tang","hidden":false},{"_id":"698424a7e34659da7e1f4eab","name":"Binxiong Zheng","hidden":false},{"_id":"698424a7e34659da7e1f4eac","name":"Bo Cui","hidden":false},{"_id":"698424a7e34659da7e1f4ead","name":"Bo Ke","hidden":false},{"_id":"698424a7e34659da7e1f4eae","name":"Bo Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4eaf","name":"Bowen Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4eb0","name":"Boyan Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4eb1","name":"Boyang Liu","hidden":false},{"_id":"698424a7e34659da7e1f4eb2","name":"Caiji Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4eb3","name":"Can Li","hidden":false},{"_id":"698424a7e34659da7e1f4eb4","name":"Chang Xu","hidden":false},{"_id":"698424a7e34659da7e1f4eb5","name":"Chao Pang","hidden":false},{"_id":"698424a7e34659da7e1f4eb6","name":"Chao Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4eb7","name":"Chaoyi Yuan","hidden":false},{"_id":"698424a7e34659da7e1f4eb8","name":"Chen Chen","hidden":false},{"_id":"698424a7e34659da7e1f4eb9","name":"Cheng Cui","hidden":false},{"_id":"698424a7e34659da7e1f4eba","name":"Chenlin Yin","hidden":false},{"_id":"698424a7e34659da7e1f4ebb","name":"Chun Gan","hidden":false},{"_id":"698424a7e34659da7e1f4ebc","name":"Chunguang Chai","hidden":false},{"_id":"698424a7e34659da7e1f4ebd","name":"Chuyu Fang","hidden":false},{"_id":"698424a7e34659da7e1f4ebe","name":"Cuiyun Han","hidden":false},{"_id":"698424a7e34659da7e1f4ebf","name":"Dan Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4ec0","name":"Danlei Feng","hidden":false},{"_id":"698424a7e34659da7e1f4ec1","name":"Danxiang Zhu","hidden":false},{"_id":"698424a7e34659da7e1f4ec2","name":"Dong Sun","hidden":false},{"_id":"698424a7e34659da7e1f4ec3","name":"Dongbo Li","hidden":false},{"_id":"698424a7e34659da7e1f4ec4","name":"Dongdong Li","hidden":false},{"_id":"698424a7e34659da7e1f4ec5","name":"Dongdong Liu","hidden":false},{"_id":"698424a7e34659da7e1f4ec6","name":"Dongxue Liu","hidden":false},{"_id":"698424a7e34659da7e1f4ec7","name":"Fan Ding","hidden":false},{"_id":"698424a7e34659da7e1f4ec8","name":"Fan Hu","hidden":false},{"_id":"698424a7e34659da7e1f4ec9","name":"Fan Li","hidden":false},{"_id":"698424a7e34659da7e1f4eca","name":"Fan Mo","hidden":false},{"_id":"698424a7e34659da7e1f4ecb","name":"Feisheng Wu","hidden":false},{"_id":"698424a7e34659da7e1f4ecc","name":"Fengwei Liu","hidden":false},{"_id":"698424a7e34659da7e1f4ecd","name":"Gangqiang Hu","hidden":false},{"_id":"698424a7e34659da7e1f4ece","name":"Gaofeng Lu","hidden":false},{"_id":"698424a7e34659da7e1f4ecf","name":"Gaopeng Yong","hidden":false},{"_id":"698424a7e34659da7e1f4ed0","name":"Gexiao Tian","hidden":false},{"_id":"698424a7e34659da7e1f4ed1","user":{"_id":"698419de94015f1e5eedacec","avatarUrl":"/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg","isPro":false,"fullname":"Guan Wang","user":"guanwcn","type":"user"},"name":"Guan Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:38.213Z","hidden":false},{"_id":"698424a7e34659da7e1f4ed2","name":"Guangchen Ni","hidden":false},{"_id":"698424a7e34659da7e1f4ed3","name":"Guangshuo Wu","hidden":false},{"_id":"698424a7e34659da7e1f4ed4","name":"Guanzhong Wang","hidden":false},{"_id":"698424a7e34659da7e1f4ed5","user":{"_id":"609cd5ab335f23cd2fa0f211","avatarUrl":"/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg","isPro":false,"fullname":"Guihua Liu","user":"LLLL","type":"user"},"name":"Guihua Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:31.029Z","hidden":false},{"_id":"698424a7e34659da7e1f4ed6","name":"Guishun Li","hidden":false},{"_id":"698424a7e34659da7e1f4ed7","name":"Haibin Li","hidden":false},{"_id":"698424a7e34659da7e1f4ed8","name":"Haijian Liang","hidden":false},{"_id":"698424a7e34659da7e1f4ed9","name":"Haipeng Ming","hidden":false},{"_id":"698424a7e34659da7e1f4eda","name":"Haisu Wang","hidden":false},{"_id":"698424a7e34659da7e1f4edb","name":"Haiyang Lu","hidden":false},{"_id":"698424a7e34659da7e1f4edc","name":"Haiye Lin","hidden":false},{"_id":"698424a7e34659da7e1f4edd","name":"Han Zhou","hidden":false},{"_id":"698424a7e34659da7e1f4ede","name":"Hangting Lou","hidden":false},{"_id":"698424a7e34659da7e1f4edf","name":"Hanwen Du","hidden":false},{"_id":"698424a7e34659da7e1f4ee0","name":"Hanzhi Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4ee1","name":"Hao Chen","hidden":false},{"_id":"698424a7e34659da7e1f4ee2","name":"Hao Du","hidden":false},{"_id":"698424a7e34659da7e1f4ee3","name":"Hao Liu","hidden":false},{"_id":"698424a7e34659da7e1f4ee4","name":"Hao Zhou","hidden":false},{"_id":"698424a7e34659da7e1f4ee5","name":"Haochen Jiang","hidden":false},{"_id":"698424a7e34659da7e1f4ee6","name":"Haodong Tian","hidden":false},{"_id":"698424a7e34659da7e1f4ee7","name":"Haoshuang Wang","hidden":false},{"_id":"698424a7e34659da7e1f4ee8","name":"Haozhe Geng","hidden":false},{"_id":"698424a7e34659da7e1f4ee9","name":"Heju Yin","hidden":false},{"_id":"698424a7e34659da7e1f4eea","name":"Hong Chen","hidden":false},{"_id":"698424a7e34659da7e1f4eeb","name":"Hongchen Xue","hidden":false},{"_id":"698424a7e34659da7e1f4eec","name":"Hongen Liu","hidden":false},{"_id":"698424a7e34659da7e1f4eed","name":"Honggeng Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4eee","name":"Hongji Xu","hidden":false},{"_id":"698424a7e34659da7e1f4eef","name":"Hongwei Chen","hidden":false},{"_id":"698424a7e34659da7e1f4ef0","name":"Hongyang Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4ef1","name":"Hongyuan Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4ef2","name":"Hua Lu","hidden":false},{"_id":"698424a7e34659da7e1f4ef3","name":"Huan Chen","hidden":false},{"_id":"698424a7e34659da7e1f4ef4","name":"Huan Wang","hidden":false},{"_id":"698424a7e34659da7e1f4ef5","name":"Huang He","hidden":false},{"_id":"698424a7e34659da7e1f4ef6","name":"Hui Liu","hidden":false},{"_id":"698424a7e34659da7e1f4ef7","name":"Hui Zhong","hidden":false},{"_id":"698424a7e34659da7e1f4ef8","name":"Huibin Ruan","hidden":false},{"_id":"698424a7e34659da7e1f4ef9","name":"Jiafeng Lu","hidden":false},{"_id":"698424a7e34659da7e1f4efa","name":"Jiage Liang","hidden":false},{"_id":"698424a7e34659da7e1f4efb","name":"Jiahao Hu","hidden":false},{"_id":"698424a7e34659da7e1f4efc","name":"Jiahao Hu","hidden":false},{"_id":"698424a7e34659da7e1f4efd","name":"Jiajie Yang","hidden":false},{"_id":"698424a7e34659da7e1f4efe","name":"Jialin Li","hidden":false},{"_id":"698424a7e34659da7e1f4eff","name":"Jian Chen","hidden":false},{"_id":"698424a7e34659da7e1f4f00","name":"Jian Wu","hidden":false},{"_id":"698424a7e34659da7e1f4f01","name":"Jianfeng Yang","hidden":false},{"_id":"698424a7e34659da7e1f4f02","name":"Jianguang Jiang","hidden":false},{"_id":"698424a7e34659da7e1f4f03","name":"Jianhua Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f04","name":"Jianye Chen","hidden":false},{"_id":"698424a7e34659da7e1f4f05","name":"Jiaodi Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f06","name":"Jiarui Zhou","hidden":false},{"_id":"698424a7e34659da7e1f4f07","name":"Jiawei Lv","hidden":false},{"_id":"698424a7e34659da7e1f4f08","name":"Jiaxin Zhou","hidden":false},{"_id":"698424a7e34659da7e1f4f09","name":"Jiaxuan Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f0a","name":"Jie Han","hidden":false},{"_id":"698424a7e34659da7e1f4f0b","name":"Jie Sun","hidden":false},{"_id":"698424a7e34659da7e1f4f0c","name":"Jiefan Fang","hidden":false},{"_id":"698424a7e34659da7e1f4f0d","name":"Jihan Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f0e","name":"Jihua Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f0f","name":"Jing Hu","hidden":false},{"_id":"698424a7e34659da7e1f4f10","name":"Jing Qian","hidden":false},{"_id":"698424a7e34659da7e1f4f11","name":"Jing Yan","hidden":false},{"_id":"698424a7e34659da7e1f4f12","name":"Jingdong Du","hidden":false},{"_id":"698424a7e34659da7e1f4f13","name":"Jingdong Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f14","name":"Jingjing Wu","hidden":false},{"_id":"698424a7e34659da7e1f4f15","name":"Jingyong Li","hidden":false},{"_id":"698424a7e34659da7e1f4f16","name":"Jinheng Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f17","name":"Jinjin Li","hidden":false},{"_id":"698424a7e34659da7e1f4f18","name":"Jinliang Lu","hidden":false},{"_id":"698424a7e34659da7e1f4f19","name":"Jinlin Yu","hidden":false},{"_id":"698424a7e34659da7e1f4f1a","name":"Jinnan Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f1b","name":"Jixiang Feng","hidden":false},{"_id":"698424a7e34659da7e1f4f1c","name":"Jiyi Huang","hidden":false},{"_id":"698424a7e34659da7e1f4f1d","name":"Jiyuan Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f1e","name":"Jun Liang","hidden":false},{"_id":"698424a7e34659da7e1f4f1f","name":"Jun Xia","hidden":false},{"_id":"698424a7e34659da7e1f4f20","name":"Jun Yu","hidden":false},{"_id":"698424a7e34659da7e1f4f21","name":"Junda Chen","hidden":false},{"_id":"698424a7e34659da7e1f4f22","name":"Junhao Feng","hidden":false},{"_id":"698424a7e34659da7e1f4f23","name":"Junhong Xiang","hidden":false},{"_id":"698424a7e34659da7e1f4f24","name":"Junliang Li","hidden":false},{"_id":"698424a7e34659da7e1f4f25","name":"Kai Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f26","name":"Kailun Chen","hidden":false},{"_id":"698424a7e34659da7e1f4f27","name":"Kairan Su","hidden":false},{"_id":"698424a7e34659da7e1f4f28","name":"Kang Hu","hidden":false},{"_id":"698424a7e34659da7e1f4f29","name":"Kangkang Zhou","hidden":false},{"_id":"698424a7e34659da7e1f4f2a","name":"Ke Chen","hidden":false},{"_id":"698424a7e34659da7e1f4f2b","name":"Ke Wei","hidden":false},{"_id":"698424a7e34659da7e1f4f2c","name":"Kui Huang","hidden":false},{"_id":"698424a7e34659da7e1f4f2d","name":"Kun Wu","hidden":false},{"_id":"698424a7e34659da7e1f4f2e","name":"Kunbin Chen","hidden":false},{"_id":"698424a7e34659da7e1f4f2f","name":"Lei Han","hidden":false},{"_id":"698424a7e34659da7e1f4f30","name":"Lei Sun","hidden":false},{"_id":"698424a7e34659da7e1f4f31","name":"Lei Wen","hidden":false},{"_id":"698424a7e34659da7e1f4f32","name":"Linghui Meng","hidden":false},{"_id":"698424a7e34659da7e1f4f33","user":{"_id":"641e69355c348064a8251471","avatarUrl":"/avatars/acad3877df27ff44ea3921bb43e34d53.svg","isPro":false,"fullname":"Linhao Yu","user":"HasuerYu","type":"user"},"name":"Linhao Yu","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:52:47.812Z","hidden":false},{"_id":"698424a7e34659da7e1f4f34","name":"Liping Ouyang","hidden":false},{"_id":"698424a7e34659da7e1f4f35","name":"Liwen Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f36","user":{"_id":"65cf859f88d13d8128bb8545","avatarUrl":"/avatars/aa18b993bd90d9c8a95913050cd955a8.svg","isPro":false,"fullname":"Longbin Ji","user":"robingg1","type":"user"},"name":"Longbin Ji","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:40.295Z","hidden":false},{"_id":"698424a7e34659da7e1f4f37","name":"Longzhi Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f38","name":"Meng Sun","hidden":false},{"_id":"698424a7e34659da7e1f4f39","name":"Meng Tian","hidden":false},{"_id":"698424a7e34659da7e1f4f3a","name":"Mengfei Li","hidden":false},{"_id":"698424a7e34659da7e1f4f3b","name":"Mengqi Zeng","hidden":false},{"_id":"698424a7e34659da7e1f4f3c","name":"Mengyu Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f3d","name":"Ming Hong","hidden":false},{"_id":"698424a7e34659da7e1f4f3e","name":"Mingcheng Zhou","hidden":false},{"_id":"698424a7e34659da7e1f4f3f","name":"Mingming Huang","hidden":false},{"_id":"698424a7e34659da7e1f4f40","name":"Mingxin Chen","hidden":false},{"_id":"698424a7e34659da7e1f4f41","name":"Mingzhu Cai","hidden":false},{"_id":"698424a7e34659da7e1f4f42","name":"Naibin Gu","hidden":false},{"_id":"698424a7e34659da7e1f4f43","name":"Nemin Qiu","hidden":false},{"_id":"698424a7e34659da7e1f4f44","name":"Nian Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f45","name":"Peng Qiu","hidden":false},{"_id":"698424a7e34659da7e1f4f46","name":"Peng Zhao","hidden":false},{"_id":"698424a7e34659da7e1f4f47","name":"Pengyu Zou","hidden":false},{"_id":"698424a7e34659da7e1f4f48","name":"Qi Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f49","name":"Qi Xin","hidden":false},{"_id":"698424a7e34659da7e1f4f4a","name":"Qian Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f4b","name":"Qiang Zhu","hidden":false},{"_id":"698424a7e34659da7e1f4f4c","name":"Qianhui Luo","hidden":false},{"_id":"698424a7e34659da7e1f4f4d","name":"Qianwei Yang","hidden":false},{"_id":"698424a7e34659da7e1f4f4e","name":"Qianyue He","hidden":false},{"_id":"698424a7e34659da7e1f4f4f","name":"Qifei Wu","hidden":false},{"_id":"698424a7e34659da7e1f4f50","name":"Qinrui Li","hidden":false},{"_id":"698424a7e34659da7e1f4f51","name":"Qiwen Bao","hidden":false},{"_id":"698424a7e34659da7e1f4f52","name":"Quan Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f53","name":"Quanxiang Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f54","name":"Qunyi Xie","hidden":false},{"_id":"698424a7e34659da7e1f4f55","name":"Rongrui Zhan","hidden":false},{"_id":"698424a7e34659da7e1f4f56","name":"Rufeng Dai","hidden":false},{"_id":"698424a7e34659da7e1f4f57","name":"Rui Peng","hidden":false},{"_id":"698424a7e34659da7e1f4f58","name":"Ruian Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f59","name":"Ruihao Xu","hidden":false},{"_id":"698424a7e34659da7e1f4f5a","name":"Ruijie Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f5b","name":"Ruixi Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f5c","name":"Ruixuan Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f5d","name":"Runsheng Shi","hidden":false},{"_id":"698424a7e34659da7e1f4f5e","name":"Ruting Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f5f","name":"Senbo Kang","hidden":false},{"_id":"698424a7e34659da7e1f4f60","name":"Shan Lu","hidden":false},{"_id":"698424a7e34659da7e1f4f61","name":"Shaofei Yu","hidden":false},{"_id":"698424a7e34659da7e1f4f62","name":"Shaotian Gong","hidden":false},{"_id":"698424a7e34659da7e1f4f63","name":"Shenwei Hu","hidden":false},{"_id":"698424a7e34659da7e1f4f64","name":"Shifeng Zheng","hidden":false},{"_id":"698424a7e34659da7e1f4f65","name":"Shihao Guo","hidden":false},{"_id":"698424a7e34659da7e1f4f66","name":"Shilong Fan","hidden":false},{"_id":"698424a7e34659da7e1f4f67","name":"Shiqin Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f68","name":"Shiwei Gu","hidden":false},{"_id":"698424a7e34659da7e1f4f69","name":"Shixi Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f6a","name":"Shuai Yao","hidden":false},{"_id":"698424a7e34659da7e1f4f6b","name":"Shuang Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f6c","name":"Shuangqiao Liu","hidden":false},{"_id":"698424a7e34659da7e1f4f6d","name":"Shuhao Liang","hidden":false},{"_id":"698424a7e34659da7e1f4f6e","name":"Shuwei He","hidden":false},{"_id":"698424a7e34659da7e1f4f6f","name":"Shuwen Yang","hidden":false},{"_id":"698424a7e34659da7e1f4f70","user":{"_id":"62769a608483d8e9ecd9b4f8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg","isPro":false,"fullname":"Sijun He","user":"sijunhe","type":"user"},"name":"Sijun He","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:33.392Z","hidden":false},{"_id":"698424a7e34659da7e1f4f71","user":{"_id":"64fada13d82fc6977d5e9c74","avatarUrl":"/avatars/776bf1257154289e919716637770ef52.svg","isPro":false,"fullname":"Siming Dai","user":"DesmonDay","type":"user"},"name":"Siming Dai","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:52:50.302Z","hidden":false},{"_id":"698424a7e34659da7e1f4f72","name":"Siming Wu","hidden":false},{"_id":"698424a7e34659da7e1f4f73","name":"Siyi Long","hidden":false},{"_id":"698424a7e34659da7e1f4f74","name":"Songhe Deng","hidden":false},{"_id":"698424a7e34659da7e1f4f75","name":"Suhui Dong","hidden":false},{"_id":"698424a7e34659da7e1f4f76","name":"Suyin Liang","hidden":false},{"_id":"698424a7e34659da7e1f4f77","name":"Teng Hu","hidden":false},{"_id":"698424a7e34659da7e1f4f78","name":"Tianchan Xu","hidden":false},{"_id":"698424a7e34659da7e1f4f79","name":"Tianliang Lv","hidden":false},{"_id":"698424a7e34659da7e1f4f7a","user":{"_id":"67bbe929593452cc18877606","avatarUrl":"/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg","isPro":false,"fullname":"tmyangcs","user":"youngtimmy","type":"user"},"name":"Tianmeng Yang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:36.143Z","hidden":false},{"_id":"698424a7e34659da7e1f4f7b","name":"Tianyi Wei","hidden":false},{"_id":"698424a7e34659da7e1f4f7c","name":"Tiezhu Gao","hidden":false},{"_id":"698424a7e34659da7e1f4f7d","name":"Ting Sun","hidden":false},{"_id":"698424a7e34659da7e1f4f7e","name":"Ting Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f7f","name":"Tingdan Luo","hidden":false},{"_id":"698424a7e34659da7e1f4f80","name":"Wei He","hidden":false},{"_id":"698424a7e34659da7e1f4f81","name":"Wei Luan","hidden":false},{"_id":"698424a7e34659da7e1f4f82","name":"Wei Yin","hidden":false},{"_id":"698424a7e34659da7e1f4f83","name":"Wei Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f84","name":"Wei Zhou","hidden":false},{"_id":"698424a7e34659da7e1f4f85","name":"Weibao Gong","hidden":false},{"_id":"698424a7e34659da7e1f4f86","name":"Weibin Li","hidden":false},{"_id":"698424a7e34659da7e1f4f87","name":"Weicheng Huang","hidden":false},{"_id":"698424a7e34659da7e1f4f88","name":"Weichong Dang","hidden":false},{"_id":"698424a7e34659da7e1f4f89","name":"Weiguo Zhu","hidden":false},{"_id":"698424a7e34659da7e1f4f8a","name":"Weilong Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f8b","name":"Weiqi Tan","hidden":false},{"_id":"698424a7e34659da7e1f4f8c","name":"Wen Huang","hidden":false},{"_id":"698424a7e34659da7e1f4f8d","name":"Wenbin Chang","hidden":false},{"_id":"698424a7e34659da7e1f4f8e","name":"Wenjing Du","hidden":false},{"_id":"698424a7e34659da7e1f4f8f","name":"Wenlong Miao","hidden":false},{"_id":"698424a7e34659da7e1f4f90","name":"Wenpei Luo","hidden":false},{"_id":"698424a7e34659da7e1f4f91","name":"Wenquan Wu","hidden":false},{"_id":"698424a7e34659da7e1f4f92","name":"Xi Shi","hidden":false},{"_id":"698424a7e34659da7e1f4f93","name":"Xi Zhao","hidden":false},{"_id":"698424a7e34659da7e1f4f94","name":"Xiang Gao","hidden":false},{"_id":"698424a7e34659da7e1f4f95","name":"Xiangguo Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4f96","name":"Xiangrui Yu","hidden":false},{"_id":"698424a7e34659da7e1f4f97","name":"Xiangsen Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f98","name":"Xiangzhe Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f99","name":"Xianlong Luo","hidden":false},{"_id":"698424a7e34659da7e1f4f9a","name":"Xianying Ma","hidden":false},{"_id":"698424a7e34659da7e1f4f9b","name":"Xiao Tan","hidden":false},{"_id":"698424a7e34659da7e1f4f9c","name":"Xiaocong Lin","hidden":false},{"_id":"698424a7e34659da7e1f4f9d","name":"Xiaofei Wang","hidden":false},{"_id":"698424a7e34659da7e1f4f9e","name":"Xiaofeng Peng","hidden":false},{"_id":"698424a7e34659da7e1f4f9f","name":"Xiaofeng Wu","hidden":false},{"_id":"698424a7e34659da7e1f4fa0","name":"Xiaojian Xu","hidden":false},{"_id":"698424a7e34659da7e1f4fa1","name":"Xiaolan Yuan","hidden":false},{"_id":"698424a7e34659da7e1f4fa2","name":"Xiaopeng Cui","hidden":false},{"_id":"698424a7e34659da7e1f4fa3","name":"Xiaotian Han","hidden":false},{"_id":"698424a7e34659da7e1f4fa4","name":"Xiaoxiong Liu","hidden":false},{"_id":"698424a7e34659da7e1f4fa5","name":"Xiaoxu Fei","hidden":false},{"_id":"698424a7e34659da7e1f4fa6","name":"Xiaoxuan Wu","hidden":false},{"_id":"698424a7e34659da7e1f4fa7","user":{"_id":"664395621b88258a527cd7d1","avatarUrl":"/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg","isPro":false,"fullname":"Kira","user":"Kira-wang","type":"user"},"name":"Xiaoyu Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:25.774Z","hidden":false},{"_id":"698424a7e34659da7e1f4fa8","name":"Xiaoyu Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4fa9","name":"Xin Sun","hidden":false},{"_id":"698424a7e34659da7e1f4faa","name":"Xin Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fab","name":"Xinhui Huang","hidden":false},{"_id":"698424a7e34659da7e1f4fac","name":"Xinming Zhu","hidden":false},{"_id":"698424a7e34659da7e1f4fad","name":"Xintong Yu","hidden":false},{"_id":"698424a7e34659da7e1f4fae","name":"Xinyi Xu","hidden":false},{"_id":"698424a7e34659da7e1f4faf","name":"Xinyu Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fb0","name":"Xiuxian Li","hidden":false},{"_id":"698424a7e34659da7e1f4fb1","name":"XuanShi Zhu","hidden":false},{"_id":"698424a7e34659da7e1f4fb2","name":"Xue Xu","hidden":false},{"_id":"698424a7e34659da7e1f4fb3","name":"Xueying Lv","hidden":false},{"_id":"698424a7e34659da7e1f4fb4","name":"Xuhong Li","hidden":false},{"_id":"698424a7e34659da7e1f4fb5","name":"Xulong Wei","hidden":false},{"_id":"698424a7e34659da7e1f4fb6","name":"Xuyi Chen","hidden":false},{"_id":"698424a7e34659da7e1f4fb7","name":"Yabing Shi","hidden":false},{"_id":"698424a7e34659da7e1f4fb8","name":"Yafeng Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fb9","name":"Yamei Li","hidden":false},{"_id":"698424a7e34659da7e1f4fba","name":"Yan Liu","hidden":false},{"_id":"698424a7e34659da7e1f4fbb","name":"Yanfu Cheng","hidden":false},{"_id":"698424a7e34659da7e1f4fbc","name":"Yang Gao","hidden":false},{"_id":"698424a7e34659da7e1f4fbd","name":"Yang Liang","hidden":false},{"_id":"698424a7e34659da7e1f4fbe","name":"Yang Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fbf","name":"Yang Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fc0","name":"Yang Yang","hidden":false},{"_id":"698424a7e34659da7e1f4fc1","name":"Yanlong Liu","hidden":false},{"_id":"698424a7e34659da7e1f4fc2","name":"Yannian Fu","hidden":false},{"_id":"698424a7e34659da7e1f4fc3","name":"Yanpeng Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fc4","name":"Yanzheng Lin","hidden":false},{"_id":"698424a7e34659da7e1f4fc5","name":"Yao Chen","hidden":false},{"_id":"698424a7e34659da7e1f4fc6","name":"Yaozong Shen","hidden":false},{"_id":"698424a7e34659da7e1f4fc7","name":"Yaqian Han","hidden":false},{"_id":"698424a7e34659da7e1f4fc8","name":"Yehua Yang","hidden":false},{"_id":"698424a7e34659da7e1f4fc9","name":"Yekun Chai","hidden":false},{"_id":"698424a7e34659da7e1f4fca","name":"Yesong Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fcb","name":"Yi Song","hidden":false},{"_id":"698424a7e34659da7e1f4fcc","name":"Yichen Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4fcd","name":"Yifei Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fce","name":"Yifeng Guo","hidden":false},{"_id":"698424a7e34659da7e1f4fcf","name":"Yifeng Kou","hidden":false},{"_id":"698424a7e34659da7e1f4fd0","name":"Yilong Chen","hidden":false},{"_id":"698424a7e34659da7e1f4fd1","name":"Yilong Guo","hidden":false},{"_id":"698424a7e34659da7e1f4fd2","name":"Yiming Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fd3","name":"Ying Chen","hidden":false},{"_id":"698424a7e34659da7e1f4fd4","name":"Ying Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fd5","name":"Yingsheng Wu","hidden":false},{"_id":"698424a7e34659da7e1f4fd6","name":"Yingzhan Lin","hidden":false},{"_id":"698424a7e34659da7e1f4fd7","name":"Yinqi Yang","hidden":false},{"_id":"698424a7e34659da7e1f4fd8","name":"Yiran Xing","hidden":false},{"_id":"698424a7e34659da7e1f4fd9","name":"Yishu Lei","hidden":false},{"_id":"698424a7e34659da7e1f4fda","name":"Yixiang Tu","hidden":false},{"_id":"698424a7e34659da7e1f4fdb","name":"Yiyan Chen","hidden":false},{"_id":"698424a7e34659da7e1f4fdc","name":"Yong Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4fdd","name":"Yonghua Li","hidden":false},{"_id":"698424a7e34659da7e1f4fde","name":"Yongqiang Ma","hidden":false},{"_id":"698424a7e34659da7e1f4fdf","name":"Yongxing Dai","hidden":false},{"_id":"698424a7e34659da7e1f4fe0","name":"Yongyue Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4fe1","name":"Yu Ran","hidden":false},{"_id":"698424a7e34659da7e1f4fe2","name":"Yu Sun","hidden":false},{"_id":"698424a7e34659da7e1f4fe3","name":"Yu-Wen Michael Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4fe4","name":"Yuang Liu","hidden":false},{"_id":"698424a7e34659da7e1f4fe5","name":"Yuanle Liu","hidden":false},{"_id":"698424a7e34659da7e1f4fe6","name":"Yuanyuan Zhou","hidden":false},{"_id":"698424a7e34659da7e1f4fe7","name":"Yubo Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4fe8","name":"Yuchen Han","hidden":false},{"_id":"698424a7e34659da7e1f4fe9","name":"Yucheng Wang","hidden":false},{"_id":"698424a7e34659da7e1f4fea","name":"Yude Gao","hidden":false},{"_id":"698424a7e34659da7e1f4feb","name":"Yuedong Luo","hidden":false},{"_id":"698424a7e34659da7e1f4fec","name":"Yuehu Dong","hidden":false},{"_id":"698424a7e34659da7e1f4fed","name":"Yufeng Hu","hidden":false},{"_id":"698424a7e34659da7e1f4fee","name":"Yuhui Cao","hidden":false},{"_id":"698424a7e34659da7e1f4fef","name":"Yuhui Yun","hidden":false},{"_id":"698424a7e34659da7e1f4ff0","name":"Yukun Chen","hidden":false},{"_id":"698424a7e34659da7e1f4ff1","name":"Yukun Gao","hidden":false},{"_id":"698424a7e34659da7e1f4ff2","name":"Yukun Li","hidden":false},{"_id":"698424a7e34659da7e1f4ff3","name":"Yumeng Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4ff4","name":"Yun Fan","hidden":false},{"_id":"698424a7e34659da7e1f4ff5","name":"Yun Ma","hidden":false},{"_id":"698424a7e34659da7e1f4ff6","name":"Yunfei Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4ff7","name":"Yunshen Xie","hidden":false},{"_id":"698424a7e34659da7e1f4ff8","name":"Yuping Xu","hidden":false},{"_id":"698424a7e34659da7e1f4ff9","name":"Yuqin Zhang","hidden":false},{"_id":"698424a7e34659da7e1f4ffa","name":"Yuqing Liu","hidden":false},{"_id":"698424a7e34659da7e1f4ffb","name":"Yurui Li","hidden":false},{"_id":"698424a7e34659da7e1f4ffc","name":"Yuwen Wang","hidden":false},{"_id":"698424a7e34659da7e1f4ffd","name":"Yuxiang Lu","hidden":false},{"_id":"698424a7e34659da7e1f4ffe","name":"Zefeng Cai","hidden":false},{"_id":"698424a7e34659da7e1f4fff","name":"Zelin Zhao","hidden":false},{"_id":"698424a7e34659da7e1f5000","name":"Zelun Zhang","hidden":false},{"_id":"698424a7e34659da7e1f5001","name":"Zenan Lin","hidden":false},{"_id":"698424a7e34659da7e1f5002","name":"Zezhao Dong","hidden":false},{"_id":"698424a7e34659da7e1f5003","name":"Zhaowu Pan","hidden":false},{"_id":"698424a7e34659da7e1f5004","name":"Zhaoyu Liu","hidden":false},{"_id":"698424a7e34659da7e1f5005","name":"Zhe Dong","hidden":false},{"_id":"698424a7e34659da7e1f5006","name":"Zhe Zhang","hidden":false},{"_id":"698424a7e34659da7e1f5007","name":"Zhen Zhang","hidden":false},{"_id":"698424a7e34659da7e1f5008","name":"Zhengfan Wu","hidden":false},{"_id":"698424a7e34659da7e1f5009","name":"Zhengrui Wei","hidden":false},{"_id":"698424a7e34659da7e1f500a","name":"Zhengsheng Ning","hidden":false},{"_id":"698424a7e34659da7e1f500b","name":"Zhenxing Li","hidden":false},{"_id":"698424a7e34659da7e1f500c","name":"Zhenyu Li","hidden":false},{"_id":"698424a7e34659da7e1f500d","name":"Zhenyu Qian","hidden":false},{"_id":"698424a7e34659da7e1f500e","name":"Zhenyun Li","hidden":false},{"_id":"698424a7e34659da7e1f500f","name":"Zhi Li","hidden":false},{"_id":"698424a7e34659da7e1f5010","name":"Zhichao Chen","hidden":false},{"_id":"698424a7e34659da7e1f5011","name":"Zhicheng Dong","hidden":false},{"_id":"698424a7e34659da7e1f5012","name":"Zhida Feng","hidden":false},{"_id":"698424a7e34659da7e1f5013","name":"Zhifan Feng","hidden":false},{"_id":"698424a7e34659da7e1f5014","name":"Zhihao Deng","hidden":false},{"_id":"698424a7e34659da7e1f5015","name":"Zhijin Yu","hidden":false},{"_id":"698424a7e34659da7e1f5016","name":"Zhiyang Chen","hidden":false},{"_id":"698424a7e34659da7e1f5017","name":"Zhonghui Zheng","hidden":false},{"_id":"698424a7e34659da7e1f5018","name":"Zhuangzhuang Guo","hidden":false},{"_id":"698424a7e34659da7e1f5019","name":"Zhujun Zhang","hidden":false},{"_id":"698424a7e34659da7e1f501a","name":"Zhuo Sun","hidden":false},{"_id":"698424a7e34659da7e1f501b","name":"Zichang Liu","hidden":false},{"_id":"698424a7e34659da7e1f501c","name":"Zihan Lin","hidden":false},{"_id":"698424a7e34659da7e1f501d","name":"Zihao Huang","hidden":false},{"_id":"698424a7e34659da7e1f501e","name":"Zihe Zhu","hidden":false},{"_id":"698424a7e34659da7e1f501f","name":"Ziheng Zhao","hidden":false},{"_id":"698424a7e34659da7e1f5020","name":"Ziping Chen","hidden":false},{"_id":"698424a7e34659da7e1f5021","name":"Zixuan Zhu","hidden":false},{"_id":"698424a7e34659da7e1f5022","name":"Ziyang Xu","hidden":false},{"_id":"698424a7e34659da7e1f5023","name":"Ziyi Liang","hidden":false},{"_id":"698424a7e34659da7e1f5024","name":"Ziyuan Gao","hidden":false}],"publishedAt":"2026-02-04T16:18:15.000Z","submittedOnDailyAt":"2026-02-05T02:34:05.150Z","title":"ERNIE 5.0 Technical Report","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.","upvotes":198,"discussionId":"698424a7e34659da7e1f5025","ai_summary":"ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.","ai_keywords":["autoregressive foundation model","unified multimodal understanding","unified next-group-of-tokens prediction objective","mixture-of-experts","modality-agnostic expert routing","elastic training paradigm","reinforcement learning","sparse MoE architecture"]},"publishedAt":"2026-02-04T11:18:15.000Z","title":"ERNIE 5.0 Technical Report","summary":"In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.03152","authors":[{"_id":"698406a2e34659da7e1f4d65","name":"Yifei Wang","hidden":false},{"_id":"698406a2e34659da7e1f4d66","name":"Yueqi Wang","hidden":false},{"_id":"698406a2e34659da7e1f4d67","name":"Zhenrui Yue","hidden":false},{"_id":"698406a2e34659da7e1f4d68","name":"Huimin Zeng","hidden":false},{"_id":"698406a2e34659da7e1f4d69","name":"Yong Wang","hidden":false},{"_id":"698406a2e34659da7e1f4d6a","name":"Ismini Lourentzou","hidden":false},{"_id":"698406a2e34659da7e1f4d6b","name":"Zhengzhong Tu","hidden":false},{"_id":"698406a2e34659da7e1f4d6c","name":"Xiangxiang Chu","hidden":false},{"_id":"698406a2e34659da7e1f4d6d","name":"Julian McAuley","hidden":false}],"publishedAt":"2026-02-03T06:09:06.000Z","submittedOnDailyAt":"2026-02-05T08:51:33.236Z","title":"FASA: Frequency-aware Sparse Attention","submittedOnDailyBy":{"_id":"66d255e3947594430c723ff6","avatarUrl":"/avatars/c56e4792332a01bf34085a75ee64916e.svg","isPro":false,"fullname":"xiaochonglinghu","user":"xiaochonglinghu","type":"user"},"summary":"The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56times speedup using just 18.9\\% of the cache on AIME24.","upvotes":103,"discussionId":"698406a2e34659da7e1f4d6e","ai_summary":"FASA is a novel framework that uses query-aware token eviction and functional sparsity in RoPE to reduce KV cache memory usage while maintaining high performance in long-context LLM tasks.","ai_keywords":["Large Language Models","Key Value cache","token pruning","attention sparsity","query-dependent token importance","RoPE","functional sparsity","frequency-chunk level","dominant frequency-chunks","token eviction","attention computation","long-context tasks","sequence modeling","CoT reasoning","token-eviction baselines","KV cache memory footprint","computational cost","LongBench-V1","AIME24"],"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}},"publishedAt":"2026-02-03T01:09:06.000Z","title":"FASA: Frequency-aware Sparse Attention","summary":"The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56times speedup using just 18.9\\% of the cache on AIME24.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03152.png","numComments":3,"submittedBy":{"_id":"66d255e3947594430c723ff6","avatarUrl":"/avatars/c56e4792332a01bf34085a75ee64916e.svg","fullname":"xiaochonglinghu","name":"xiaochonglinghu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.04634","authors":[{"_id":"69840a42e34659da7e1f4da8","user":{"_id":"653a5b0f7c01c693a16dd184","avatarUrl":"/avatars/4b43d88709dc8037250404452e81adcf.svg","isPro":false,"fullname":"Zelai Xu","user":"zelaix","type":"user"},"name":"Zelai Xu","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:21.312Z","hidden":false},{"_id":"69840a42e34659da7e1f4da9","name":"Zhexuan Xu","hidden":false},{"_id":"69840a42e34659da7e1f4daa","user":{"_id":"683fb41cb1bf6fbcce6bc205","avatarUrl":"/avatars/544ff46b9ff78f8420981fa507da767e.svg","isPro":false,"fullname":"Ruize Zhang","user":"Ruize-Zhang","type":"user"},"name":"Ruize Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:19.167Z","hidden":false},{"_id":"69840a42e34659da7e1f4dab","name":"Chunyang Zhu","hidden":false},{"_id":"69840a42e34659da7e1f4dac","name":"Shi Yu","hidden":false},{"_id":"69840a42e34659da7e1f4dad","name":"Weilin Liu","hidden":false},{"_id":"69840a42e34659da7e1f4dae","name":"Quanlu Zhang","hidden":false},{"_id":"69840a42e34659da7e1f4daf","name":"Wenbo Ding","hidden":false},{"_id":"69840a42e34659da7e1f4db0","name":"Chao Yu","hidden":false},{"_id":"69840a42e34659da7e1f4db1","name":"Yu Wang","hidden":false}],"publishedAt":"2026-02-04T15:05:12.000Z","submittedOnDailyAt":"2026-02-05T02:30:13.468Z","title":"WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning","submittedOnDailyBy":{"_id":"653a5b0f7c01c693a16dd184","avatarUrl":"/avatars/4b43d88709dc8037250404452e81adcf.svg","isPro":false,"fullname":"Zelai Xu","user":"zelaix","type":"user"},"summary":"Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.","upvotes":71,"discussionId":"69840a43e34659da7e1f4db2","projectPage":"https://wideseek-r1.github.io/","githubRepo":"https://github.com/RLinf/RLinf/tree/main/examples/wideseek_r1","githubRepoAddedBy":"user","ai_summary":"Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents.","ai_keywords":["Large Language Models","multi-agent systems","multi-agent reinforcement learning","lead-agent-subagent framework","parallel execution","information seeking","WideSearch benchmark","F1 score"],"githubStars":2379,"organization":{"_id":"689ea978824b212c988bc8f5","name":"RLinf","fullname":"RLinf","avatar":"https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"}},"publishedAt":"2026-02-04T10:05:12.000Z","title":"WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning","summary":"Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04634.png","numComments":2,"submittedBy":{"_id":"653a5b0f7c01c693a16dd184","avatarUrl":"/avatars/4b43d88709dc8037250404452e81adcf.svg","fullname":"Zelai Xu","name":"zelaix","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"689ea978824b212c988bc8f5","name":"RLinf","fullname":"RLinf","avatar":"https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.04145","authors":[{"_id":"698412bbe34659da7e1f4e04","user":{"_id":"64fc20d899123d7698a30e61","avatarUrl":"/avatars/9231982cf70a0689f50accedf1004702.svg","isPro":false,"fullname":"Jinyuan Li","user":"jinyuan222","type":"user"},"name":"Jinyuan Li","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:57.944Z","hidden":false},{"_id":"698412bbe34659da7e1f4e05","name":"Chengsong Huang","hidden":false},{"_id":"698412bbe34659da7e1f4e06","name":"Langlin Huang","hidden":false},{"_id":"698412bbe34659da7e1f4e07","name":"Shaoyang Xu","hidden":false},{"_id":"698412bbe34659da7e1f4e08","name":"Haolin Liu","hidden":false},{"_id":"698412bbe34659da7e1f4e09","name":"Wenxuan Zhang","hidden":false},{"_id":"698412bbe34659da7e1f4e0a","name":"Jiaxin Huang","hidden":false}],"publishedAt":"2026-02-04T02:27:38.000Z","submittedOnDailyAt":"2026-02-05T01:21:27.343Z","title":"Training Data Efficiency in Multimodal Process Reward Models","submittedOnDailyBy":{"_id":"65e02d89574e5aa0e9ce3efa","avatarUrl":"/avatars/2ab152a10b21d81fb1defc726b8e951a.svg","isPro":false,"fullname":"Langlin Huang","user":"shrango","type":"user"},"summary":"Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.","upvotes":70,"discussionId":"698412bbe34659da7e1f4e0b","ai_summary":"Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data.","ai_keywords":["Multimodal Process Reward Models","Monte Carlo-annotated corpora","VisualProcessBench","Balanced-Information Score","label mixtures","label reliability","gradient updates","data efficiency"]},"publishedAt":"2026-02-03T21:27:38.000Z","title":"Training Data Efficiency in Multimodal Process Reward Models","summary":"Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04145.png","numComments":1,"submittedBy":{"_id":"65e02d89574e5aa0e9ce3efa","avatarUrl":"/avatars/2ab152a10b21d81fb1defc726b8e951a.svg","fullname":"Langlin Huang","name":"shrango","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.04804","authors":[{"_id":"69841218e34659da7e1f4df3","user":{"_id":"66100bacac50abb8d56dece6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66100bacac50abb8d56dece6/fd-4VMpb_1nl903yAIK4K.jpeg","isPro":false,"fullname":"Ding Yue","user":"dingyue1011","type":"user"},"name":"Yue Ding","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:01.167Z","hidden":false},{"_id":"69841218e34659da7e1f4df4","name":"Yiyan Ji","hidden":false},{"_id":"69841218e34659da7e1f4df5","user":{"_id":"64b76528fdb702b3d8641514","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/Ho-uWcQCAEIURM1lhWEWJ.jpeg","isPro":false,"fullname":"Jungang Li","user":"Jungang","type":"user"},"name":"Jungang Li","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:09.654Z","hidden":false},{"_id":"69841218e34659da7e1f4df6","user":{"_id":"66a0caa1a7a6ed88ad1c0ddf","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66a0caa1a7a6ed88ad1c0ddf/WoOP24-ruuHy4ryNhRp0D.jpeg","isPro":false,"fullname":"Xuyang Liu","user":"xuyang-liu16","type":"user"},"name":"Xuyang Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:06.046Z","hidden":false},{"_id":"69841218e34659da7e1f4df7","name":"Xinlong Chen","hidden":false},{"_id":"69841218e34659da7e1f4df8","name":"Junfei Wu","hidden":false},{"_id":"69841218e34659da7e1f4df9","name":"Bozhou Li","hidden":false},{"_id":"69841218e34659da7e1f4dfa","name":"Bohan Zeng","hidden":false},{"_id":"69841218e34659da7e1f4dfb","name":"Yang Shi","hidden":false},{"_id":"69841218e34659da7e1f4dfc","user":{"_id":"66ac46766c3f950f4f10b9f9","avatarUrl":"/avatars/027b573bc6e5b18107e762645cec6069.svg","isPro":false,"fullname":"Yushuo Guan","user":"UnnamedWatcher","type":"user"},"name":"Yushuo Guan","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:04.081Z","hidden":false},{"_id":"69841218e34659da7e1f4dfd","name":"Yuanxing Zhang","hidden":false},{"_id":"69841218e34659da7e1f4dfe","name":"Jiaheng Liu","hidden":false},{"_id":"69841218e34659da7e1f4dff","name":"Qiang Liu","hidden":false},{"_id":"69841218e34659da7e1f4e00","name":"Pengfei Wan","hidden":false},{"_id":"69841218e34659da7e1f4e01","name":"Liang Wang","hidden":false}],"publishedAt":"2026-02-04T17:51:05.000Z","submittedOnDailyAt":"2026-02-05T02:48:46.241Z","title":"OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models","submittedOnDailyBy":{"_id":"64b76528fdb702b3d8641514","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/Ho-uWcQCAEIURM1lhWEWJ.jpeg","isPro":false,"fullname":"Jungang Li","user":"Jungang","type":"user"},"summary":"Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.","upvotes":41,"discussionId":"69841219e34659da7e1f4e02","ai_summary":"OmniSIFT is a modality-asymmetric token compression framework for Omni-LLMs that reduces computational overhead through spatio-temporal video pruning and vision-guided audio selection while maintaining superior performance.","ai_keywords":["Omni-modal Large Language Models","token compression","spatio-temporal video pruning","vision-guided audio selection","differentiable straight-through estimator","end-to-end optimization","multimodal token sequences"]},"publishedAt":"2026-02-04T12:51:05.000Z","title":"OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models","summary":"Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04804.png","numComments":1,"submittedBy":{"_id":"64b76528fdb702b3d8641514","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/Ho-uWcQCAEIURM1lhWEWJ.jpeg","fullname":"Jungang Li","name":"Jungang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.03560","authors":[{"_id":"6984201be34659da7e1f4e3c","name":"Yizhao Gao","hidden":false},{"_id":"6984201be34659da7e1f4e3d","name":"Jianyu Wei","hidden":false},{"_id":"6984201be34659da7e1f4e3e","name":"Qihao Zhang","hidden":false},{"_id":"6984201be34659da7e1f4e3f","name":"Yu Cheng","hidden":false},{"_id":"6984201be34659da7e1f4e40","name":"Shimao Chen","hidden":false},{"_id":"6984201be34659da7e1f4e41","name":"Zhengju Tang","hidden":false},{"_id":"6984201be34659da7e1f4e42","name":"Zihan Jiang","hidden":false},{"_id":"6984201be34659da7e1f4e43","name":"Yifan Song","hidden":false},{"_id":"6984201be34659da7e1f4e44","name":"Hailin Zhang","hidden":false},{"_id":"6984201be34659da7e1f4e45","name":"Liang Zhao","hidden":false},{"_id":"6984201be34659da7e1f4e46","name":"Bo Yang","hidden":false},{"_id":"6984201be34659da7e1f4e47","name":"Gang Wang","hidden":false},{"_id":"6984201be34659da7e1f4e48","name":"Shijie Cao","hidden":false},{"_id":"6984201be34659da7e1f4e49","name":"Fuli Luo","hidden":false}],"publishedAt":"2026-02-03T14:05:57.000Z","submittedOnDailyAt":"2026-02-05T02:18:16.310Z","title":"HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing","submittedOnDailyBy":{"_id":"661c96f48921f03a9dae04c3","avatarUrl":"/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg","isPro":false,"fullname":"Yizhao Gao","user":"LongMountain","type":"user"},"summary":"This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.","upvotes":36,"discussionId":"6984201be34659da7e1f4e4a","ai_summary":"Hybrid Sparse Attention architecture interleaves full and sparse attention layers, using full attention output to guide sparse layer token selection and cache reuse for improved efficiency and performance.","ai_keywords":["Hybrid Sparse Attention","sparse attention layers","full attention layer","token selection","KV caches","attention mechanism","MoE models","computational efficiency","memory reduction"],"organization":{"_id":"680cb4c37f289defb2210940","name":"XiaomiMiMo","fullname":"Xiaomi MiMo","avatar":"https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"}},"publishedAt":"2026-02-03T09:05:57.000Z","title":"HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing","summary":"This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03560.png","numComments":1,"submittedBy":{"_id":"661c96f48921f03a9dae04c3","avatarUrl":"/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg","fullname":"Yizhao Gao","name":"LongMountain","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"680cb4c37f289defb2210940","name":"XiaomiMiMo","fullname":"Xiaomi MiMo","avatar":"https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.04515","authors":[{"_id":"69840866e34659da7e1f4d86","name":"Yu Bai","hidden":false},{"_id":"69840866e34659da7e1f4d87","name":"MingMing Yu","hidden":false},{"_id":"69840866e34659da7e1f4d88","name":"Chaojie Li","hidden":false},{"_id":"69840866e34659da7e1f4d89","name":"Ziyi Bai","hidden":false},{"_id":"69840866e34659da7e1f4d8a","name":"Xinlong Wang","hidden":false},{"_id":"69840866e34659da7e1f4d8b","user":{"_id":"61e52be53d6dbb1da842316a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg","isPro":false,"fullname":"Brje Karlsson","user":"tellarin","type":"user"},"name":"Brje F. Karlsson","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:26.009Z","hidden":false}],"publishedAt":"2026-02-04T13:04:56.000Z","submittedOnDailyAt":"2026-02-05T00:35:43.372Z","title":"EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models","submittedOnDailyBy":{"_id":"61e52be53d6dbb1da842316a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg","isPro":false,"fullname":"Brje Karlsson","user":"tellarin","type":"user"},"summary":"Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.","upvotes":31,"discussionId":"69840866e34659da7e1f4d8c","projectPage":"https://baai-agents.github.io/RoboNoid/EgoActor/","ai_summary":"EgoActor is a unified vision-language model that translates high-level instructions into precise humanoid robot actions through integrated perception and execution across simulated and real-world environments.","ai_keywords":["vision-language model","locomotion primitives","head movements","manipulation commands","human-robot interactions","egocentric RGB-only data","spatial reasoning","question-answering","simulated environment demonstrations","action inference","motor execution"]},"publishedAt":"2026-02-04T08:04:56.000Z","title":"EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models","summary":"Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04515.png","numComments":1,"submittedBy":{"_id":"61e52be53d6dbb1da842316a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg","fullname":"Brje Karlsson","name":"tellarin","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":27,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.02958","authors":[{"_id":"698409abe34659da7e1f4d8e","name":"Haocheng Xi","hidden":false},{"_id":"698409abe34659da7e1f4d8f","name":"Shuo Yang","hidden":false},{"_id":"698409abe34659da7e1f4d90","name":"Yilong Zhao","hidden":false},{"_id":"698409abe34659da7e1f4d91","name":"Muyang Li","hidden":false},{"_id":"698409abe34659da7e1f4d92","name":"Han Cai","hidden":false},{"_id":"698409abe34659da7e1f4d93","name":"Xingyang Li","hidden":false},{"_id":"698409abe34659da7e1f4d94","name":"Yujun Lin","hidden":false},{"_id":"698409abe34659da7e1f4d95","name":"Zhuoyang Zhang","hidden":false},{"_id":"698409abe34659da7e1f4d96","user":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","isPro":false,"fullname":"Jintao Zhang","user":"jt-zhang","type":"user"},"name":"Jintao Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:23.289Z","hidden":false},{"_id":"698409abe34659da7e1f4d97","name":"Xiuyu Li","hidden":false},{"_id":"698409abe34659da7e1f4d98","name":"Zhiying Xu","hidden":false},{"_id":"698409abe34659da7e1f4d99","name":"Jun Wu","hidden":false},{"_id":"698409abe34659da7e1f4d9a","name":"Chenfeng Xu","hidden":false},{"_id":"698409abe34659da7e1f4d9b","name":"Ion Stoica","hidden":false},{"_id":"698409abe34659da7e1f4d9c","name":"Song Han","hidden":false},{"_id":"698409abe34659da7e1f4d9d","name":"Kurt Keutzer","hidden":false}],"publishedAt":"2026-02-03T00:54:32.000Z","submittedOnDailyAt":"2026-02-05T00:40:03.849Z","title":"Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization","submittedOnDailyBy":{"_id":"66ce751a8ec9fda2cf5a9e85","avatarUrl":"/avatars/c17093ca81dad007b3e50bae503955a7.svg","isPro":false,"fullname":"Haocheng Xi","user":"xihc-ucb","type":"user"},"summary":"Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.","upvotes":31,"discussionId":"698409ace34659da7e1f4d9e","ai_summary":"Quant VideoGen addresses KV cache memory limitations in autoregressive video diffusion models through semantic-aware smoothing and progressive residual quantization, achieving significant memory reduction with minimal latency impact.","ai_keywords":["KV cache","autoregressive video diffusion","video spatiotemporal redundancy","Semantic Aware Smoothing","Progressive Residual Quantization","quantization error","Pareto frontier","memory efficiency"],"organization":{"_id":"66b1baeff10262fc4fa61961","name":"UCBerkeley","fullname":"University of California, Berkeley","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}},"publishedAt":"2026-02-02T19:54:32.000Z","title":"Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization","summary":"Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02958.png","numComments":1,"submittedBy":{"_id":"66ce751a8ec9fda2cf5a9e85","avatarUrl":"/avatars/c17093ca81dad007b3e50bae503955a7.svg","fullname":"Haocheng Xi","name":"xihc-ucb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"66b1baeff10262fc4fa61961","name":"UCBerkeley","fullname":"University of California, Berkeley","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.02402","authors":[{"_id":"698188face18b1862809636b","name":"Mu Huang","hidden":false},{"_id":"698188face18b1862809636c","name":"Hui Wang","hidden":false},{"_id":"698188face18b1862809636d","name":"Kerui Ren","hidden":false},{"_id":"698188face18b1862809636e","name":"Linning Xu","hidden":false},{"_id":"698188face18b1862809636f","name":"Yunsong Zhou","hidden":false},{"_id":"698188face18b18628096370","name":"Mulin Yu","hidden":false},{"_id":"698188face18b18628096371","name":"Bo Dai","hidden":false},{"_id":"698188face18b18628096372","name":"Jiangmiao Pang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/656e9e26bbec423d88b603e8/NQPQQdvpSKid9FRT1hgUc.mp4"],"publishedAt":"2026-02-02T17:59:31.000Z","submittedOnDailyAt":"2026-02-05T00:30:24.170Z","title":"SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation","submittedOnDailyBy":{"_id":"656e9e26bbec423d88b603e8","avatarUrl":"/avatars/10d8cb945a60e0401bfa4f74137cb203.svg","isPro":false,"fullname":"MulinYu","user":"UML","type":"user"},"summary":"Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.","upvotes":29,"discussionId":"698188fbce18b18628096373","ai_summary":"SoMA is a 3D Gaussian Splat simulator that enables stable, long-horizon manipulation of soft bodies by coupling deformable dynamics, environmental forces, and robot actions in a unified latent neural space.","ai_keywords":["3D Gaussian Splat","deformable dynamics","latent neural space","real-to-sim simulation","robot manipulation","cloth folding"],"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "}},"publishedAt":"2026-02-02T12:59:31.000Z","title":"SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation","summary":"Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/656e9e26bbec423d88b603e8/NQPQQdvpSKid9FRT1hgUc.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02402.png","numComments":2,"submittedBy":{"_id":"656e9e26bbec423d88b603e8","avatarUrl":"/avatars/10d8cb945a60e0401bfa4f74137cb203.svg","fullname":"MulinYu","name":"UML","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "},"isAuthorParticipating":false},{"paper":{"id":"2602.02196","authors":[{"_id":"6984207fe34659da7e1f4e4c","name":"Hang Yan","hidden":false},{"_id":"6984207fe34659da7e1f4e4d","name":"Xinyu Che","hidden":false},{"_id":"6984207fe34659da7e1f4e4e","name":"Fangzhi Xu","hidden":false},{"_id":"6984207fe34659da7e1f4e4f","name":"Qiushi Sun","hidden":false},{"_id":"6984207fe34659da7e1f4e50","name":"Zichen Ding","hidden":false},{"_id":"6984207fe34659da7e1f4e51","name":"Kanzhi Cheng","hidden":false},{"_id":"6984207fe34659da7e1f4e52","user":{"_id":"658be7fe135580745c510323","avatarUrl":"/avatars/830e5cec4565efdc23226a86a0fcef0e.svg","isPro":false,"fullname":"Jian Zhang","user":"VentureZJ","type":"user"},"name":"Jian Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:53:43.024Z","hidden":false},{"_id":"6984207fe34659da7e1f4e53","name":"Tao Qin","hidden":false},{"_id":"6984207fe34659da7e1f4e54","name":"Jun Liu","hidden":false},{"_id":"6984207fe34659da7e1f4e55","name":"Qika Lin","hidden":false}],"publishedAt":"2026-02-02T15:00:47.000Z","submittedOnDailyAt":"2026-02-05T02:17:45.489Z","title":"TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents","submittedOnDailyBy":{"_id":"64ca227b667f4f80850e6bd1","avatarUrl":"/avatars/db083cbc9d1540cdaa8b270c7e3f0304.svg","isPro":false,"fullname":"Hang Yan","user":"beatccjiang","type":"user"},"summary":"Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.","upvotes":29,"discussionId":"69842080e34659da7e1f4e56","ai_summary":"Test-Time Improvement (TTI) in autonomous LLM agents involves iterative environmental interaction that enhances performance, but current evaluation methods inadequately capture task optimization efficiency and memory utilization.","ai_keywords":["Test-Time Improvement","autonomous LLM agents","iterative interaction","environmental interaction","task optimization efficiency","working memory","agent-agnostic framework","environment-agnostic framework"]},"publishedAt":"2026-02-02T10:00:47.000Z","title":"TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents","summary":"Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02196.png","numComments":1,"submittedBy":{"_id":"64ca227b667f4f80850e6bd1","avatarUrl":"/avatars/db083cbc9d1540cdaa8b270c7e3f0304.svg","fullname":"Hang Yan","name":"beatccjiang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.22954","authors":[{"_id":"698018846676f93322706587","user":{"_id":"67dc66fe55c24fc4f981a4ab","avatarUrl":"/avatars/7bd900ade802d99db7c562ad6c2f6661.svg","isPro":false,"fullname":"Yuezhou Hu","user":"yuezhouhu","type":"user"},"name":"Yuezhou Hu","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:53:04.233Z","hidden":false},{"_id":"698018846676f93322706588","name":"Harman Singh","hidden":false},{"_id":"698018846676f93322706589","name":"Monishwaran Maheswaran","hidden":false},{"_id":"698018846676f9332270658a","name":"Haocheng Xi","hidden":false},{"_id":"698018846676f9332270658b","name":"Coleman Hooper","hidden":false},{"_id":"698018846676f9332270658c","user":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","isPro":false,"fullname":"Jintao Zhang","user":"jt-zhang","type":"user"},"name":"Jintao Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:57:09.830Z","hidden":false},{"_id":"698018846676f9332270658d","user":{"_id":"65fe49de871b36bf84c0ba05","avatarUrl":"/avatars/0fe082518fb9ea40e23414c83ee5043e.svg","isPro":false,"fullname":"Aditya Tomar","user":"adityastomar","type":"user"},"name":"Aditya Tomar","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:57:07.939Z","hidden":false},{"_id":"698018846676f9332270658e","name":"Michael W. Mahoney","hidden":false},{"_id":"698018846676f9332270658f","name":"Sewon Min","hidden":false},{"_id":"698018846676f93322706590","name":"Mehrdad Farajtabar","hidden":false},{"_id":"698018846676f93322706591","name":"Kurt Keutzer","hidden":false},{"_id":"698018846676f93322706592","name":"Amir Gholami","hidden":false},{"_id":"698018846676f93322706593","name":"Chenfeng Xu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67dc66fe55c24fc4f981a4ab/FVQu1kNOzJkrevK4FdpOP.mp4"],"publishedAt":"2026-01-30T13:16:32.000Z","submittedOnDailyAt":"2026-02-05T00:48:00.818Z","title":"Residual Context Diffusion Language Models","submittedOnDailyBy":{"_id":"67dc66fe55c24fc4f981a4ab","avatarUrl":"/avatars/7bd900ade802d99db7c562ad6c2f6661.svg","isPro":false,"fullname":"Yuezhou Hu","user":"yuezhouhu","type":"user"},"summary":"Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a \"remasking\" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.","upvotes":28,"discussionId":"698018856676f93322706594","projectPage":"https://yuezhouhu.github.io/projects/residual-context-diffusion/index.html","githubRepo":"https://github.com/yuezhouhu/residual-context-diffusion","githubRepoAddedBy":"user","ai_summary":"Residual Context Diffusion (RCD) enhances diffusion large language models by recycling discarded token information through contextual residuals, improving accuracy with minimal computational overhead.","ai_keywords":["diffusion large language models","autoregressive language models","remasking mechanism","token representations","contextual residuals","denoising steps","decoupled two-stage training","backpropagation","long CoT reasoning","short CoT instruction following","AIME tasks"],"githubStars":44,"organization":{"_id":"66b1baeff10262fc4fa61961","name":"UCBerkeley","fullname":"University of California, Berkeley","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}},"publishedAt":"2026-01-30T08:16:32.000Z","title":"Residual Context Diffusion Language Models","summary":"Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a \"remasking\" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67dc66fe55c24fc4f981a4ab/FVQu1kNOzJkrevK4FdpOP.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22954.png","numComments":1,"submittedBy":{"_id":"67dc66fe55c24fc4f981a4ab","avatarUrl":"/avatars/7bd900ade802d99db7c562ad6c2f6661.svg","fullname":"Yuezhou Hu","name":"yuezhouhu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"66b1baeff10262fc4fa61961","name":"UCBerkeley","fullname":"University of California, Berkeley","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.04879","authors":[{"_id":"69840d8ae34659da7e1f4dda","user":{"_id":"63885f1d0bebb233d8ad6e5b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg","isPro":false,"fullname":"Penghui Qi","user":"QPHutu","type":"user"},"name":"Penghui Qi","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:11.972Z","hidden":false},{"_id":"69840d8ae34659da7e1f4ddb","user":{"_id":"66129c7b50350afe76757262","avatarUrl":"/avatars/a2f4fac076b9d658a0d904ed54960f6f.svg","isPro":false,"fullname":"Xiangxin Zhou","user":"zhouxiangxin","type":"user"},"name":"Xiangxin Zhou","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:14.108Z","hidden":false},{"_id":"69840d8ae34659da7e1f4ddc","name":"Zichen Liu","hidden":false},{"_id":"69840d8ae34659da7e1f4ddd","name":"Tianyu Pang","hidden":false},{"_id":"69840d8ae34659da7e1f4dde","name":"Chao Du","hidden":false},{"_id":"69840d8ae34659da7e1f4ddf","name":"Min Lin","hidden":false},{"_id":"69840d8ae34659da7e1f4de0","name":"Wee Sun Lee","hidden":false}],"publishedAt":"2026-02-04T18:59:04.000Z","submittedOnDailyAt":"2026-02-05T01:03:28.783Z","title":"Rethinking the Trust Region in LLM Reinforcement Learning","submittedOnDailyBy":{"_id":"63885f1d0bebb233d8ad6e5b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg","isPro":false,"fullname":"Penghui Qi","user":"QPHutu","type":"user"},"summary":"Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.","upvotes":25,"discussionId":"69840d8ae34659da7e1f4de1","ai_summary":"DPPO addresses limitations in PPO for LLM fine-tuning by replacing ratio clipping with direct policy divergence constraints, improving training stability and efficiency.","ai_keywords":["Proximal Policy Optimization","policy divergence","policy updates","token probability ratios","Monte Carlo estimates","reinforcement learning","Large Language Models","Total Variation","KL divergence","binary approximation","Top-K approximation"],"organization":{"_id":"61f4e841c771e23a1abb61ff","name":"sail","fullname":"Sea AI Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"}},"publishedAt":"2026-02-04T13:59:04.000Z","title":"Rethinking the Trust Region in LLM Reinforcement Learning","summary":"Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04879.png","numComments":1,"submittedBy":{"_id":"63885f1d0bebb233d8ad6e5b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg","fullname":"Penghui Qi","name":"QPHutu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"61f4e841c771e23a1abb61ff","name":"sail","fullname":"Sea AI Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643440185801-5df833bdda6d0311fd3d5403.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.03510","authors":[{"_id":"69843228e34659da7e1f5060","name":"Bozhou Li","hidden":false},{"_id":"69843228e34659da7e1f5061","user":{"_id":"66ac46766c3f950f4f10b9f9","avatarUrl":"/avatars/027b573bc6e5b18107e762645cec6069.svg","isPro":false,"fullname":"Yushuo Guan","user":"UnnamedWatcher","type":"user"},"name":"Yushuo Guan","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:52:24.319Z","hidden":false},{"_id":"69843228e34659da7e1f5062","user":{"_id":"64858cbb6121946cf1e3d41b","avatarUrl":"/avatars/cbe80473069512ba74e9713f8fa0942b.svg","isPro":false,"fullname":"lihaolin","user":"tdlhl","type":"user"},"name":"Haolin Li","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:52:21.747Z","hidden":false},{"_id":"69843228e34659da7e1f5063","name":"Bohan Zeng","hidden":false},{"_id":"69843228e34659da7e1f5064","name":"Yiyan Ji","hidden":false},{"_id":"69843228e34659da7e1f5065","name":"Yue Ding","hidden":false},{"_id":"69843228e34659da7e1f5066","name":"Pengfei Wan","hidden":false},{"_id":"69843228e34659da7e1f5067","name":"Kun Gai","hidden":false},{"_id":"69843228e34659da7e1f5068","name":"Yuanxing Zhang","hidden":false},{"_id":"69843228e34659da7e1f5069","name":"Wentao Zhang","hidden":false}],"publishedAt":"2026-02-03T13:30:13.000Z","submittedOnDailyAt":"2026-02-05T03:45:54.690Z","title":"Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers","submittedOnDailyBy":{"_id":"661e62c6bac5d981f886f77b","avatarUrl":"/avatars/f1eb51ed4499ca434c8939573dfbd5e2.svg","isPro":false,"fullname":"Bozhou Li","user":"zooblastlbz","type":"user"},"summary":"Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.","upvotes":23,"discussionId":"69843228e34659da7e1f506a","ai_summary":"Text conditioning in DiT-based models is enhanced through a unified normalized convex fusion framework that optimizes multi-layer LLM hidden states via depth-wise semantic routing, improving text-image alignment and compositional generation.","ai_keywords":["DiT-based text-to-image models","LLMs","text encoders","diffusion models","normalized convex fusion","lightweight gates","multi-layer LLM hidden states","time-wise fusion","depth-wise fusion","joint fusion","classifier-free guidance","effective SNR","semantic hierarchy","generative capability","text-image alignment","compositional generation","Depth-wise Semantic Routing","train-inference trajectory mismatch"],"organization":{"_id":"662c559b322afcbae51b3c8b","name":"KlingTeam","fullname":"Kling Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}},"publishedAt":"2026-02-03T08:30:13.000Z","title":"Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers","summary":"Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03510.png","numComments":1,"submittedBy":{"_id":"661e62c6bac5d981f886f77b","avatarUrl":"/avatars/f1eb51ed4499ca434c8939573dfbd5e2.svg","fullname":"Bozhou Li","name":"zooblastlbz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"662c559b322afcbae51b3c8b","name":"KlingTeam","fullname":"Kling Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.03907","authors":[{"_id":"69840d41e34659da7e1f4dbf","name":"Team Hunyuan3D","hidden":false},{"_id":"69840d41e34659da7e1f4dc1","name":"Bowen Zhang","hidden":false},{"_id":"69840d41e34659da7e1f4dc2","name":"Chunchao Guo","hidden":false},{"_id":"69840d41e34659da7e1f4dc3","name":"Dongyuan Guo","hidden":false},{"_id":"69840d41e34659da7e1f4dc4","name":"Haolin Liu","hidden":false},{"_id":"69840d41e34659da7e1f4dc5","name":"Hongyu Yan","hidden":false},{"_id":"69840d41e34659da7e1f4dc6","name":"Huiwen Shi","hidden":false},{"_id":"69840d41e34659da7e1f4dc7","name":"Jiaao Yu","hidden":false},{"_id":"69840d41e34659da7e1f4dc8","name":"Jiachen Xu","hidden":false},{"_id":"69840d41e34659da7e1f4dc9","name":"Jingwei Huang","hidden":false},{"_id":"69840d41e34659da7e1f4dca","name":"Kunhong Li","hidden":false},{"_id":"69840d41e34659da7e1f4dcb","name":"Lifu Wang","hidden":false},{"_id":"69840d41e34659da7e1f4dcc","name":"Linus","hidden":false},{"_id":"69840d41e34659da7e1f4dcd","name":"Penghao Wang","hidden":false},{"_id":"69840d41e34659da7e1f4dce","name":"Qingxiang Lin","hidden":false},{"_id":"69840d41e34659da7e1f4dcf","name":"Ruining Tang","hidden":false},{"_id":"69840d41e34659da7e1f4dd0","user":{"_id":"647d9e881a1fcad2fdbf4954","avatarUrl":"/avatars/92ee8727d5c9063d852d3537b7690843.svg","isPro":false,"fullname":"SeanYoung","user":"SeanYoungxh","type":"user"},"name":"Xianghui Yang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:16.517Z","hidden":false},{"_id":"69840d41e34659da7e1f4dd1","name":"Yang Li","hidden":false},{"_id":"69840d41e34659da7e1f4dd2","name":"Yirui Guan","hidden":false},{"_id":"69840d41e34659da7e1f4dd3","name":"Yunfei Zhao","hidden":false},{"_id":"69840d41e34659da7e1f4dd4","name":"Yunhan Yang","hidden":false},{"_id":"69840d41e34659da7e1f4dd5","name":"Zeqiang Lai","hidden":false},{"_id":"69840d41e34659da7e1f4dd6","name":"Zhihao Liang","hidden":false},{"_id":"69840d41e34659da7e1f4dd7","name":"Zibo Zhao","hidden":false}],"publishedAt":"2026-02-03T14:13:09.000Z","submittedOnDailyAt":"2026-02-05T02:07:42.618Z","title":"HY3D-Bench: Generation of 3D Assets","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.","upvotes":22,"discussionId":"69840d41e34659da7e1f4dd8","projectPage":"https://3d.hunyuan.tencent.com/login?redirect_url=https%3A%2F%2F3d.hunyuan.tencent.com%2F","ai_summary":"HY3D-Bench presents an open-source ecosystem for 3D content creation that provides high-fidelity 3D objects and synthetic assets to advance 3D generation capabilities.","ai_keywords":["neural representations","generative models","3D content creation","3D generation","3D objects","watertight meshes","multi-view renderings","part-level decomposition","AIGC synthesis pipeline","3D perception","robotics","digital content creation"],"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}},"publishedAt":"2026-02-03T09:13:09.000Z","title":"HY3D-Bench: Generation of 3D Assets","summary":"While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03907.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03828","authors":[{"_id":"69838eece34659da7e1f4c3e","name":"Minjun Zhu","hidden":false},{"_id":"69838eece34659da7e1f4c3f","name":"Zhen Lin","hidden":false},{"_id":"69838eece34659da7e1f4c40","name":"Yixuan Weng","hidden":false},{"_id":"69838eece34659da7e1f4c41","name":"Panzhong Lu","hidden":false},{"_id":"69838eece34659da7e1f4c42","name":"Qiujie Xie","hidden":false},{"_id":"69838eece34659da7e1f4c43","name":"Yifan Wei","hidden":false},{"_id":"69838eece34659da7e1f4c44","name":"Sifan Liu","hidden":false},{"_id":"69838eece34659da7e1f4c45","name":"Qiyao Sun","hidden":false},{"_id":"69838eece34659da7e1f4c46","name":"Yue Zhang","hidden":false}],"publishedAt":"2026-02-03T18:41:43.000Z","submittedOnDailyAt":"2026-02-05T09:06:07.045Z","title":"AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations","submittedOnDailyBy":{"_id":"611568222999876a45605af5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1628792849997-noauth.jpeg","isPro":false,"fullname":"WENGSYX","user":"WENGSYX","type":"user"},"summary":"High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.","upvotes":19,"discussionId":"69838eede34659da7e1f4c47","ai_summary":"FigureBench presents the first large-scale benchmark for generating scientific illustrations from long-form scientific texts, while AutoFigure introduces an agentic framework that produces publication-ready illustrations through extensive thinking, recombination, and validation processes.","ai_keywords":["scientific illustrations","long-form scientific text","FigureBench","AutoFigure","agentic framework","text-to-illustration tasks","scientific papers","surveys","textbooks","structural completeness","aesthetic appeal","publication-ready"],"organization":{"_id":"66bb231e40d36c70d6ad0c4b","name":"WestlakeNLP","fullname":"Text Intelligence Lab of Westlake University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/622ee9f3165ba2c1bcbc7706/KpIm3isRczYp7kSnfNGSL.png"}},"publishedAt":"2026-02-03T13:41:43.000Z","title":"AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations","summary":"High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03828.png","numComments":1,"submittedBy":{"_id":"611568222999876a45605af5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1628792849997-noauth.jpeg","fullname":"WENGSYX","name":"WENGSYX","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"66bb231e40d36c70d6ad0c4b","name":"WestlakeNLP","fullname":"Text Intelligence Lab of Westlake University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/622ee9f3165ba2c1bcbc7706/KpIm3isRczYp7kSnfNGSL.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03143","authors":[{"_id":"698403b9e34659da7e1f4d42","name":"Baohao Liao","hidden":false},{"_id":"698403b9e34659da7e1f4d43","name":"Hanze Dong","hidden":false},{"_id":"698403b9e34659da7e1f4d44","name":"Xinxing Xu","hidden":false},{"_id":"698403b9e34659da7e1f4d45","name":"Christof Monz","hidden":false},{"_id":"698403b9e34659da7e1f4d46","name":"Jiang Bian","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/62c414354ce7250560a1f67f/aTiNZFW3UyQ6ssvN_mosJ.png"],"publishedAt":"2026-02-03T05:56:20.000Z","submittedOnDailyAt":"2026-02-05T00:20:20.229Z","title":"Self-Hinting Language Models Enhance Reinforcement Learning","submittedOnDailyBy":{"_id":"62c414354ce7250560a1f67f","avatarUrl":"/avatars/28fd73973d1703c84f4f59644fef8a80.svg","isPro":false,"fullname":"Baohao Liao","user":"baohao","type":"user"},"summary":"Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt x, the model samples a compact hint h (e.g., a plan or decomposition) and then generates a solution  conditioned on (x,h). Crucially, the task reward R(x,) is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set h=varnothing and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.","upvotes":19,"discussionId":"698403b9e34659da7e1f4d47","githubRepo":"https://github.com/BaohaoLiao/SAGE","githubRepoAddedBy":"user","ai_summary":"SAGE is an on-policy reinforcement learning framework that enhances GRPO by injecting self-hints during training to increase outcome diversity under sparse rewards, improving alignment of large language models.","ai_keywords":["Group Relative Policy Optimization","reinforcement learning","privileged supervision","self-hint","rollout distribution","terminal rewards","sparse rewards","on-policy learning","large language models","policy optimization"],"githubStars":9,"organization":{"_id":"68151d0f51add3813f3f7d1b","name":"MicrosoftResearch","fullname":"Microsoft Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}},"publishedAt":"2026-02-03T00:56:20.000Z","title":"Self-Hinting Language Models Enhance Reinforcement Learning","summary":"Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt x, the model samples a compact hint h (e.g., a plan or decomposition) and then generates a solution  conditioned on (x,h). Crucially, the task reward R(x,) is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set h=varnothing and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/62c414354ce7250560a1f67f/aTiNZFW3UyQ6ssvN_mosJ.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03143.png","numComments":1,"submittedBy":{"_id":"62c414354ce7250560a1f67f","avatarUrl":"/avatars/28fd73973d1703c84f4f59644fef8a80.svg","fullname":"Baohao Liao","name":"baohao","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"68151d0f51add3813f3f7d1b","name":"MicrosoftResearch","fullname":"Microsoft Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03587","authors":[{"_id":"6982e28a9084cb4f0ecb5847","name":"Shihan Dou","hidden":false},{"_id":"6982e28a9084cb4f0ecb5848","user":{"_id":"65b71c0582d38451342f7334","avatarUrl":"/avatars/f9763a0ac361c350e6c6732e23564567.svg","isPro":false,"fullname":"Ming Zhang","user":"konglongge","type":"user"},"name":"Ming Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T19:03:53.545Z","hidden":false},{"_id":"6982e28a9084cb4f0ecb5849","name":"Zhangyue Yin","hidden":false},{"_id":"6982e28a9084cb4f0ecb584a","name":"Chenhao Huang","hidden":false},{"_id":"6982e28a9084cb4f0ecb584b","name":"Yujiong Shen","hidden":false},{"_id":"6982e28a9084cb4f0ecb584c","name":"Junzhe Wang","hidden":false},{"_id":"6982e28a9084cb4f0ecb584d","name":"Jiayi Chen","hidden":false},{"_id":"6982e28a9084cb4f0ecb584e","name":"Yuchen Ni","hidden":false},{"_id":"6982e28a9084cb4f0ecb584f","name":"Junjie Ye","hidden":false},{"_id":"6982e28a9084cb4f0ecb5850","name":"Cheng Zhang","hidden":false},{"_id":"6982e28a9084cb4f0ecb5851","name":"Huaibing Xie","hidden":false},{"_id":"6982e28a9084cb4f0ecb5852","name":"Jianglu Hu","hidden":false},{"_id":"6982e28a9084cb4f0ecb5853","name":"Shaolei Wang","hidden":false},{"_id":"6982e28a9084cb4f0ecb5854","name":"Weichao Wang","hidden":false},{"_id":"6982e28a9084cb4f0ecb5855","name":"Yanling Xiao","hidden":false},{"_id":"6982e28a9084cb4f0ecb5856","name":"Yiting Liu","hidden":false},{"_id":"6982e28a9084cb4f0ecb5857","name":"Zenan Xu","hidden":false},{"_id":"6982e28a9084cb4f0ecb5858","name":"Zhen Guo","hidden":false},{"_id":"6982e28a9084cb4f0ecb5859","name":"Pluto Zhou","hidden":false},{"_id":"6982e28a9084cb4f0ecb585a","name":"Tao Gui","hidden":false},{"_id":"6982e28a9084cb4f0ecb585b","name":"Zuxuan Wu","hidden":false},{"_id":"6982e28a9084cb4f0ecb585c","name":"Xipeng Qiu","hidden":false},{"_id":"6982e28a9084cb4f0ecb585d","name":"Qi Zhang","hidden":false},{"_id":"6982e28a9084cb4f0ecb585e","name":"Xuanjing Huang","hidden":false},{"_id":"6982e28a9084cb4f0ecb585f","name":"Yu-Gang Jiang","hidden":false},{"_id":"6982e28a9084cb4f0ecb5860","name":"Di Wang","hidden":false},{"_id":"6982e28a9084cb4f0ecb5861","name":"Shunyu Yao","hidden":false}],"publishedAt":"2026-02-03T14:37:47.000Z","submittedOnDailyAt":"2026-02-05T08:07:21.691Z","title":"CL-bench: A Benchmark for Context Learning","submittedOnDailyBy":{"_id":"64ae0b6ccf90fe27556cf56e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uVXJd3TBssT_Ug4yAl3c9.jpeg","isPro":false,"fullname":"Shihan Dou","user":"Ablustrund","type":"user"},"summary":"Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.","upvotes":18,"discussionId":"6982e28a9084cb4f0ecb5862","projectPage":"https://www.clbench.com","githubRepo":"https://github.com/Tencent-Hunyuan/CL-bench","githubRepoAddedBy":"user","ai_summary":"Language models struggle with context learning, requiring new knowledge and reasoning beyond pre-training, as demonstrated by a comprehensive benchmark revealing poor performance on real-world tasks.","ai_keywords":["language models","context learning","pre-trained knowledge","real-world tasks","in-context learning","long-context tasks","retrieval","reading comprehension","task patterns","frontier LMs","GPT-5.1"],"githubStars":312,"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-03T09:37:47.000Z","title":"CL-bench: A Benchmark for Context Learning","summary":"Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03587.png","numComments":1,"submittedBy":{"_id":"64ae0b6ccf90fe27556cf56e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uVXJd3TBssT_Ug4yAl3c9.jpeg","fullname":"Shihan Dou","name":"Ablustrund","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.04575","authors":[{"_id":"69840aeee34659da7e1f4db4","name":"Jiaheng Liu","hidden":false},{"_id":"69840aeee34659da7e1f4db5","name":"Yuanxing Zhang","hidden":false},{"_id":"69840aeee34659da7e1f4db6","name":"Shihao Li","hidden":false},{"_id":"69840aeee34659da7e1f4db7","name":"Xinping Lei","hidden":false}],"publishedAt":"2026-02-04T14:01:44.000Z","submittedOnDailyAt":"2026-02-05T00:44:32.628Z","title":"Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration","submittedOnDailyBy":{"_id":"65377c30e48353201e6fdda0","avatarUrl":"/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg","isPro":false,"fullname":"Jiaheng Liu","user":"CheeryLJH","type":"user"},"summary":"For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the Vibe AIGC, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.\n  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.","upvotes":17,"discussionId":"69840aefe34659da7e1f4db8","ai_summary":"Vibe AIGC introduces a new generative AI paradigm where users provide high-level aesthetic and functional preferences, which are then orchestrated through multi-agent workflows to bridge the gap between human intent and machine execution.","ai_keywords":["Vibe Coding","Vibe AIGC","agentic orchestration","Intent-Execution Gap","multi-agent workflows","Meta-Planner","stochastic inference","logical orchestration"],"organization":{"_id":"68edc767abe005ac1b354573","name":"NJU-LINK","fullname":"NJU-LINK Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"}},"publishedAt":"2026-02-04T09:01:44.000Z","title":"Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration","summary":"For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the Vibe AIGC, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.\n  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04575.png","numComments":1,"submittedBy":{"_id":"65377c30e48353201e6fdda0","avatarUrl":"/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg","fullname":"Jiaheng Liu","name":"CheeryLJH","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":25,"isUserFollowing":false},"organization":{"_id":"68edc767abe005ac1b354573","name":"NJU-LINK","fullname":"NJU-LINK Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03973","authors":[{"_id":"69842b7ee34659da7e1f5047","user":{"_id":"6687a6261c9173b89f325323","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6687a6261c9173b89f325323/7GpTfSD30KPbg6gXIWLbv.jpeg","isPro":true,"fullname":"Shuo Liu","user":"TreeePlanter","type":"user"},"name":"Shuo Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:52:26.764Z","hidden":false},{"_id":"69842b7ee34659da7e1f5048","name":"Ishneet Sukhvinder Singh","hidden":false},{"_id":"69842b7ee34659da7e1f5049","name":"Yiqing Xu","hidden":false},{"_id":"69842b7ee34659da7e1f504a","name":"Jiafei Duan","hidden":false},{"_id":"69842b7ee34659da7e1f504b","name":"Ranjay Krishna","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/VKyD81egcdrmYQEoV2Afw.mp4"],"publishedAt":"2026-02-03T19:50:16.000Z","submittedOnDailyAt":"2026-02-05T03:04:25.871Z","title":"VLS: Steering Pretrained Robot Policies via Vision-Language Models","submittedOnDailyBy":{"_id":"632b42626110e37dba3d5bcb","avatarUrl":"/avatars/ca70a15def71ee84f4f149db5e954843.svg","isPro":false,"fullname":"Duan","user":"Jiafei1224","type":"user"},"summary":"Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/","upvotes":17,"discussionId":"69842b7ee34659da7e1f504c","projectPage":"https://vision-language-steering.github.io/webpage/","githubRepo":"https://github.com/Vision-Language-Steering/code","githubRepoAddedBy":"user","ai_summary":"Pretrained diffusion and flow-matching policies fail under test-time shifts due to tight coupling with training configurations, prompting the development of Vision-Language Steering (VLS) for training-free inference-time adaptation through vision-language model-guided trajectory steering.","ai_keywords":["diffusion policies","flow-matching policies","imitation learning","train-test shifts","action generation","fine-tuning","generative robot policies","vision-language models","trajectory-differentiable reward functions","denoising","inference-time adaptation"],"githubStars":9,"organization":{"_id":"5e70f3648ce3c604d78fe132","name":"allenai","fullname":"Ai2","avatar":"https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}},"publishedAt":"2026-02-03T14:50:16.000Z","title":"VLS: Steering Pretrained Robot Policies via Vision-Language Models","summary":"Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/VKyD81egcdrmYQEoV2Afw.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03973.png","numComments":1,"submittedBy":{"_id":"632b42626110e37dba3d5bcb","avatarUrl":"/avatars/ca70a15def71ee84f4f149db5e954843.svg","fullname":"Duan","name":"Jiafei1224","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"5e70f3648ce3c604d78fe132","name":"allenai","fullname":"Ai2","avatar":"https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03442","authors":[{"_id":"698405d5e34659da7e1f4d5c","name":"Mingxuan Du","hidden":false},{"_id":"698405d5e34659da7e1f4d5d","name":"Benfeng Xu","hidden":false},{"_id":"698405d5e34659da7e1f4d5e","name":"Chiwei Zhu","hidden":false},{"_id":"698405d5e34659da7e1f4d5f","user":{"_id":"6440d49b7663594a126716f2","avatarUrl":"/avatars/bb04af9d1ae9c5bc058ebfbf08f4ebc8.svg","isPro":false,"fullname":"shaohanwang","user":"WShao","type":"user"},"name":"Shaohan Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:39.504Z","hidden":false},{"_id":"698405d5e34659da7e1f4d60","name":"Pengyu Wang","hidden":false},{"_id":"698405d5e34659da7e1f4d61","name":"Xiaorui Wang","hidden":false},{"_id":"698405d5e34659da7e1f4d62","name":"Zhendong Mao","hidden":false}],"publishedAt":"2026-02-03T12:07:21.000Z","submittedOnDailyAt":"2026-02-05T00:29:43.463Z","title":"A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces","submittedOnDailyBy":{"_id":"646dbba74ad7f907279dd486","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/646dbba74ad7f907279dd486/XGHJMFEIpWeDlh0cn9Slu.png","isPro":false,"fullname":"Mingxuan Du","user":"Ayanami0730","type":"user"},"summary":"Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.","upvotes":17,"discussionId":"698405d5e34659da7e1f4d63","projectPage":"https://agentresearchlab.org/agents/a-rag","githubRepo":"https://github.com/Ayanami0730/arag","githubRepoAddedBy":"user","ai_summary":"Agentic RAG framework enables models to dynamically adapt retrieval decisions across multiple granularities, outperforming traditional approaches while scaling efficiently with model improvements.","ai_keywords":["RAG systems","retrieval-augmented generation","agentic frameworks","hierarchical retrieval interfaces","keyword search","semantic search","chunk read","multi-granularity retrieval","model scaling","test-time compute"],"githubStars":39,"organization":{"_id":"6912993f45f02a20f4c50b8a","name":"muset-ai","fullname":"muset.ai","avatar":"https://cdn-uploads.huggingface.co/production/uploads/646dbba74ad7f907279dd486/C5j0HEiqEmmxshM61WRON.png"}},"publishedAt":"2026-02-03T07:07:21.000Z","title":"A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces","summary":"Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03442.png","numComments":1,"submittedBy":{"_id":"646dbba74ad7f907279dd486","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/646dbba74ad7f907279dd486/XGHJMFEIpWeDlh0cn9Slu.png","fullname":"Mingxuan Du","name":"Ayanami0730","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":10,"isUserFollowing":false},"organization":{"_id":"6912993f45f02a20f4c50b8a","name":"muset-ai","fullname":"muset.ai","avatar":"https://cdn-uploads.huggingface.co/production/uploads/646dbba74ad7f907279dd486/C5j0HEiqEmmxshM61WRON.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.18207","authors":[{"_id":"6979279adf44b75fa47e46bc","user":{"_id":"650871aeb44445e9b3625c7b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png","isPro":false,"fullname":"James Burgess","user":"jmhb","type":"user"},"name":"James Burgess","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:34:09.480Z","hidden":false},{"_id":"6979279adf44b75fa47e46bd","name":"Jan N. Hansen","hidden":false},{"_id":"6979279adf44b75fa47e46be","user":{"_id":"6682599523b532718e4bd56e","avatarUrl":"/avatars/89eeaa25b10edffc33a53ed1f07a0ed2.svg","isPro":false,"fullname":"Duo Peng","user":"pengxunduo","type":"user"},"name":"Duo Peng","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:57:17.400Z","hidden":false},{"_id":"6979279adf44b75fa47e46bf","user":{"_id":"62da55164398e21bf7f0e292","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62da55164398e21bf7f0e292/xjKkG8IA2IZZqCdjApSh3.jpeg","isPro":false,"fullname":"Yuhui Zhang","user":"yuhuizhang","type":"user"},"name":"Yuhui Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:34:07.233Z","hidden":false},{"_id":"6979279adf44b75fa47e46c0","name":"Alejandro Lozano","hidden":false},{"_id":"6979279adf44b75fa47e46c1","name":"Min Woo Sun","hidden":false},{"_id":"6979279adf44b75fa47e46c2","name":"Emma Lundberg","hidden":false},{"_id":"6979279adf44b75fa47e46c3","name":"Serena Yeung-Levy","hidden":false}],"publishedAt":"2026-01-26T06:46:16.000Z","submittedOnDailyAt":"2026-02-05T00:45:19.168Z","title":"PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR","submittedOnDailyBy":{"_id":"650871aeb44445e9b3625c7b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png","isPro":false,"fullname":"James Burgess","user":"jmhb","type":"user"},"summary":"Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.","upvotes":16,"discussionId":"6979279adf44b75fa47e46c4","projectPage":"https://jmhb0.github.io/PaperSearchQA/","githubRepo":"https://github.com/jmhb0/PaperSearchQA","githubRepoAddedBy":"user","ai_summary":"Search agents trained on scientific paper corpora demonstrate advanced reasoning capabilities for technical question-answering tasks, outperforming traditional retrieval methods through reinforcement learning with verifiable rewards.","ai_keywords":["language models","reinforcement learning with verifiable rewards","search agents","knowledge bases","scientific papers","biomedical paper abstracts","factoid QA","PaperSearchQA","RLVR","Search-R1"],"githubStars":21},"publishedAt":"2026-01-26T01:46:16.000Z","title":"PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR","summary":"Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18207.png","numComments":2,"submittedBy":{"_id":"650871aeb44445e9b3625c7b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png","fullname":"James Burgess","name":"jmhb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.04816","authors":[{"_id":"69841540e34659da7e1f4e20","name":"Zhengqing Yuan","hidden":false},{"_id":"69841540e34659da7e1f4e21","name":"Lichao Sun","hidden":false},{"_id":"69841540e34659da7e1f4e22","name":"Yanfang","hidden":false},{"_id":"69841540e34659da7e1f4e23","name":"Ye","hidden":false}],"publishedAt":"2026-02-04T18:04:46.000Z","submittedOnDailyAt":"2026-02-05T01:45:19.676Z","title":"Horizon-LM: A RAM-Centric Architecture for LLM Training","submittedOnDailyBy":{"_id":"64574d8e182c64e989846ba2","avatarUrl":"/avatars/db4bc496a745e1d7de48215b30f6fd3e.svg","isPro":false,"fullname":"Tyrannosaurus","user":"Tyrannosaurus","type":"user"},"summary":"The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2times higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.","upvotes":14,"discussionId":"69841540e34659da7e1f4e24","githubRepo":"https://github.com/DLYuanGod/Horizon-LM","githubRepoAddedBy":"user","ai_summary":"Horizon-LM enables large-model training on single GPUs by redefining CPU-GPU roles and eliminating persistent GPU memory usage through explicit recomputation and pipelined execution.","ai_keywords":["large language models","distributed parallelism","offloading","GPU-centric execution","autograd graphs","CPU-master","GPU-template","explicit recomputation","manual gradient propagation","pipelined double-buffered execution","DeepSpeed ZeRO-3"],"githubStars":6,"organization":{"_id":"6356ef35fe4ffe942db2460b","name":"notredame","fullname":"University of Notre Dame","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/RJJ94XCJw7R0WkOyrvXIU.png"}},"publishedAt":"2026-02-04T13:04:46.000Z","title":"Horizon-LM: A RAM-Centric Architecture for LLM Training","summary":"The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2times higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04816.png","numComments":1,"submittedBy":{"_id":"64574d8e182c64e989846ba2","avatarUrl":"/avatars/db4bc496a745e1d7de48215b30f6fd3e.svg","fullname":"Tyrannosaurus","name":"Tyrannosaurus","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"organization":{"_id":"6356ef35fe4ffe942db2460b","name":"notredame","fullname":"University of Notre Dame","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/RJJ94XCJw7R0WkOyrvXIU.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.04735","authors":[{"_id":"6984133de34659da7e1f4e0d","name":"Mengru Wang","hidden":false},{"_id":"6984133de34659da7e1f4e0e","name":"Zhenqian Xu","hidden":false},{"_id":"6984133de34659da7e1f4e0f","name":"Junfeng Fang","hidden":false},{"_id":"6984133de34659da7e1f4e10","name":"Yunzhi Yao","hidden":false},{"_id":"6984133de34659da7e1f4e11","name":"Shumin Deng","hidden":false},{"_id":"6984133de34659da7e1f4e12","name":"Huajun Chen","hidden":false},{"_id":"6984133de34659da7e1f4e13","name":"Ningyu Zhang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/wt8-tC1fOhvt6ocXghZRE.png"],"publishedAt":"2026-02-04T16:37:17.000Z","submittedOnDailyAt":"2026-02-05T02:36:11.038Z","title":"From Data to Behavior: Predicting Unintended Model Behaviors Before Training","submittedOnDailyBy":{"_id":"620b3bbb0668e435407c8d0a","avatarUrl":"/avatars/e0fccbb2577d76088e09f054c35cffbc.svg","isPro":true,"fullname":"Ningyu Zhang","user":"Ningyu","type":"user"},"summary":"Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.","upvotes":13,"discussionId":"6984133de34659da7e1f4e14","githubRepo":"https://github.com/zjunlp/Data2Behavior","githubRepoAddedBy":"user","ai_summary":"Data2Behavior predicts unintended model behaviors before training using MDF, a lightweight method that analyzes data features to reveal potential biases without parameter updates.","ai_keywords":["Large Language Models","unintended biases","Data2Behavior","Manipulating Data Features","mean representations","forward pass","latent statistical signals","model activations","pre-training vulnerabilities"],"githubStars":2,"organization":{"_id":"6345aadf5efccdc07f1365a5","name":"ZhejiangUniversity","fullname":"Zhejiang University"}},"publishedAt":"2026-02-04T11:37:17.000Z","title":"From Data to Behavior: Predicting Unintended Model Behaviors Before Training","summary":"Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/wt8-tC1fOhvt6ocXghZRE.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04735.png","numComments":1,"submittedBy":{"_id":"620b3bbb0668e435407c8d0a","avatarUrl":"/avatars/e0fccbb2577d76088e09f054c35cffbc.svg","fullname":"Ningyu Zhang","name":"Ningyu","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":39,"isUserFollowing":false},"organization":{"_id":"6345aadf5efccdc07f1365a5","name":"ZhejiangUniversity","fullname":"Zhejiang University"},"isAuthorParticipating":false},{"paper":{"id":"2601.22859","authors":[{"_id":"69815cd4ce18b18628095f55","user":{"_id":"654a3703ebec0c55a5a2adec","avatarUrl":"/avatars/07e6d5de5a6c5eb7ab738bc568f25675.svg","isPro":false,"fullname":"chuanzheguo","user":"czguo","type":"user"},"name":"Chuanzhe Guo","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:57:02.035Z","hidden":false},{"_id":"69815cd4ce18b18628095f56","name":"Jingjing Wu","hidden":false},{"_id":"69815cd4ce18b18628095f57","user":{"_id":"62769a608483d8e9ecd9b4f8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg","isPro":false,"fullname":"Sijun He","user":"sijunhe","type":"user"},"name":"Sijun He","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:57:03.932Z","hidden":false},{"_id":"69815cd4ce18b18628095f58","name":"Yang Chen","hidden":false},{"_id":"69815cd4ce18b18628095f59","name":"Zhaoqi Kuang","hidden":false},{"_id":"69815cd4ce18b18628095f5a","name":"Shilong Fan","hidden":false},{"_id":"69815cd4ce18b18628095f5b","name":"Bingjin Chen","hidden":false},{"_id":"69815cd4ce18b18628095f5c","name":"Siqi Bao","hidden":false},{"_id":"69815cd4ce18b18628095f5d","name":"Jing Liu","hidden":false},{"_id":"69815cd4ce18b18628095f5e","name":"Hua Wu","hidden":false},{"_id":"69815cd4ce18b18628095f5f","name":"Qingfu Zhu","hidden":false},{"_id":"69815cd4ce18b18628095f60","name":"Wanxiang Che","hidden":false},{"_id":"69815cd4ce18b18628095f61","name":"Haifeng Wang","hidden":false}],"publishedAt":"2026-01-30T11:36:10.000Z","submittedOnDailyAt":"2026-02-05T03:45:18.841Z","title":"MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering","submittedOnDailyBy":{"_id":"62769a608483d8e9ecd9b4f8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg","isPro":false,"fullname":"Sijun He","user":"sijunhe","type":"user"},"summary":"The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.","upvotes":13,"discussionId":"69815cd5ce18b18628095f62","githubRepo":"https://github.com/ernie-research/MEnvAgent","githubRepoAddedBy":"user","ai_summary":"MEnvAgent is a multi-language framework that automates environment construction for software engineering tasks using a planning-execution-verification architecture and environment reuse mechanism, achieving improved performance on a new benchmark and creating the largest open-source polyglot dataset of verifiable Docker environments.","ai_keywords":["Large Language Model agents","software engineering","verifiable datasets","multi-agent Planning-Execution-Verification architecture","Environment Reuse Mechanism","Docker environments","MEnvBench","MEnvData-SWE"],"githubStars":8,"organization":{"_id":"682dc063d57ba1e4d149e134","name":"ernie-research","fullname":"ernie-research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/626a6ce8adf559c88de9ace0/FFebJQj-LjMu6XkztMbH2.webp"}},"publishedAt":"2026-01-30T06:36:10.000Z","title":"MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering","summary":"The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22859.png","numComments":1,"submittedBy":{"_id":"62769a608483d8e9ecd9b4f8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg","fullname":"Sijun He","name":"sijunhe","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"682dc063d57ba1e4d149e134","name":"ernie-research","fullname":"ernie-research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/626a6ce8adf559c88de9ace0/FFebJQj-LjMu6XkztMbH2.webp"},"isAuthorParticipating":true},{"paper":{"id":"2602.04284","authors":[{"_id":"6984154ee34659da7e1f4e26","name":"Yansong Ning","hidden":false},{"_id":"6984154ee34659da7e1f4e27","name":"Jun Fang","hidden":false},{"_id":"6984154ee34659da7e1f4e28","name":"Naiqiang Tan","hidden":false},{"_id":"6984154ee34659da7e1f4e29","name":"Hao Liu","hidden":false}],"publishedAt":"2026-02-04T07:26:23.000Z","submittedOnDailyAt":"2026-02-05T01:30:24.837Z","title":"Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning","submittedOnDailyBy":{"_id":"65d6a6f7654f85ff0baf161f","avatarUrl":"/avatars/4a46f8b0522fa572c122249a9d6526c4.svg","isPro":false,"fullname":"Yansong NING","user":"yasNing","type":"user"},"summary":"Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.","upvotes":12,"discussionId":"6984154ee34659da7e1f4e2a","projectPage":"https://github.com/usail-hkust/Agent-Omit","githubRepo":"https://github.com/usail-hkust/Agent-Omit","githubRepoAddedBy":"user","ai_summary":"Agent-Omit is a training framework that enables LLM agents to adaptively omit redundant thoughts and observations during multi-turn interactions, achieving superior effectiveness-efficiency trade-offs compared to existing methods.","ai_keywords":["LLM agents","multi-turn interactions","thought necessity","observation utility","cold-start data","fine-tune","agentic reinforcement learning","dual sampling mechanism","omission reward","KL-divergence"],"githubStars":8,"organization":{"_id":"630da9a4c677278c1804028e","name":"Didichuxing","fullname":"Didi Chuxing","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1661839713821-630d471d467bc15dec893907.png"}},"publishedAt":"2026-02-04T02:26:23.000Z","title":"Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning","summary":"Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04284.png","numComments":2,"submittedBy":{"_id":"65d6a6f7654f85ff0baf161f","avatarUrl":"/avatars/4a46f8b0522fa572c122249a9d6526c4.svg","fullname":"Yansong NING","name":"yasNing","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"630da9a4c677278c1804028e","name":"Didichuxing","fullname":"Didi Chuxing","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1661839713821-630d471d467bc15dec893907.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.02160","authors":[{"_id":"698172e2ce18b18628096033","user":{"_id":"6418142857c3a491a4d64f1f","avatarUrl":"/avatars/85db855cdc65ef9e3fd20d15e50ef1cc.svg","isPro":false,"fullname":"bowen xu","user":"bowiehsu","type":"user"},"name":"Bowen Xu","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:56:55.418Z","hidden":false},{"_id":"698172e2ce18b18628096034","name":"Shaoyu Wu","hidden":false},{"_id":"698172e2ce18b18628096035","name":"Hao Jiang","hidden":false},{"_id":"698172e2ce18b18628096036","name":"Kai Liu","hidden":false},{"_id":"698172e2ce18b18628096037","name":"Xin Chen","hidden":false},{"_id":"698172e2ce18b18628096038","name":"Lulu Hu","hidden":false},{"_id":"698172e2ce18b18628096039","name":"Bin Yang","hidden":false}],"publishedAt":"2026-02-02T14:36:15.000Z","submittedOnDailyAt":"2026-02-05T10:02:16.651Z","title":"D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use","submittedOnDailyBy":{"_id":"6418142857c3a491a4d64f1f","avatarUrl":"/avatars/85db855cdc65ef9e3fd20d15e50ef1cc.svg","isPro":false,"fullname":"bowen xu","user":"bowiehsu","type":"user"},"summary":"Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\\textbf{D}ecomposing tasks and \\textbf{Co}mposing \\textbf{Re}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\\% accuracy, surpassing the best-performing 8B model by 5.7\\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\\%, outperforming 70B models despite being 5times smaller. The source code is available at https://github.com/alibaba/EfficientAI.","upvotes":11,"discussionId":"698172e2ce18b1862809603a","githubRepo":"https://github.com/alibaba/EfficientAI","githubRepoAddedBy":"user","ai_summary":"A two-stage training framework called D-CORE is proposed to improve large reasoning models' ability to decompose complex tasks and compose reasoning processes, achieving superior performance in tool-use benchmarks.","ai_keywords":["large reasoning models","sub-task decomposition","Lazy Reasoning","self-distillation","diversity-aware reinforcement learning","BFCLv3"],"githubStars":7,"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}},"publishedAt":"2026-02-02T09:36:15.000Z","title":"D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use","summary":"Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\\textbf{D}ecomposing tasks and \\textbf{Co}mposing \\textbf{Re}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\\% accuracy, surpassing the best-performing 8B model by 5.7\\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\\%, outperforming 70B models despite being 5times smaller. The source code is available at https://github.com/alibaba/EfficientAI.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02160.png","numComments":5,"submittedBy":{"_id":"6418142857c3a491a4d64f1f","avatarUrl":"/avatars/85db855cdc65ef9e3fd20d15e50ef1cc.svg","fullname":"bowen xu","name":"bowiehsu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.03916","authors":[{"_id":"69845ad8e34659da7e1f5136","name":"Azmine Toushik Wasi","hidden":false},{"_id":"69845ad8e34659da7e1f5137","name":"Wahid Faisal","hidden":false},{"_id":"69845ad8e34659da7e1f5138","name":"Abdur Rahman","hidden":false},{"_id":"69845ad8e34659da7e1f5139","name":"Mahfuz Ahmed Anik","hidden":false},{"_id":"69845ad8e34659da7e1f513a","name":"Munem Shahriar","hidden":false},{"_id":"69845ad8e34659da7e1f513b","name":"Mohsin Mahmud Topu","hidden":false},{"_id":"69845ad8e34659da7e1f513c","name":"Sadia Tasnim Meem","hidden":false},{"_id":"69845ad8e34659da7e1f513d","name":"Rahatun Nesa Priti","hidden":false},{"_id":"69845ad8e34659da7e1f513e","name":"Sabrina Afroz Mitu","hidden":false},{"_id":"69845ad8e34659da7e1f513f","name":"Md. Iqramul Hoque","hidden":false},{"_id":"69845ad8e34659da7e1f5140","name":"Shahriyar Zaman Ridoy","hidden":false},{"_id":"69845ad8e34659da7e1f5141","name":"Mohammed Eunus Ali","hidden":false},{"_id":"69845ad8e34659da7e1f5142","name":"Majd Hawasly","hidden":false},{"_id":"69845ad8e34659da7e1f5143","name":"Mohammad Raza","hidden":false},{"_id":"69845ad8e34659da7e1f5144","name":"Md Rizwan Parvez","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/627608ec624e6576d693c05c/_EIWFs2gqsP3zDQ-sq3Iq.png"],"publishedAt":"2026-02-03T17:52:02.000Z","submittedOnDailyAt":"2026-02-05T06:29:27.585Z","title":"SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?","submittedOnDailyBy":{"_id":"627608ec624e6576d693c05c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1677077880239-627608ec624e6576d693c05c.png","isPro":false,"fullname":"Azmine Toushik Wasi","user":"azminetoushikwasi","type":"user"},"summary":"Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.","upvotes":9,"discussionId":"69845ad8e34659da7e1f5145","projectPage":"https://spatialab-reasoning.github.io/","githubRepo":"https://github.com/SpatiaLab-Reasoning/SpatiaLab","githubRepoAddedBy":"user","ai_summary":"SpatiaLab presents a comprehensive benchmark for evaluating vision-language models' spatial reasoning capabilities across realistic, diverse scenarios, revealing significant gaps compared to human performance.","ai_keywords":["vision-language models","spatial reasoning","benchmark","visual question-answer pairs","real-world complexity","spatial relationships","depth perception","spatial navigation","3D geometry"],"githubStars":2,"organization":{"_id":"666592f9d507e6a951662a33","name":"ciol-research","fullname":"Computational Intelligence and Operations Laboratory (CIOL)","avatar":"https://cdn-uploads.huggingface.co/production/uploads/627608ec624e6576d693c05c/8Ug9HEozxycN2QOZFmypY.png"}},"publishedAt":"2026-02-03T12:52:02.000Z","title":"SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?","summary":"Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/627608ec624e6576d693c05c/_EIWFs2gqsP3zDQ-sq3Iq.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03916.png","numComments":1,"submittedBy":{"_id":"627608ec624e6576d693c05c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1677077880239-627608ec624e6576d693c05c.png","fullname":"Azmine Toushik Wasi","name":"azminetoushikwasi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"666592f9d507e6a951662a33","name":"ciol-research","fullname":"Computational Intelligence and Operations Laboratory (CIOL)","avatar":"https://cdn-uploads.huggingface.co/production/uploads/627608ec624e6576d693c05c/8Ug9HEozxycN2QOZFmypY.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.02140","authors":[{"_id":"69842c8fe34659da7e1f504e","name":"Chenlong Wang","hidden":false},{"_id":"69842c8fe34659da7e1f504f","name":"Yuhang Chen","hidden":false},{"_id":"69842c8fe34659da7e1f5050","name":"Zhihan Hu","hidden":false},{"_id":"69842c8fe34659da7e1f5051","name":"Dongping Chen","hidden":false},{"_id":"69842c8fe34659da7e1f5052","name":"Wenhu Chen","hidden":false},{"_id":"69842c8fe34659da7e1f5053","name":"Sarah Wiegreffe","hidden":false},{"_id":"69842c8fe34659da7e1f5054","name":"Tianyi Zhou","hidden":false}],"publishedAt":"2026-02-02T14:19:37.000Z","submittedOnDailyAt":"2026-02-05T03:08:49.499Z","title":"Quantifying the Gap between Understanding and Generation within Unified Multimodal Models","submittedOnDailyBy":{"_id":"643be8879f5d314db2d9ed23","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png","isPro":false,"fullname":"Chen Dongping","user":"shuaishuaicdp","type":"user"},"summary":"Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two \"unified\" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.","upvotes":9,"discussionId":"69842c90e34659da7e1f5055","ai_summary":"Unified multimodal models exhibit a persistent gap between understanding and generation capabilities, indicating only surface-level integration rather than deep cognitive convergence.","ai_keywords":["unified multimodal models","bidirectional benchmark","cognitive coherence","bidirectional inference capability","cross-modal consistency","knowledge manipulation","capability emergence"],"organization":{"_id":"647f5b7daa8c04bbf938c625","name":"umd-zhou-lab","fullname":"Tianyi Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"}},"publishedAt":"2026-02-02T09:19:37.000Z","title":"Quantifying the Gap between Understanding and Generation within Unified Multimodal Models","summary":"Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two \"unified\" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02140.png","numComments":1,"submittedBy":{"_id":"643be8879f5d314db2d9ed23","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png","fullname":"Chen Dongping","name":"shuaishuaicdp","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":14,"isUserFollowing":false},"organization":{"_id":"647f5b7daa8c04bbf938c625","name":"umd-zhou-lab","fullname":"Tianyi Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.02554","authors":[{"_id":"6982e1679084cb4f0ecb5834","name":"Jingwen Xu","hidden":false},{"_id":"6982e1679084cb4f0ecb5835","name":"Yiyang Lu","hidden":false},{"_id":"6982e1679084cb4f0ecb5836","name":"Zisu Huang","hidden":false},{"_id":"6982e1679084cb4f0ecb5837","user":{"_id":"60efa4da8432bc401cd0abc6","avatarUrl":"/avatars/3c8d8db9aaa5bd5fd1f870ac0a6b655a.svg","isPro":false,"fullname":"Changze Lv","user":"fdu-lcz","type":"user"},"name":"Changze Lv","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:55:18.660Z","hidden":false},{"_id":"6982e1679084cb4f0ecb5838","name":"Xiaohua Wang","hidden":false},{"_id":"6982e1679084cb4f0ecb5839","name":"Shizheng Li","hidden":false},{"_id":"6982e1679084cb4f0ecb583a","name":"Zhibo Xu","hidden":false},{"_id":"6982e1679084cb4f0ecb583b","name":"Zhengkang Guo","hidden":false},{"_id":"6982e1679084cb4f0ecb583c","name":"Zhengyuan Wang","hidden":false},{"_id":"6982e1679084cb4f0ecb583d","name":"Muzhao Tian","hidden":false},{"_id":"6982e1679084cb4f0ecb583e","name":"Xuanjing Huang","hidden":false},{"_id":"6982e1679084cb4f0ecb583f","name":"Xiaoqing Zheng","hidden":false}],"publishedAt":"2026-01-30T11:32:15.000Z","submittedOnDailyAt":"2026-02-05T01:01:15.746Z","title":"BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation","submittedOnDailyBy":{"_id":"60efa4da8432bc401cd0abc6","avatarUrl":"/avatars/3c8d8db9aaa5bd5fd1f870ac0a6b655a.svg","isPro":false,"fullname":"Changze Lv","user":"fdu-lcz","type":"user"},"summary":"Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.","upvotes":8,"discussionId":"6982e1679084cb4f0ecb5840","ai_summary":"BatCoder is a self-supervised reinforcement learning framework that jointly optimizes code and documentation generation through back-translation, achieving superior performance on code-related benchmarks.","ai_keywords":["self-supervised reinforcement learning","back-translation","code generation","documentation production","semantic similarity","implicit reward","reinforcement learning","pass@1","model capacity"]},"publishedAt":"2026-01-30T06:32:15.000Z","title":"BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation","summary":"Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02554.png","numComments":1,"submittedBy":{"_id":"60efa4da8432bc401cd0abc6","avatarUrl":"/avatars/3c8d8db9aaa5bd5fd1f870ac0a6b655a.svg","fullname":"Changze Lv","name":"fdu-lcz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.03979","authors":[{"_id":"69842525e34659da7e1f5027","name":"Ariel Kwiatkowski","hidden":false},{"_id":"69842525e34659da7e1f5028","name":"Natasha Butt","hidden":false},{"_id":"69842525e34659da7e1f5029","name":"Ismail Labiad","hidden":false},{"_id":"69842525e34659da7e1f502a","name":"Julia Kempe","hidden":false},{"_id":"69842525e34659da7e1f502b","name":"Yann Ollivier","hidden":false}],"publishedAt":"2026-02-03T20:04:21.000Z","submittedOnDailyAt":"2026-02-05T02:35:51.445Z","title":"Likelihood-Based Reward Designs for General LLM Reasoning","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function, often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards. Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining. In verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and yield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short, verifiable and long, non-verifiable answer settings.","upvotes":7,"discussionId":"69842525e34659da7e1f502c","ai_summary":"Log-probability rewards derived from the reference answer's likelihood outperform binary rewards in chain-of-thought fine-tuning across both verifiable and non-verifiable reasoning benchmarks.","ai_keywords":["large language models","reinforcement learning","reward function","binary rewards","likelihood-based rewards","log-probability","chain-of-thought","mathematical reasoning benchmarks","verifiable settings","non-verifiable settings","pretraining","next-token log-likelihood loss","fine-tuning"],"organization":{"_id":"66b54027408752ae16404b05","name":"metaresearch","fullname":"Meta Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"}},"publishedAt":"2026-02-03T15:04:21.000Z","title":"Likelihood-Based Reward Designs for General LLM Reasoning","summary":"Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function, often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards. Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining. In verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and yield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short, verifiable and long, non-verifiable answer settings.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03979.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"organization":{"_id":"66b54027408752ae16404b05","name":"metaresearch","fullname":"Meta Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.01640","authors":[{"_id":"69840412e34659da7e1f4d49","name":"Shuai Zhang","hidden":false},{"_id":"69840412e34659da7e1f4d4a","name":"Jiayu Hu","hidden":false},{"_id":"69840412e34659da7e1f4d4b","name":"Zijie Chen","hidden":false},{"_id":"69840412e34659da7e1f4d4c","name":"Zeyuan Ding","hidden":false},{"_id":"69840412e34659da7e1f4d4d","name":"Yi Zhang","hidden":false},{"_id":"69840412e34659da7e1f4d4e","name":"Yingji Zhang","hidden":false},{"_id":"69840412e34659da7e1f4d4f","name":"Ziyi Zhou","hidden":false},{"_id":"69840412e34659da7e1f4d50","name":"Junwei Liao","hidden":false},{"_id":"69840412e34659da7e1f4d51","name":"Shengjie Zhou","hidden":false},{"_id":"69840412e34659da7e1f4d52","name":"Yong Dai","hidden":false},{"_id":"69840412e34659da7e1f4d53","name":"Zhenzhong Lan","hidden":false},{"_id":"69840412e34659da7e1f4d54","name":"Xiaozhu Ju","hidden":false}],"publishedAt":"2026-02-02T04:55:27.000Z","submittedOnDailyAt":"2026-02-05T00:20:28.744Z","title":"A2Eval: Agentic and Automated Evaluation for Embodied Brain","submittedOnDailyBy":{"_id":"630a2bcf4bee441367afba10","avatarUrl":"/avatars/463a62b296887f1be8290475b91af098.svg","isPro":false,"fullname":"Zijie Chen","user":"Zijie-chen","type":"user"},"summary":"Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.","upvotes":7,"discussionId":"69840413e34659da7e1f4d55","ai_summary":"Agentic automatic evaluation framework automates embodied vision-language model assessment through collaborative agents that reduce evaluation costs and improve ranking accuracy.","ai_keywords":["embodied VLM","benchmark curation","evaluation suite","computational costs","ranking bias","human alignment","Spearman's rho","Kendall's tau"],"organization":{"_id":"690af874ebec2e7e2f30eb2c","name":"X-Humanoid","fullname":"X-Humanoid","avatar":"https://cdn-uploads.huggingface.co/production/uploads/690af62c1c460ec9687cbc3f/KIC79Joju07S7TJLasrtJ.png"}},"publishedAt":"2026-02-01T23:55:27.000Z","title":"A2Eval: Agentic and Automated Evaluation for Embodied Brain","summary":"Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01640.png","numComments":1,"submittedBy":{"_id":"630a2bcf4bee441367afba10","avatarUrl":"/avatars/463a62b296887f1be8290475b91af098.svg","fullname":"Zijie Chen","name":"Zijie-chen","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"690af874ebec2e7e2f30eb2c","name":"X-Humanoid","fullname":"X-Humanoid","avatar":"https://cdn-uploads.huggingface.co/production/uploads/690af62c1c460ec9687cbc3f/KIC79Joju07S7TJLasrtJ.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.04486","authors":[{"_id":"698410ebe34659da7e1f4de9","name":"Jinlong Ma","hidden":false},{"_id":"698410ebe34659da7e1f4dea","name":"Yu Zhang","hidden":false},{"_id":"698410ebe34659da7e1f4deb","name":"Xuefeng Bai","hidden":false},{"_id":"698410ebe34659da7e1f4dec","name":"Kehai Chen","hidden":false},{"_id":"698410ebe34659da7e1f4ded","name":"Yuwei Wang","hidden":false},{"_id":"698410ebe34659da7e1f4dee","name":"Zeming Liu","hidden":false},{"_id":"698410ebe34659da7e1f4def","name":"Jun Yu","hidden":false},{"_id":"698410ebe34659da7e1f4df0","name":"Min Zhang","hidden":false}],"publishedAt":"2026-02-04T12:12:49.000Z","submittedOnDailyAt":"2026-02-05T01:10:09.368Z","title":"Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition","submittedOnDailyBy":{"_id":"65fe9599d74aed6c3e6d93a2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65fe9599d74aed6c3e6d93a2/WxPDKPJ6-p0_xu0dzOYXw.jpeg","isPro":false,"fullname":"Yu Zhang","user":"271754echo","type":"user"},"summary":"Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit modality bias, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning (MCR), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines.","upvotes":6,"discussionId":"698410ebe34659da7e1f4df1","ai_summary":"MLLMs suffer from modality bias in GMNER tasks, which is addressed through a proposed method that enforces cross-modal reasoning via multi-style reasoning schema injection and constraint-guided verifiable optimization.","ai_keywords":["Multimodal Large Language Models","GMNER","modality bias","cross-modal reasoning","Multi-style Reasoning Schema Injection","Constraint-guided Verifiable Optimization","Group Relative Policy Optimization"]},"publishedAt":"2026-02-04T07:12:49.000Z","title":"Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition","summary":"Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit modality bias, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning (MCR), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04486.png","numComments":1,"submittedBy":{"_id":"65fe9599d74aed6c3e6d93a2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65fe9599d74aed6c3e6d93a2/WxPDKPJ6-p0_xu0dzOYXw.jpeg","fullname":"Yu Zhang","name":"271754echo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.20499","authors":[{"_id":"69844621e34659da7e1f50d5","user":{"_id":"67d6bb22eab66ce9cb4e3662","avatarUrl":"/avatars/814704eef9e6907d9d4ab407b605566b.svg","isPro":false,"fullname":"Hang Guo","user":"HangGuo","type":"user"},"name":"Hang Guo","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:52:07.030Z","hidden":false},{"_id":"69844621e34659da7e1f50d6","name":"Zhaoyang Jia","hidden":false},{"_id":"69844621e34659da7e1f50d7","name":"Jiahao Li","hidden":false},{"_id":"69844621e34659da7e1f50d8","name":"Bin Li","hidden":false},{"_id":"69844621e34659da7e1f50d9","name":"Yuanhao Cai","hidden":false},{"_id":"69844621e34659da7e1f50da","name":"Jiangshan Wang","hidden":false},{"_id":"69844621e34659da7e1f50db","name":"Yawei Li","hidden":false},{"_id":"69844621e34659da7e1f50dc","name":"Yan Lu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67d6bb22eab66ce9cb4e3662/Rv8QZ2iY4ipyUaTkQytH8.mp4"],"publishedAt":"2026-01-28T11:20:43.000Z","submittedOnDailyAt":"2026-02-05T05:03:59.115Z","title":"Efficient Autoregressive Video Diffusion with Dummy Head","submittedOnDailyBy":{"_id":"67d6bb22eab66ce9cb4e3662","avatarUrl":"/avatars/814704eef9e6907d9d4ab407b605566b.svg","isPro":false,"fullname":"Hang Guo","user":"HangGuo","type":"user"},"summary":"The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.","upvotes":5,"discussionId":"69844621e34659da7e1f50dd","projectPage":"https://csguoh.github.io/project/DummyForcing/","githubRepo":"https://github.com/csguoh/DummyForcing","githubRepoAddedBy":"user","ai_summary":"Autoregressive video diffusion models suffer from inefficient attention mechanisms that underutilize historical frames, but a new method called Dummy Forcing improves efficiency through heterogeneous memory allocation and dynamic head programming while maintaining quality.","ai_keywords":["autoregressive video diffusion model","multi-head self-attention","causal modeling","iterative denoising","KV caches","heterogeneous memory allocation","dynamic head programming","context packing","cache compression"],"githubStars":32,"organization":{"_id":"68151d0f51add3813f3f7d1b","name":"MicrosoftResearch","fullname":"Microsoft Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}},"publishedAt":"2026-01-28T06:20:43.000Z","title":"Efficient Autoregressive Video Diffusion with Dummy Head","summary":"The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67d6bb22eab66ce9cb4e3662/Rv8QZ2iY4ipyUaTkQytH8.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20499.png","numComments":1,"submittedBy":{"_id":"67d6bb22eab66ce9cb4e3662","avatarUrl":"/avatars/814704eef9e6907d9d4ab407b605566b.svg","fullname":"Hang Guo","name":"HangGuo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"68151d0f51add3813f3f7d1b","name":"MicrosoftResearch","fullname":"Microsoft Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.04442","authors":[{"_id":"6984771e5de9acb9debfb1e6","user":{"_id":"67139b18ac58fedd4128bf5a","avatarUrl":"/avatars/22e41569f9aa444d6f9c0ca4b91e2da9.svg","isPro":false,"fullname":"  ","user":"dimakarp1996","type":"user"},"name":"Dmitry Karpov","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:55:46.816Z","hidden":false}],"publishedAt":"2026-02-04T11:14:29.000Z","submittedOnDailyAt":"2026-02-05T09:46:16.458Z","title":"No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data","submittedOnDailyBy":{"_id":"67139b18ac58fedd4128bf5a","avatarUrl":"/avatars/22e41569f9aa444d6f9c0ca4b91e2da9.svg","isPro":false,"fullname":"  ","user":"dimakarp1996","type":"user"},"summary":"We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.","upvotes":4,"discussionId":"6984771f5de9acb9debfb1e7","ai_summary":"Machine translation experiments for Turkic languages using nllb-200, LoRA fine-tuning, and prompt-based approaches achieved varying chrF++ scores across language pairs.","ai_keywords":["nllb-200","LoRA","synthetic data","chrF++","DeepSeek-V3.2","retrieval-based approaches","zero-shot"]},"publishedAt":"2026-02-04T06:14:29.000Z","title":"No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data","summary":"We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04442.png","numComments":1,"submittedBy":{"_id":"67139b18ac58fedd4128bf5a","avatarUrl":"/avatars/22e41569f9aa444d6f9c0ca4b91e2da9.svg","fullname":"  ","name":"dimakarp1996","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.02350","authors":[{"_id":"69832a259084cb4f0ecb597b","user":{"_id":"68ad8ff044c8254ac4b6ad6e","avatarUrl":"/avatars/a566b764e100edf1e2d633623e32f5cf.svg","isPro":false,"fullname":"Xingyuan Hua","user":"hansenhua","type":"user"},"name":"Xingyuan Hua","status":"claimed_verified","statusLastChangedAt":"2026-02-04T19:03:49.511Z","hidden":false},{"_id":"69832a259084cb4f0ecb597c","name":"Sheng Yue","hidden":false},{"_id":"69832a259084cb4f0ecb597d","name":"Xinyi Li","hidden":false},{"_id":"69832a259084cb4f0ecb597e","name":"Yizhe Zhao","hidden":false},{"_id":"69832a259084cb4f0ecb597f","name":"Jinrui Zhang","hidden":false},{"_id":"69832a259084cb4f0ecb5980","name":"Ju Ren","hidden":false}],"publishedAt":"2026-02-02T17:15:17.000Z","submittedOnDailyAt":"2026-02-05T00:22:27.818Z","title":"Context Learning for Multi-Agent Discussion","submittedOnDailyBy":{"_id":"68ad8ff044c8254ac4b6ad6e","avatarUrl":"/avatars/a566b764e100edf1e2d633623e32f5cf.svg","isPro":false,"fullname":"Xingyuan Hua","user":"hansenhua","type":"user"},"summary":"Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.","upvotes":4,"discussionId":"69832a269084cb4f0ecb5981","githubRepo":"https://github.com/HansenHua/M2CL-ICLR26","githubRepoAddedBy":"user","ai_summary":"Multi-Agent Discussion methods suffer from inconsistency due to individual context misalignment, which is addressed through a context learning approach that dynamically generates context instructions for each agent to improve consensus reaching and performance.","ai_keywords":["multi-LLM context learning","context generator","discussion consistency","context coherence","self-adaptive mechanism","premature convergence","consensus reaching"],"githubStars":3,"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"}},"publishedAt":"2026-02-02T12:15:17.000Z","title":"Context Learning for Multi-Agent Discussion","summary":"Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02350.png","numComments":1,"submittedBy":{"_id":"68ad8ff044c8254ac4b6ad6e","avatarUrl":"/avatars/a566b764e100edf1e2d633623e32f5cf.svg","fullname":"Xingyuan Hua","name":"hansenhua","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"},"isAuthorParticipating":true},{"paper":{"id":"2602.04805","authors":[{"_id":"698423e0e34659da7e1f4e68","name":"Jia-peng Zhang","hidden":false},{"_id":"698423e0e34659da7e1f4e69","name":"Cheng-Feng Pu","hidden":false},{"_id":"698423e0e34659da7e1f4e6a","name":"Meng-Hao Guo","hidden":false},{"_id":"698423e0e34659da7e1f4e6b","name":"Yan-Pei Cao","hidden":false},{"_id":"698423e0e34659da7e1f4e6c","name":"Shi-Min Hu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hPeTK92WFsALPUYWcnxCW.mp4"],"publishedAt":"2026-02-04T17:52:17.000Z","submittedOnDailyAt":"2026-02-05T02:30:39.994Z","title":"Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.","upvotes":3,"discussionId":"698423e0e34659da7e1f4e6d","projectPage":"https://zjp-shadow.github.io/works/SkinTokens/","ai_summary":"Generative 3D models face challenges in animation rigging, which this work addresses by introducing SkinTokensa learned discrete representation for skinning weightsand TokenRig, a unified autoregressive framework that models skeletons and skin deformations together, improving rigging accuracy through reinforcement learning.","ai_keywords":["FSQ-CVAE","skinning weights","token sequence prediction","autoregressive framework","reinforcement learning","geometric rewards","semantic rewards"]},"publishedAt":"2026-02-04T12:52:17.000Z","title":"Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging","summary":"The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hPeTK92WFsALPUYWcnxCW.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04805.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.01849","authors":[{"_id":"6983802be34659da7e1f4c05","user":{"_id":"63dad66a0cc3bc12bc089598","avatarUrl":"/avatars/025d5ee31fdad61a6087ed9265bf76c2.svg","isPro":false,"fullname":"Ziwei Luo","user":"weblzw","type":"user"},"name":"Ziwei Luo","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:55:00.685Z","hidden":false},{"_id":"6983802be34659da7e1f4c06","name":"Ziqi Jin","hidden":false},{"_id":"6983802be34659da7e1f4c07","name":"Lei Wang","hidden":false},{"_id":"6983802be34659da7e1f4c08","name":"Lidong Bing","hidden":false},{"_id":"6983802be34659da7e1f4c09","name":"Thomas B. Schn","hidden":false}],"publishedAt":"2026-02-02T09:21:45.000Z","submittedOnDailyAt":"2026-02-05T05:53:25.698Z","title":"Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models","submittedOnDailyBy":{"_id":"63dad66a0cc3bc12bc089598","avatarUrl":"/avatars/025d5ee31fdad61a6087ed9265bf76c2.svg","isPro":false,"fullname":"Ziwei Luo","user":"weblzw","type":"user"},"summary":"This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.","upvotes":3,"discussionId":"6983802ce34659da7e1f4c0a","projectPage":"https://algolzw.github.io/sr-smc/","githubRepo":"https://github.com/Algolzw/self-rewarding-smc","githubRepoAddedBy":"user","ai_summary":"Self-rewarding sequential Monte Carlo enables effective sampling of masked diffusion language models by using parallel diffusion processes and trajectory-level confidence signals to improve generation quality.","ai_keywords":["sequential Monte Carlo","masked diffusion language models","confidence-based sampling","particle filtering","trajectory-level confidence","importance weighting","resampling","parallel inference","diffusion processes"],"githubStars":7},"publishedAt":"2026-02-02T04:21:45.000Z","title":"Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models","summary":"This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01849.png","numComments":1,"submittedBy":{"_id":"63dad66a0cc3bc12bc089598","avatarUrl":"/avatars/025d5ee31fdad61a6087ed9265bf76c2.svg","fullname":"Ziwei Luo","name":"weblzw","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.04883","authors":[{"_id":"69841dffe34659da7e1f4e35","name":"Yanru Qu","hidden":false},{"_id":"69841dffe34659da7e1f4e36","name":"Cheng-Yen Hsieh","hidden":false},{"_id":"69841dffe34659da7e1f4e37","name":"Zaixiang Zheng","hidden":false},{"_id":"69841dffe34659da7e1f4e38","name":"Ge Liu","hidden":false},{"_id":"69841dffe34659da7e1f4e39","name":"Quanquan Gu","hidden":false}],"publishedAt":"2026-02-04T18:59:49.000Z","submittedOnDailyAt":"2026-02-05T02:05:29.390Z","title":"Protein Autoregressive Modeling via Multiscale Structure Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.","upvotes":2,"discussionId":"69841dffe34659da7e1f4e3a","ai_summary":"PAR is a multi-scale autoregressive framework for protein backbone generation that uses hierarchical structure modeling, autoregressive transformers, and flow-based decoding to produce high-quality protein structures with improved generalization and reduced exposure bias.","ai_keywords":["autoregressive modeling","multi-scale autoregressive framework","protein backbone generation","hierarchical nature","autoregressive transformer","conditional embeddings","flow-based backbone decoder","exposure bias","noisy context learning","scheduled sampling","zero-shot generalization","motif scaffolding","unconditional generation benchmark"],"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}},"publishedAt":"2026-02-04T13:59:49.000Z","title":"Protein Autoregressive Modeling via Multiscale Structure Generation","summary":"We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04883.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.04651","authors":[{"_id":"69849f394ad556f294b7e932","name":"Dipan Maity","hidden":false}],"publishedAt":"2026-02-04T15:26:44.000Z","submittedOnDailyAt":"2026-02-05T11:18:50.724Z","title":"SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF","submittedOnDailyBy":{"_id":"63e364f1fae035bdc4c324d9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e364f1fae035bdc4c324d9/U4lmLltdPE42Q1wmAOU91.jpeg","isPro":false,"fullname":"Dipan Maity","user":"DipanAI","type":"user"},"summary":"Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE","upvotes":1,"discussionId":"69849f3a4ad556f294b7e933","ai_summary":"A new reinforcement learning algorithm for language model alignment that improves stability and performance over PPO through enhanced KL divergence control and adaptive reward management.","ai_keywords":["PPO","RLHF","actor-critic","Double Soft-Min Critic","entropy-gated KL regulation","PID-controlled adaptive thresholds","reward oscillations","entropy collapse","value function drift","policy divergence","training-average reward"]},"publishedAt":"2026-02-04T10:26:44.000Z","title":"SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF","summary":"Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04651.png","numComments":1,"submittedBy":{"_id":"63e364f1fae035bdc4c324d9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e364f1fae035bdc4c324d9/U4lmLltdPE42Q1wmAOU91.jpeg","fullname":"Dipan Maity","name":"DipanAI","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":10,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.04605","authors":[{"_id":"698406eae34659da7e1f4d70","user":{"_id":"6893dd21467f7d2f5f358a95","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6893dd21467f7d2f5f358a95/3buD-PC8cvzsS__NJjdUi.png","isPro":true,"fullname":"Rahul Bajaj","user":"thebajajra","type":"user"},"name":"Rahul Bajaj","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:54:37.262Z","hidden":false},{"_id":"698406eae34659da7e1f4d71","name":"Anuj Garg","hidden":false}],"publishedAt":"2026-02-04T14:32:37.000Z","submittedOnDailyAt":"2026-02-05T00:34:31.714Z","title":"RexBERT: Context Specialized Bidirectional Encoders for E-commerce","submittedOnDailyBy":{"_id":"6893dd21467f7d2f5f358a95","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6893dd21467f7d2f5f358a95/3buD-PC8cvzsS__NJjdUi.png","isPro":true,"fullname":"Rahul Bajaj","user":"thebajajra","type":"user"},"summary":"Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.","upvotes":1,"discussionId":"698406eae34659da7e1f4d72","ai_summary":"RexBERT, a family of BERT-style encoders designed for e-commerce semantics, achieves superior performance on domain-specific tasks through specialized pretraining and high-quality in-domain data.","ai_keywords":["encoder-only transformers","BERT-style encoders","e-commerce semantics","Ecom-niverse","ModernBERT","pretraining recipe","context extension","annealed domain specialization","token classification","semantic similarity","natural language understanding"],"organization":{"_id":"689ba1a36eb1ced69a174924","name":"owlgebra-ai","fullname":"Owlgebra AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6893dd21467f7d2f5f358a95/apTHmVGh6-bBORoro8zP1.png"}},"publishedAt":"2026-02-04T09:32:37.000Z","title":"RexBERT: Context Specialized Bidirectional Encoders for E-commerce","summary":"Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04605.png","numComments":1,"submittedBy":{"_id":"6893dd21467f7d2f5f358a95","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6893dd21467f7d2f5f358a95/3buD-PC8cvzsS__NJjdUi.png","fullname":"Rahul Bajaj","name":"thebajajra","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":24,"isUserFollowing":false},"organization":{"_id":"689ba1a36eb1ced69a174924","name":"owlgebra-ai","fullname":"Owlgebra AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6893dd21467f7d2f5f358a95/apTHmVGh6-bBORoro8zP1.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.04581","authors":[{"_id":"698531a54ad556f294b7ea78","name":"Debargha Ganguly","hidden":false},{"_id":"698531a54ad556f294b7ea79","name":"Sreehari Sankar","hidden":false},{"_id":"698531a54ad556f294b7ea7a","name":"Biyao Zhang","hidden":false},{"_id":"698531a54ad556f294b7ea7b","name":"Vikash Singh","hidden":false},{"_id":"698531a54ad556f294b7ea7c","name":"Kanan Gupta","hidden":false},{"_id":"698531a54ad556f294b7ea7d","name":"Harshini Kavuru","hidden":false},{"_id":"698531a54ad556f294b7ea7e","name":"Alan Luo","hidden":false},{"_id":"698531a54ad556f294b7ea7f","name":"Weicong Chen","hidden":false},{"_id":"698531a54ad556f294b7ea80","name":"Warren Morningstar","hidden":false},{"_id":"698531a54ad556f294b7ea81","name":"Raghu Machiraju","hidden":false},{"_id":"698531a54ad556f294b7ea82","name":"Vipin Chaudhary","hidden":false}],"publishedAt":"2026-02-04T14:06:46.000Z","submittedOnDailyAt":"2026-02-05T21:43:04.700Z","title":"Trust The Typical","submittedOnDailyBy":{"_id":"61deb0a302496c6d78da4ade","avatarUrl":"/avatars/1d31be74c0e6c983860d94846b4d3770.svg","isPro":false,"fullname":"Debargha Ganguly","user":"Debargha","type":"user"},"summary":"Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.","upvotes":1,"discussionId":"698531a54ad556f294b7ea83","ai_summary":"A novel framework for LLM safety that treats safety as an out-of-distribution detection problem, achieving state-of-the-art performance without harmful example training through semantic space analysis and efficient GPU implementation.","ai_keywords":["out-of-distribution detection","guardrails","semantic space","false positive rates","multilingual harms","over-refusal","vLLM","GPU optimization"]},"publishedAt":"2026-02-04T09:06:46.000Z","title":"Trust The Typical","summary":"Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04581.png","numComments":1,"submittedBy":{"_id":"61deb0a302496c6d78da4ade","avatarUrl":"/avatars/1d31be74c0e6c983860d94846b4d3770.svg","fullname":"Debargha Ganguly","name":"Debargha","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.04547","authors":[{"_id":"69844334e34659da7e1f50b4","name":"Luca Zedda","hidden":false},{"_id":"69844334e34659da7e1f50b5","name":"Andrea Loddo","hidden":false},{"_id":"69844334e34659da7e1f50b6","name":"Cecilia Di Ruberto","hidden":false}],"publishedAt":"2026-02-04T13:38:51.000Z","submittedOnDailyAt":"2026-02-05T04:54:41.041Z","title":"OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis","submittedOnDailyBy":{"_id":"67bed13027ea9754039d63c3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JHlZXuwcY0y_estgKPkPT.jpeg","isPro":false,"fullname":"Luca Zedda","user":"Snarcy","type":"user"},"summary":"Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.","upvotes":1,"discussionId":"69844334e34659da7e1f50b7","githubRepo":"https://github.com/unica-visual-intelligence-lab/OmniRad","githubRepoAddedBy":"user","ai_summary":"OmniRad is a self-supervised radiological foundation model pretrained on 1.2 million medical images that demonstrates improved performance in classification and segmentation tasks through representation reuse and cross-task transferability.","ai_keywords":["pretrained visual representations","self-supervised learning","radiological foundation model","downstream adaptation","task-specific adapters","end-to-end fine-tuning","representation quality","cross-task transferability","MedMNISTv2","MedSegBench","F1 score","Dice score","feature clustering","latent-space visualization"],"githubStars":2},"publishedAt":"2026-02-04T08:38:51.000Z","title":"OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis","summary":"Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04547.png","numComments":1,"submittedBy":{"_id":"67bed13027ea9754039d63c3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JHlZXuwcY0y_estgKPkPT.jpeg","fullname":"Luca Zedda","name":"Snarcy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.04289","authors":[{"_id":"69846d265de9acb9debfb1cd","name":"Lin Zheng","hidden":false},{"_id":"69846d265de9acb9debfb1ce","name":"Xinyu Li","hidden":false},{"_id":"69846d265de9acb9debfb1cf","name":"Qian Liu","hidden":false},{"_id":"69846d265de9acb9debfb1d0","name":"Xiachong Feng","hidden":false},{"_id":"69846d265de9acb9debfb1d1","name":"Lingpeng Kong","hidden":false}],"publishedAt":"2026-02-04T07:36:46.000Z","submittedOnDailyAt":"2026-02-05T07:46:16.356Z","title":"Proxy Compression for Language Modeling","submittedOnDailyBy":{"_id":"6461f7f4ddb3aaa43c8c079d","avatarUrl":"/avatars/0e21a335b61ae6043a7e580662452b8c.svg","isPro":false,"fullname":"Lin Zheng","user":"linzheng","type":"user"},"summary":"Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.","upvotes":1,"discussionId":"69846d265de9acb9debfb1d2","githubRepo":"https://github.com/LZhengisme/proxy-compression","githubRepoAddedBy":"user","ai_summary":"Proxy compression trains language models on both raw byte sequences and compressed views, enabling efficient training with end-to-end raw-byte inference while maintaining model robustness.","ai_keywords":["language models","token sequences","tokenizer","UTF-8 byte sequences","proxy compression","compressed inputs","raw byte sequences","external compressors","joint training","internal alignment","transfer learning","code language modeling","training efficiency","inference time","model scale","byte-level modeling"],"githubStars":1},"publishedAt":"2026-02-04T02:36:46.000Z","title":"Proxy Compression for Language Modeling","summary":"Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04289.png","numComments":1,"submittedBy":{"_id":"6461f7f4ddb3aaa43c8c079d","avatarUrl":"/avatars/0e21a335b61ae6043a7e580662452b8c.svg","fullname":"Lin Zheng","name":"linzheng","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.04271","authors":[{"_id":"698489995de9acb9debfb219","name":"Lifan Wu","hidden":false},{"_id":"698489995de9acb9debfb21a","name":"Ruijie Zhu","hidden":false},{"_id":"698489995de9acb9debfb21b","name":"Yubo Ai","hidden":false},{"_id":"698489995de9acb9debfb21c","name":"Tianzhu Zhang","hidden":false}],"publishedAt":"2026-02-04T07:00:44.000Z","submittedOnDailyAt":"2026-02-05T09:49:38.676Z","title":"SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization","submittedOnDailyBy":{"_id":"6697ac8427e4e21a3a92da27","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png","isPro":false,"fullname":"Ruijie Zhu","user":"RuijieZhu","type":"user"},"summary":"4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/","upvotes":1,"discussionId":"698489995de9acb9debfb21d","projectPage":"https://wusar.github.io/projects/skeletongaussian/","ai_summary":"SkeletonGaussian enables editable 4D generation by decomposing motion into rigid skeleton-driven and non-rigid fine-grained components using hexplane-based refinement.","ai_keywords":["4D generation","dynamic 3D objects","monocular video input","articulated representation","skeleton-driven motion","linear blend skinning","hexplane-based refinement","non-rigid deformations","motion editing"],"organization":{"_id":"61d8000084231b832e5bbd99","name":"ustc","fullname":"university of science and technology  of china","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"}},"publishedAt":"2026-02-04T02:00:44.000Z","title":"SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization","summary":"4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04271.png","numComments":1,"submittedBy":{"_id":"6697ac8427e4e21a3a92da27","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png","fullname":"Ruijie Zhu","name":"RuijieZhu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"61d8000084231b832e5bbd99","name":"ustc","fullname":"university of science and technology  of china","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03955","authors":[{"_id":"6985136a4ad556f294b7ea61","name":"Yinyi Luo","hidden":false},{"_id":"6985136a4ad556f294b7ea62","name":"Yiqiao Jin","hidden":false},{"_id":"6985136a4ad556f294b7ea63","name":"Weichen Yu","hidden":false},{"_id":"6985136a4ad556f294b7ea64","name":"Mengqi Zhang","hidden":false},{"_id":"6985136a4ad556f294b7ea65","name":"Srijan Kumar","hidden":false},{"_id":"6985136a4ad556f294b7ea66","name":"Xiaoxiao Li","hidden":false},{"_id":"6985136a4ad556f294b7ea67","name":"Weijie Xu","hidden":false},{"_id":"6985136a4ad556f294b7ea68","name":"Xin Chen","hidden":false},{"_id":"6985136a4ad556f294b7ea69","name":"Jindong Wang","hidden":false}],"publishedAt":"2026-02-03T19:18:28.000Z","submittedOnDailyAt":"2026-02-05T19:33:42.471Z","title":"AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent","submittedOnDailyBy":{"_id":"6204cc0d522e40b4a18d86e2","avatarUrl":"/avatars/18daf2de5671e711dc745388dd60569d.svg","isPro":false,"fullname":"Jindong Wang","user":"jindongwang","type":"user"},"summary":"While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.","upvotes":1,"discussionId":"6985136a4ad556f294b7ea6a","githubRepo":"https://github.com/AIFrontierLab/AgentArk","githubRepoAddedBy":"user","ai_summary":"AgentArk distills multi-agent reasoning dynamics into a single model through hierarchical distillation strategies, enabling efficient yet powerful reasoning capabilities.","ai_keywords":["multi-agent systems","large language models","distillation","reasoning-enhanced fine-tuning","trajectory-based augmentation","process-aware distillation","multi-agent dynamics","model distillation"],"githubStars":2,"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"}},"publishedAt":"2026-02-03T14:18:28.000Z","title":"AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent","summary":"While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03955.png","numComments":1,"submittedBy":{"_id":"6204cc0d522e40b4a18d86e2","avatarUrl":"/avatars/18daf2de5671e711dc745388dd60569d.svg","fullname":"Jindong Wang","name":"jindongwang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.02863","authors":[{"_id":"6983799a37bca54cd0587dc4","name":"Jinkun Chen","hidden":false},{"_id":"6983799a37bca54cd0587dc5","name":"Fengxiang Cheng","hidden":false},{"_id":"6983799a37bca54cd0587dc6","name":"Sijia Han","hidden":false},{"_id":"6983799a37bca54cd0587dc7","name":"Vlado Keselj","hidden":false}],"publishedAt":"2026-02-02T22:11:25.000Z","submittedOnDailyAt":"2026-02-05T14:12:17.301Z","title":"\"I May Not Have Articulated Myself Clearly\": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time","submittedOnDailyBy":{"_id":"6331dcb29e3604f3f18382f3","avatarUrl":"/avatars/64d08cfce64a9efda0896f7359e30281.svg","isPro":false,"fullname":"Jinkun Chen","user":"Jinnkunn","type":"user"},"summary":"Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model \"loses the thread\" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer (corrective instability), whereas late instability is more often followed by failure (destructive instability), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism.","upvotes":1,"discussionId":"6983799a37bca54cd0587dc8","ai_summary":"Analysis of reasoning failures in large language models reveals that instability signals derived from token log probabilities and entropy can predict incorrect answers and distinguish between corrective and destructive instability based on timing of distribution shifts.","ai_keywords":["large language models","reasoning failures","inference-time observables","token log probabilities","distributional shift","Jensen-Shannon divergence","entropy","GSM8K","HotpotQA","instability signal","corrective instability","destructive instability"]},"publishedAt":"2026-02-02T17:11:25.000Z","title":"\"I May Not Have Articulated Myself Clearly\": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time","summary":"Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model \"loses the thread\" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer (corrective instability), whereas late instability is more often followed by failure (destructive instability), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02863.png","numComments":1,"submittedBy":{"_id":"6331dcb29e3604f3f18382f3","avatarUrl":"/avatars/64d08cfce64a9efda0896f7359e30281.svg","fullname":"Jinkun Chen","name":"Jinnkunn","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.02495","authors":[{"_id":"698215b29c2f139721ec34e3","name":"Peter Chen","hidden":false},{"_id":"698215b29c2f139721ec34e4","name":"Xiaopeng Li","hidden":false},{"_id":"698215b29c2f139721ec34e5","name":"Xi Chen","hidden":false},{"_id":"698215b29c2f139721ec34e6","name":"Tianyi Lin","hidden":false}],"publishedAt":"2026-02-02T18:59:52.000Z","submittedOnDailyAt":"2026-02-05T12:06:00.060Z","title":"Reward-free Alignment for Conflicting Objectives","submittedOnDailyBy":{"_id":"678323cb4bd851a06acb936f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/amD4z7Yr6mCh-mQ0_Rnb1.png","isPro":false,"fullname":"Peter L. Chen","user":"PeterLauLukCh","type":"user"},"summary":"Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.","upvotes":1,"discussionId":"698215b29c2f139721ec34e7","ai_summary":"A reward-free alignment framework addresses multi-objective conflicts in language models through conflict-averse gradient descent with clipping, improving Pareto trade-offs across diverse model architectures.","ai_keywords":["reward-free alignment","conflicted objectives","pairwise preference data","conflict-averse gradient descent","clipped gradient descent","Pareto-critical points","multi-objective alignment","language models","gradient conflicts","convergence guarantees"]},"publishedAt":"2026-02-02T13:59:52.000Z","title":"Reward-free Alignment for Conflicting Objectives","summary":"Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02495.png","numComments":1,"submittedBy":{"_id":"678323cb4bd851a06acb936f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/amD4z7Yr6mCh-mQ0_Rnb1.png","fullname":"Peter L. Chen","name":"PeterLauLukCh","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.02341","authors":[{"_id":"69834c2437bca54cd0587d46","user":{"_id":"62bec18e7e808565cc15610f","avatarUrl":"/avatars/78e8b98120a61bf90c43bd8c8ea8d375.svg","isPro":false,"fullname":"Zhenpeng Huang","user":"hzp","type":"user"},"name":"Zhenpeng Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-05T10:55:14.003Z","hidden":false},{"_id":"69834c2437bca54cd0587d47","name":"Jiaqi Li","hidden":false},{"_id":"69834c2437bca54cd0587d48","name":"Zihan Jia","hidden":false},{"_id":"69834c2437bca54cd0587d49","name":"Xinhao Li","hidden":false},{"_id":"69834c2437bca54cd0587d4a","name":"Desen Meng","hidden":false},{"_id":"69834c2437bca54cd0587d4b","name":"Lingxue Song","hidden":false},{"_id":"69834c2437bca54cd0587d4c","name":"Xi Chen","hidden":false},{"_id":"69834c2437bca54cd0587d4d","name":"Liang Li","hidden":false},{"_id":"69834c2437bca54cd0587d4e","name":"Limin Wang","hidden":false}],"publishedAt":"2026-02-02T17:03:37.000Z","submittedOnDailyAt":"2026-02-05T05:35:33.988Z","title":"LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization","submittedOnDailyBy":{"_id":"62bec18e7e808565cc15610f","avatarUrl":"/avatars/78e8b98120a61bf90c43bd8c8ea8d375.svg","isPro":false,"fullname":"Zhenpeng Huang","user":"hzp","type":"user"},"summary":"We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.","upvotes":1,"discussionId":"69834c2537bca54cd0587d4f","ai_summary":"LongVPO is a two-stage Direct Preference Optimization framework that enables short-context vision-language models to understand ultra-long videos through synthetic preference triples and recursive captioning, achieving state-of-the-art performance with minimal human annotation.","ai_keywords":["Direct Preference Optimization","vision-language models","ultra-long videos","preference triples","visual-similarity filtering","question-specificity filtering","positional bias","recursive captioning","multi-segment reasoning","large language model","scene-level metadata"]},"publishedAt":"2026-02-02T12:03:37.000Z","title":"LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization","summary":"We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02341.png","numComments":1,"submittedBy":{"_id":"62bec18e7e808565cc15610f","avatarUrl":"/avatars/78e8b98120a61bf90c43bd8c8ea8d375.svg","fullname":"Zhenpeng Huang","name":"hzp","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.22596","authors":[{"_id":"69803f086676f9332270667d","user":{"_id":"665219adedd332b944a648aa","avatarUrl":"/avatars/076e0aab194c623897db55b0f9f81aa2.svg","isPro":false,"fullname":"abdel","user":"abdevd","type":"user"},"name":"Abdelrrahman Moubane","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:19.874Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/665219adedd332b944a648aa/Lldwg2-srw6dmVY4C5tPA.png"],"publishedAt":"2026-01-30T05:42:42.000Z","submittedOnDailyAt":"2026-02-05T12:01:22.677Z","title":"FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data","submittedOnDailyBy":{"_id":"665219adedd332b944a648aa","avatarUrl":"/avatars/076e0aab194c623897db55b0f9f81aa2.svg","isPro":false,"fullname":"abdel","user":"abdevd","type":"user"},"summary":"We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.","upvotes":1,"discussionId":"69803f096676f9332270667e","githubRepo":"https://github.com/abdelpy/FOTBCD-datasets","githubRepoAddedBy":"user","ai_summary":"A large-scale building change detection dataset named FOTBCD is introduced, covering 28 French departments with high-resolution imagery and comprehensive annotations for both binary and instance-level change detection tasks.","ai_keywords":["building change detection","orthophotos","topographic data","domain shift","geographic diversity","cross-domain generalization"],"githubStars":3,"organization":{"_id":"6839829d8987f50a5ed39a3a","name":"retgenai","fullname":"Retgen AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/665219adedd332b944a648aa/fTOiVKOsQ6iqZNmQHHQGh.png"}},"publishedAt":"2026-01-30T00:42:42.000Z","title":"FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data","summary":"We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/665219adedd332b944a648aa/Lldwg2-srw6dmVY4C5tPA.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22596.png","numComments":1,"submittedBy":{"_id":"665219adedd332b944a648aa","avatarUrl":"/avatars/076e0aab194c623897db55b0f9f81aa2.svg","fullname":"abdel","name":"abdevd","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6839829d8987f50a5ed39a3a","name":"retgenai","fullname":"Retgen AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/665219adedd332b944a648aa/fTOiVKOsQ6iqZNmQHHQGh.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.01031","authors":[{"_id":"698510704ad556f294b7ea5b","name":"Dongyang Fan","hidden":false},{"_id":"698510704ad556f294b7ea5c","name":"Sebastien Delsad","hidden":false},{"_id":"698510704ad556f294b7ea5d","name":"Nicolas Flammarion","hidden":false},{"_id":"698510704ad556f294b7ea5e","name":"Maksym Andriushchenko","hidden":false}],"publishedAt":"2026-02-01T05:35:07.000Z","submittedOnDailyAt":"2026-02-05T19:21:42.182Z","title":"HalluHard: A Hard Multi-Turn Hallucination Benchmark","submittedOnDailyBy":{"_id":"64fee425d8c6c30ec155c2ff","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64fee425d8c6c30ec155c2ff/hkQjGchyP7CSVa8vOjAC2.jpeg","isPro":false,"fullname":"Dongyang Fan","user":"dyfan","type":"user"},"summary":"Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce HalluHard, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search (approx 30% for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.","upvotes":0,"discussionId":"698510714ad556f294b7ea5f","ai_summary":"Large language models continue to generate plausible but ungrounded factual claims in multi-turn dialogue, with hallucinations remaining significant even when utilizing web search for verification across high-stakes domains.","ai_keywords":["large language models","multi-turn dialogue","hallucination","groundedness","web search","evidence retrieval","factual assertions","citation requirement","open-ended settings","model capacity","turn position","effective reasoning","knowledge type"]},"publishedAt":"2026-02-01T00:35:07.000Z","title":"HalluHard: A Hard Multi-Turn Hallucination Benchmark","summary":"Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce HalluHard, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search (approx 30% for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01031.png","numComments":1,"submittedBy":{"_id":"64fee425d8c6c30ec155c2ff","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64fee425d8c6c30ec155c2ff/hkQjGchyP7CSVa8vOjAC2.jpeg","fullname":"Dongyang Fan","name":"dyfan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false}]